# pages/Wordãƒ™ã‚¯ãƒˆãƒ«åŒ–.py
# ------------------------------------------------------------
# ğŸ“ Build Knowledge Base from Word (.docx) with Structured Meta
# - word/*.docx ã‚’èª­ã¿è¾¼ã¿ â†’ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆæ®µè½ï¼‹è¡¨ï¼‰
# - split_text ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆspan_start / span_end ã‚’ä»˜ä¸ï¼‰
# - Embedding â†’ vectors.npy è¿½åŠ ä¿å­˜
# - meta.jsonl ã«æ§‹é€ åŒ–ãƒ¡ã‚¿ã‚’1è¡Œ1ãƒãƒ£ãƒ³ã‚¯ã§è¿½è¨˜
#   {file, page=1, chunk_id, chunk_index, text, span_start, span_end}
# - æ—¢å­˜PDF/TXTãƒšãƒ¼ã‚¸ã¨åŒã˜UXï¼ˆreset / skip_done / é€²æ—ãƒãƒ¼ç­‰ï¼‰
# ------------------------------------------------------------

import os
from pathlib import Path
from typing import List, Tuple, Dict

import streamlit as st
from dotenv import load_dotenv

from lib.rag_utils import split_text, EmbeddingStore, NumpyVectorDB, ProcessedFilesSimple

# ====== åˆæœŸåŒ– ======
load_dotenv()
st.set_page_config(page_title="Build KB from Word (Structured Meta)", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ Build Knowledge Base from Word (.docx) (Structured Meta)")

word_dir = Path("word")
word_dir.mkdir(exist_ok=True)

# ====== ã‚ªãƒ—ã‚·ãƒ§ãƒ³ UI ======
with st.expander("ğŸ“¥ ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š", expanded=True):
    backend = st.radio("åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["local", "openai"], index=1)
    chunk_size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰", 300, 2000, 900, 50)
    overlap   = st.slider("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰", 0, 400, 150, 10)
    batch_size = st.slider("åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚º", 8, 256, 64, 8)
    reset = st.checkbox("æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–ï¼ˆä½œã‚Šç›´ã™ï¼‰", value=False)
    skip_done = st.checkbox("å‰å›å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã§åˆ¤å®šï¼‰", value=True)

# ====== ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ======
store_name = "openai" if backend == "openai" else "local"
vs_dir = Path("vectorstore") / store_name
vs_dir.mkdir(parents=True, exist_ok=True)

# ====== å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç† ======
pstore = ProcessedFilesSimple(vs_dir / "processed_files.json")

# ====== Word ãƒ­ãƒ¼ãƒ€ ======
def _import_docx():
    try:
        from docx import Document  # python-docx
        return Document
    except Exception as e:
        return None

def load_docx_files(data_dir: Path) -> List[Tuple[str, str]]:
    """
    data_dir é…ä¸‹ã® .docx ã‚’èµ°æŸ»ã—ã€(ãƒ•ã‚¡ã‚¤ãƒ«å, ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    - æ®µè½ã«åŠ ãˆã¦ã€è¡¨ï¼ˆtablesï¼‰å†…ã®ã‚»ãƒ«æ–‡å­—åˆ—ã‚‚æŠ½å‡º
    - ä½™åˆ†ãªç©ºè¡Œã‚’è©°ã‚ã‚‹
    """
    Document = _import_docx()
    if Document is None:
        raise RuntimeError("python-docx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install python-docx` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    results: List[Tuple[str, str]] = []
    for p in sorted(data_dir.glob("*.docx")):
        try:
            doc = Document(p)
            texts: List[str] = []

            # æ®µè½
            for para in doc.paragraphs:
                t = (para.text or "").strip()
                if t:
                    texts.append(t)

            # è¡¨ï¼ˆè¡ŒÃ—åˆ—ï¼‰ã‚‚æ–‡å­—ã‚’æŠ½å‡º
            for tbl in getattr(doc, "tables", []):
                for row in tbl.rows:
                    row_cells = []
                    for cell in row.cells:
                        ct = (cell.text or "").strip()
                        row_cells.append(ct)
                    # ã‚¿ãƒ–åŒºåˆ‡ã‚Šã§1è¡Œã«
                    if any(row_cells):
                        texts.append("\t".join(row_cells))

            # çµåˆ & ç©ºè¡Œè©°ã‚
            raw = "\n".join(texts)
            # é€£ç¶šæ”¹è¡Œã®åœ§ç¸®
            cleaned = "\n".join([line for line in raw.splitlines() if line.strip() != ""]).strip()

            if cleaned:
                results.append((p.name, cleaned))
        except Exception as e:
            st.warning(f"Word èª­ã¿è¾¼ã¿å¤±æ•—: {p.name} / {e}")

    return results

# ====== å®Ÿè¡Œãƒœã‚¿ãƒ³ ======
st.write("ğŸ“‚ `word/` ã« .docx ã‚’ç½®ã„ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
if st.button("ğŸ”¨ Word ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜", type="primary"):
    try:
        files = load_docx_files(word_dir)  # -> [(fname, full_text)]
    except Exception as e:
        st.error(f"Word èª­ã¿è¾¼ã¿æ™‚ã«ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    if not files:
        st.warning("æŠ½å‡ºã§ããŸãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.stop()

    if store_name == "openai" and not os.getenv("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    estore = EmbeddingStore(backend=store_name)
    vdb = NumpyVectorDB(vs_dir)

    if reset:
        vdb.reset()
        pstore.reset()
        st.info(f"åˆæœŸåŒ–ã—ã¾ã—ãŸ: {vs_dir}")

    # ====== å‰å‡¦ç†ï¼ˆã‚¹ã‚­ãƒƒãƒ—åæ˜  & ãƒãƒ£ãƒ³ã‚¯æ•°é›†è¨ˆï¼‰ ======
    to_process: List[Tuple[str, List[Tuple[str, int, int]]]] = []  # (fname, [(chunk, start, end)])
    skipped = 0
    for fname, full_text in files:
        if skip_done and pstore.is_done(fname):
            skipped += 1
            continue
        chs = split_text(full_text, chunk_size=chunk_size, overlap=overlap)  # -> [(chunk, start, end)]
        if chs:
            to_process.append((fname, chs))

    if not to_process:
        st.success(f"å‡¦ç†å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ— {skipped} ä»¶ï¼‰ã€‚")
        st.stop()

    total_chunks = sum(len(chs) for _, chs in to_process)
    st.write(f"å¯¾è±¡Word: **{len(to_process)}** ä»¶ / ã‚¹ã‚­ãƒƒãƒ—: **{skipped}** ä»¶ / ãƒãƒ£ãƒ³ã‚¯ç·æ•°: **{total_chunks}**")

    prog = st.progress(0.0)
    done_chunks = 0

    # ====== ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»ä¿å­˜ ======
    try:
        processed_files = 0

        for fname, chs in to_process:
            texts: List[str] = []
            meta_items: List[Dict] = []

            for idx, (chunk, start, end) in enumerate(chs):
                # Word ã¯ãƒšãƒ¼ã‚¸æ¦‚å¿µãŒå–ã‚Šã«ãã„ã®ã§ page=1 å›ºå®šï¼ˆTXT ã¨åŒæ§˜ï¼‰
                chunk_id = f"{fname}#p1#{idx}"
                meta_items.append({
                    "file": fname,
                    "page": 1,
                    "chunk_id": chunk_id,
                    "chunk_index": idx,
                    "text": chunk,
                    "span_start": start,
                    "span_end": end,
                })
                texts.append(chunk)

            # ãƒãƒƒãƒã§åŸ‹ã‚è¾¼ã¿â†’ä¿å­˜
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_meta = meta_items[i:i + batch_size]
                embs = estore.embed(batch_texts, batch_size=batch_size)
                vdb.add(embs, batch_meta)
                done_chunks += len(batch_texts)
                prog.progress(done_chunks / total_chunks)

            pstore.mark_done(fname)
            processed_files += 1

        st.success(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {total_chunks} ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({vs_dir})")
        st.caption("meta.jsonl ã«ã¯ file/page/chunk_id/chunk_index/span_start/span_end ã‚’å«ã¿ã¾ã™ã€‚")
        st.caption(f"è¿½åŠ  {processed_files} ä»¶ / ã‚¹ã‚­ãƒƒãƒ— {skipped} ä»¶ / ãƒãƒ£ãƒ³ã‚¯ {total_chunks} ä»¶")

    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
