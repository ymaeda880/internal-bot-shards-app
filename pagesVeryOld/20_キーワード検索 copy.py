# pages/20_ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py
# ------------------------------------------------------------
# ğŸ” meta.jsonl æ¨ªæ–­æ¤œç´¢ + ï¼ˆä»»æ„ï¼‰OpenAI ç”Ÿæˆè¦ç´„
# - gpt-5 ç³»ï¼ˆç‰¹ã« gpt-5-miniï¼‰å‘ã‘ã«äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³è¦‹ç© & è‡ªå‹•ãƒãƒƒãƒè¦ç´„ã‚’å®Ÿè£…
# - ç”Ÿæˆå‰ã«ã€Œè¦ç´„ã‚’å®Ÿè¡Œã€ãƒã‚§ãƒƒã‚¯ï¼‹ãƒœã‚¿ãƒ³ã®äºŒé‡ç¢ºèªï¼ˆsession_stateã§ä¿æŒï¼‰
# - ã‚ªãƒ¼ãƒãƒ¼æ™‚ã¯è‡ªå‹•ã§åˆ†å‰²â†’å„ãƒãƒƒãƒè¦ç´„â†’çµ±åˆ
# - gpt-5 ç³»ã¯ Responses APIï¼ˆmax_output_tokensï¼‰ã€ä»–ã¯ Chat Completions
# - temperature: gpt-5 ç³»ã¯ 1.0 å›ºå®šï¼ˆUIã¯ metric è¡¨ç¤ºï¼‰
# - ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¯æ—¢å®šã§ç•³ã¿è¡¨ç¤ºï¼ˆst.expanderï¼‰
# - OpenAI å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ã¯ UI ã«æ˜ç¤ºï¼ˆè©³ç´°ãƒ­ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚ã‚Šï¼‰
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Iterable, Any
import re
import json
import os
import pandas as pd
import streamlit as st
import traceback

# ==== å¤–éƒ¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆlib/ é…ä¸‹ï¼‰ ====
from lib.text_utils import normalize_ja_text, strip_html, make_snippet
from lib.search_utils import iter_jsonl
from lib.openai_utils import (
    count_tokens, truncate_by_tokens, is_gpt5,
    chat_complete_safely, extract_text_from_chat,
    responses_generate, responses_text
)

# ============== ãƒ‘ã‚¹ ==============
APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
VS_ROOT  = DATA_DIR / "vectorstore"

# ============== Streamlit åŸºæœ¬è¨­å®š ==============
st.set_page_config(page_title="20 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmetaæ¨ªæ–­ï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmeta.jsonl æ¨ªæ–­ï¼‰")

# ============== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ==============
with st.sidebar:
    st.header("æ¤œç´¢å¯¾è±¡")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
    base_dir = VS_ROOT / backend
    if not base_dir.exists():
        st.error(f"vectorstore/{backend} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« 03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs = sorted([p for p in base_dir.iterdir() if p.is_dir()])
    shard_ids = [p.name for p in shard_dirs]
    sel_shards = st.multiselect("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰", shard_ids, default=shard_ids)

    st.divider()
    st.subheader("çµã‚Šè¾¼ã¿ï¼ˆä»»æ„ï¼‰")
    year_min = st.number_input("å¹´ï¼ˆä¸‹é™ï¼‰", value=0, step=1, help="0 ã§ç„¡åŠ¹")
    year_max = st.number_input("å¹´ï¼ˆä¸Šé™ï¼‰", value=9999, step=1, help="9999 ã§ç„¡åŠ¹")
    file_filter = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", value="").strip()

    st.divider()
    st.subheader("è¡¨ç¤ºè¨­å®š")
    max_rows = st.number_input("æœ€å¤§è¡¨ç¤ºä»¶æ•°", min_value=50, max_value=5000, value=500, step=50)
    snippet_len = st.slider("ã‚¹ãƒ‹ãƒšãƒƒãƒˆé•·ï¼ˆå‰å¾Œåˆè¨ˆï¼‰", min_value=80, max_value=800, value=240, step=20)
    show_cols = st.multiselect(
        "è¡¨ç¤ºã‚«ãƒ©ãƒ ",
        ["file","year","page","shard_id","chunk_id","chunk_index","score","text"],
        default=["file","year","page","shard_id","score","text"]
    )

    # ============== ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ ==============
    st.divider()
    st.subheader("ğŸ§  ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆOpenAIï¼‰")

    # secrets.toml å„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    def _get_openai_key() -> str | None:
        try:
            return (
                st.secrets.get("OPENAI_API_KEY")
                or (st.secrets.get("openai") or {}).get("api_key")
                or os.getenv("OPENAI_API_KEY")
            )
        except Exception:
            return os.getenv("OPENAI_API_KEY")

    OPENAI_API_KEY = _get_openai_key()
    has_key = bool(OPENAI_API_KEY)

    gen_enabled = st.checkbox("ãƒ’ãƒƒãƒˆè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹", value=True if has_key else False, disabled=not has_key)
    if not has_key:
        st.warning("OPENAI_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ã€ç”Ÿæˆã¯ç„¡åŠ¹ã§ã™ã€‚", icon="âš ï¸")

    model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«",
        ["gpt-5-mini", "gpt-5", "gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"],
        index=0,
        disabled=not gen_enabled
    )

    # gpt-5 ç³»ã¯ temperature=1 å›ºå®šï¼ˆSliderã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§ metric è¡¨ç¤ºï¼‰
    if is_gpt5(model):
        temperature = 1.0
        st.metric(label="temperature", value="1.0")
        st.caption("ğŸ”’ gpt-5 ç³»ãƒ¢ãƒ‡ãƒ«ã§ã¯ temperature=1 ã«å›ºå®šã•ã‚Œã¾ã™ã€‚")
    else:
        temperature = st.slider("temperature", 0.0, 1.0, 0.2, 0.05, disabled=not gen_enabled)

    # ä¸Šé™ã¯åºƒã‚ã«ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã—ãªã„å€¤ã¯ API ãŒå¼¾ãï¼‰
    max_tokens = st.slider("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™", 128, 32000, 2000, 128, disabled=not gen_enabled)
    topn_snippets = st.slider("ç”Ÿæˆã«ä½¿ã†ä¸Šä½ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°", 5, 200, 30, 5, disabled=not gen_enabled)

    auto_batch = st.checkbox("ã‚ªãƒ¼ãƒãƒ¼æ™‚ã¯è‡ªå‹•ãƒãƒƒãƒè¦ç´„ã§å‡¦ç†ã™ã‚‹ï¼ˆæ¨å¥¨ï¼‰", value=True, disabled=not gen_enabled)
    verbose_log = st.checkbox("ãƒãƒƒãƒå‡¦ç†ã®è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º", value=True, disabled=not gen_enabled)
    debug_mode = st.checkbox("è©³ç´°ãƒ­ã‚°ï¼ˆã‚¨ãƒ©ãƒ¼è©³ç´°ãƒ»ãƒˆãƒ¬ãƒ¼ã‚¹è¡¨ç¤ºï¼‰", value=False, disabled=not gen_enabled)

    sys_prompt = st.text_area(
        "System Prompt",
        value="ã‚ãªãŸã¯äº‹å®Ÿã«å¿ å®Ÿãªãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ ¹æ‹ ã®ã‚ã‚‹è¨˜è¿°ã®ã¿ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
        height=80,
        disabled=not gen_enabled
    )
    user_prompt_tpl = st.text_area(
        "User Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ{query}, {snippets} ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰",
        value=(
            "ä»¥ä¸‹ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã™ã€‚"
            "ã“ã®æƒ…å ±ã€ã®ã¿ã€‘ã‚’æ ¹æ‹ ã«ã€ã‚¯ã‚¨ãƒªã€{query}ã€ã«ã¤ã„ã¦è¦ç‚¹ã‚’ç®‡æ¡æ›¸ãâ†’çŸ­ã„ã¾ã¨ã‚ã®é †ã§æ•´ç†ã—ã¦ãã ã•ã„ã€‚"
            "\n\n# ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆ\n{snippets}"
        ),
        height=140,
        disabled=not gen_enabled
    )

# ============== æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ==============
st.markdown("### ã‚¯ã‚¨ãƒª")
c1, c2 = st.columns([3,2])
with c1:
    query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šã§ AND / OR æŒ‡å®šï¼‰", value="")
with c2:
    bool_mode = st.radio("ãƒ¢ãƒ¼ãƒ‰", ["AND", "OR"], index=0, horizontal=True)

c3, c4, c5, c6 = st.columns(4)
with c3:
    use_regex = st.checkbox("æ­£è¦è¡¨ç¾", value=False)
with c4:
    case_sensitive = st.checkbox("å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥", value=False)
with c5:
    normalize_query = st.checkbox("æ—¥æœ¬èªã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–ï¼ˆæ¨å¥¨ï¼‰", value=True)
with c6:
    norm_body = st.checkbox("æœ¬æ–‡ã‚‚æ­£è¦åŒ–ã—ã¦æ¤œç´¢", value=True, help="å–ã‚Šè¾¼ã¿æ™‚ã«æ­£è¦åŒ–ã—ã¦ã„ãªã„ã‚³ãƒ¼ãƒ‘ã‚¹å‘ã‘")

go = st.button("æ¤œç´¢ã‚’å®Ÿè¡Œ", type="primary")

# ============== æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ==============
def compile_terms(q: str, use_regex: bool, case_sensitive: bool) -> List[re.Pattern]:
    if normalize_query:
        q = normalize_ja_text(q)
    terms = [t for t in q.split() if t]
    if not terms:
        return []
    flags = 0 if case_sensitive else re.IGNORECASE
    pats = []
    for t in terms:
        if use_regex:
            try:
                pats.append(re.compile(t, flags))
            except re.error:
                pats.append(re.compile(re.escape(t), flags))
        else:
            pats.append(re.compile(re.escape(t), flags))
    return pats

def copy_button(text: str, label: str, key: str):
    payload = json.dumps(text, ensure_ascii=False)
    html = f"""
    <button id="{key}" style="
        padding:6px 10px;border-radius:8px;border:1px solid #dadce0;
        background:#fff;cursor:pointer;font-size:0.9rem;">ğŸ“‹ {label}</button>
    <script>
      const btn = document.getElementById("{key}");
      if (btn) {{
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({payload});
            const old = btn.innerText;
            btn.innerText = "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(()=>{{ btn.innerText = old; }}, 1200);
          }} catch(e) {{
            alert("ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: " + e);
          }}
        }});
      }}
    </script>
    """
    st.components.v1.html(html, height=38)

# ---------- ãƒãƒƒãƒåˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆgpt-5-miniå‰æï¼‰ ----------
def _gpt5mini_limits():
    # æ—¢å®šï¼šcontext 128kã€ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒãƒ¼ã‚¸ãƒ³ 2k
    return 128_000, 2_000

def _estimate_prompt_tokens(model: str, sys_prompt: str, user_prompt_prefix: str) -> int:
    return count_tokens(sys_prompt, model) + count_tokens(user_prompt_prefix, model)

def _split_into_batches(snippets: List[str], model: str, input_budget_tokens: int) -> List[List[str]]:
    batches: List[List[str]] = []
    current: List[str] = []
    cur_tokens = 0
    for s in snippets:
        t = count_tokens(s, model)
        if t > input_budget_tokens:
            s = truncate_by_tokens(s, input_budget_tokens, model)
            t = count_tokens(s, model)
        if cur_tokens + t <= input_budget_tokens:
            current.append(s); cur_tokens += t
        else:
            if current:
                batches.append(current)
            current = [s]; cur_tokens = t
    if current:
        batches.append(current)
    return batches

def _render_snippets(snips: List[str]) -> str:
    return "\n\n".join(snips)

# --------------- OpenAI ã‚¨ãƒ©ãƒ¼ã®å¯è¦–åŒ– ---------------
def _render_openai_error(err: Exception, *, context: Dict[str, Any], debug: bool):
    st.error("OpenAI ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", icon="ğŸ›‘")
    msg = str(err) or repr(err)
    hints = []
    low = msg.lower()
    if "401" in msg or "unauthorized" in low or "authentication" in low:
        hints.append("ãƒ»èªè¨¼ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ãŒç„¡åŠ¹/æœªè¨­å®šã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    if "429" in msg or "rate" in low or "quota" in low:
        hints.append("ãƒ»ãƒ¬ãƒ¼ãƒˆåˆ¶é™/ã‚¯ã‚©ãƒ¼ã‚¿è¶…éï¼šæ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã€ã¾ãŸã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    if "404" in msg and "model" in low:
        hints.append("ãƒ»ãƒ¢ãƒ‡ãƒ«åã®èª¤ã‚Š/æ¨©é™ä¸è¶³ï¼šé¸æŠãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ä¸å¯ã®å¯èƒ½æ€§ã€‚")
    if "context" in low or "max_tokens" in low:
        hints.append("ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ï¼šå…¥åŠ›ã‚„å‡ºåŠ›ä¸Šé™ãŒå¤§ãã™ãã‚‹å¯èƒ½æ€§ã€‚")
    if hints:
        st.write("è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ï¼š")
        for h in hints:
            st.write(h)
    with st.expander("å®Ÿè¡Œæ™‚ã®æ¡ä»¶ï¼ˆé€ä¿¡å´ã®è¦ç´„ï¼‰", expanded=False):
        st.json({
            "model": context.get("model"),
            "temperature": context.get("temperature"),
            "max_tokens": context.get("max_tokens"),
            "topn_snippets": context.get("topn_snippets"),
            "snippets_count": context.get("snippets_count"),
            "query": context.get("query"),
        })
    if debug:
        st.caption("ä¾‹å¤–è©³ç´°")
        st.code(f"{type(err).__name__}: {msg}")
        st.caption("Traceback")
        st.code("".join(traceback.format_exc()))

# ============== å®Ÿè¡Œ ==============
if go:
    if not sel_shards:
        st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    pats = compile_terms(query, use_regex=use_regex, case_sensitive=case_sensitive)
    if not pats:
        st.warning("æ¤œç´¢èªãŒç©ºã§ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    rows: List[Dict[str,Any]] = []
    total_scanned = 0

    for sid in sel_shards:
        meta_path = base_dir / sid / "meta.jsonl"
        if not meta_path.exists():
            st.warning(f"{meta_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        for obj in iter_jsonl(meta_path):
            total_scanned += 1
            yr = obj.get("year", None)
            if isinstance(yr, int):
                if year_min and yr < year_min: continue
                if year_max and year_max < 9999 and yr > year_max: continue
            if file_filter and file_filter.lower() not in str(obj.get("file","")).lower():
                continue

            text = str(obj.get("text",""))
            text_for_match = normalize_ja_text(text) if norm_body else text

            ok = all(p.search(text_for_match) for p in pats) if bool_mode == "AND" \
                 else any(p.search(text_for_match) for p in pats)
            if not ok:
                continue

            score = sum(len(list(p.finditer(text_for_match))) for p in pats)
            rows.append({
                "file": obj.get("file"),
                "year": obj.get("year"),
                "page": obj.get("page"),
                "shard_id": obj.get("shard_id", sid),
                "chunk_id": obj.get("chunk_id"),
                "chunk_index": obj.get("chunk_index"),
                "score": int(score),
                "text": make_snippet(text, pats, total_len=int(snippet_len)),
            })

            if len(rows) >= int(max_rows):
                break
        if len(rows) >= int(max_rows):
            break

    if not rows:
        st.warning("ãƒ’ãƒƒãƒˆãªã—ã€‚æ¤œç´¢èªã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    df = pd.DataFrame(rows).sort_values(["score","year","file","page"], ascending=[False, True, True, True])

    st.success(f"ãƒ’ãƒƒãƒˆ {len(df):,d} ä»¶ / èµ°æŸ» {total_scanned:,d} ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆä¸Šä½ã®ã¿è¡¨ç¤ºï¼‰")

    show_order = [c for c in show_cols if c in df.columns]
    if not show_order:
        show_order = ["file","year","page","shard_id","score","text"]
    non_text_cols = [c for c in show_order if c != "text"]
    st.dataframe(df[non_text_cols], use_container_width=True, height=420)

    csv_bytes = df[show_order].to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ“¥ CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name="keyword_hits.csv", mime="text/csv")

    # ============== ç”Ÿæˆï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚ˆã‚Šå‰ã«è¡¨ç¤ºï¼‰ ==============
    if gen_enabled:
        st.divider()
        st.subheader("ğŸ§  ç”Ÿæˆè¦ç´„ï¼ˆOpenAIï¼‰")

        # 1) ä¸Šä½ N ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆHTMLé™¤å» & 1ã¤ã²ã¨ã¤ã« Source ãƒ©ãƒ™ãƒ«ï¼‰
        take_n = int(topn_snippets)
        selected = df.head(take_n).copy()

        labelled_snips: List[str] = []
        for _, r in selected.iterrows():
            src = f"{r.get('file')} p.{r.get('page')} (score={r.get('score')})"
            snip = strip_html(str(r.get("text","")))
            labelled_snips.append(f"---\n# Source: {src}\n{snip}")

        # 2) äº‹å‰è¦‹ç©ï¼ˆã¾ãšã¯è¡¨ç¤ºã ã‘è¡Œã†ï¼‰
        model_hint = model
        context_limit, safety_margin = _gpt5mini_limits() if is_gpt5(model) else (128_000, 1_000)
        user_prefix = user_prompt_tpl.replace("{snippets}", "").format(query=query, snippets="")
        prompt_overhead = _estimate_prompt_tokens(model_hint, sys_prompt, user_prefix)
        snippets_tokens = sum(count_tokens(s, model_hint) for s in labelled_snips)
        want_output = int(max_tokens)
        needed_total = prompt_overhead + snippets_tokens + want_output + safety_margin

        st.caption(f"è¦‹ç©: prompt+snips={prompt_overhead + snippets_tokens:,} / "
                   f"å‡ºåŠ›ä¸Šé™={want_output:,} / safety={safety_margin:,} / "
                   f"context_limit~{context_limit:,}")

        will_overflow = needed_total > context_limit
        if will_overflow:
            over = needed_total - context_limit
            st.error(f"âš ï¸ å…¥åŠ›ãŒå¤§ãã™ãã¾ã™ï¼ˆæ¨å®š {needed_total:,} tok ãŒä¸Šé™ {context_limit:,} tok ã‚’ {over:,} tok è¶…éï¼‰ã€‚", icon="âš ï¸")
            if not auto_batch:
                st.info("å¯¾å‡¦: â‘  ç”Ÿæˆã«ä½¿ã†ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°ã‚’æ¸›ã‚‰ã™ â‘¡ å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’ä¸‹ã’ã‚‹ â‘¢ ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ï¼ˆå¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰")

        # 3) å®Ÿè¡Œå‰ç¢ºèª UIï¼ˆsession_stateã§ä¿æŒï¼‰
        with st.form("run_summary_form"):
            agree = st.checkbox(
                "ã“ã®æ¡ä»¶ã§è¦ç´„ã‚’å®Ÿè¡Œã—ã¦ã‚ˆã„ï¼ˆå®Ÿè¡Œå‰ã®æœ€çµ‚ç¢ºèªï¼‰",
                value=st.session_state.get("agree_summary", False),
                key="agree_summary"
            )
            run_summary = st.form_submit_button("ğŸ§  è¦ç´„ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

        if not (st.session_state.get("agree_summary") and run_summary):
            st.info("è¦ç´„ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦ã€ğŸ§  è¦ç´„ã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            # ==== ã“ã“ã‹ã‚‰å®Ÿéš›ã®ç”Ÿæˆ ====
            try:
                client = OpenAI(api_key=OPENAI_API_KEY)
            except Exception as e:
                _render_openai_error(e, context={
                    "model": model, "temperature": temperature, "max_tokens": want_output,
                    "topn_snippets": take_n, "snippets_count": len(labelled_snips), "query": query
                }, debug=debug_mode)
                st.stop()

            if will_overflow and not auto_batch:
                st.stop()

            # --- è‡ªå‹•ãƒãƒƒãƒè¦ç´„ ---
            if will_overflow and auto_batch:
                st.info("ğŸª„ è‡ªå‹•ãƒãƒƒãƒè¦ç´„ã‚’é–‹å§‹ã—ã¾ã™ï¼šã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¤‡æ•°ãƒãƒƒãƒã«åˆ†å‰² â†’ å„ãƒãƒƒãƒè¦ç´„ â†’ æœ€çµ‚çµ±åˆã€‚")
                per_batch_budget = max(1000, context_limit - want_output - safety_margin - prompt_overhead)
                batches = _split_into_batches(labelled_snips, model_hint, per_batch_budget)
                st.caption(f"ãƒãƒƒãƒæ•°: {len(batches)} / ãƒãƒƒãƒå…¥åŠ›ãƒã‚¸ã‚§ãƒƒãƒˆ~{per_batch_budget:,} tok")

                batch_summaries: List[str] = []
                for bi, batch in enumerate(batches, start=1):
                    batch_snips = _render_snippets(batch)
                    user_prompt = user_prompt_tpl.format(query=query, snippets=batch_snips)
                    approx_in = count_tokens(user_prompt, model_hint) + count_tokens(sys_prompt, model_hint)
                    if verbose_log:
                        st.write(f"Batch {bi}/{len(batches)}: å…¥åŠ›æ¨å®š ~{approx_in:,} tok / å‡ºåŠ›ä¸Šé™ {want_output:,} tok")
                    try:
                        with st.spinner(f"Batch {bi}/{len(batches)} ã‚’è¦ç´„ä¸­â€¦"):
                            if is_gpt5(model):
                                resp = responses_generate(
                                    client, model=model, temperature=float(temperature),
                                    max_output_tokens=want_output, system_prompt=sys_prompt,
                                    user_prompt=user_prompt
                                )
                                text = responses_text(resp)
                                finish = getattr(resp, "finish_reason", None)
                                usage  = getattr(resp, "usage", None)
                            else:
                                resp = chat_complete_safely(
                                    client, model=model, temperature=float(temperature),
                                    limit_tokens=want_output, system_prompt=sys_prompt,
                                    user_prompt=user_prompt
                                )
                                text = extract_text_from_chat(resp)
                                try:
                                    finish = resp.choices[0].finish_reason
                                    usage  = resp.usage
                                except Exception:
                                    finish, usage = None, None
                        if verbose_log:
                            try:
                                ct = getattr(usage, "completion_tokens", None) or usage.get("completion_tokens")
                                pt = getattr(usage, "prompt_tokens", None) or usage.get("prompt_tokens")
                                st.caption(f"Batch {bi}: finish={finish} / tokens(c/p)={ct}/{pt}")
                            except Exception:
                                st.caption(f"Batch {bi}: finish={finish}")
                        batch_summaries.append(f"[Batch {bi} è¦ç´„]\n{(text or '').strip()}")
                    except Exception as e:
                        _render_openai_error(e, context={
                            "model": model, "temperature": temperature, "max_tokens": want_output,
                            "topn_snippets": take_n, "snippets_count": len(batch), "query": query
                        }, debug=debug_mode)
                        st.stop()

                # æœ€çµ‚çµ±åˆ
                st.info("ğŸ§© ãƒãƒƒãƒè¦ç´„ã‚’çµ±åˆã—ã¦ã„ã¾ã™â€¦")
                joined_batch = "\n\n".join(batch_summaries)
                prefix = "ä»¥ä¸‹ã¯è¤‡æ•°ãƒãƒƒãƒã®è¦ç´„ã§ã™ã€‚é‡è¤‡ã‚’çµ±åˆã—ã€çŸ›ç›¾ã‚’è§£æ¶ˆã—ã€æœ€çµ‚ã®ç°¡æ½”ãªæ—¥æœ¬èªã‚µãƒãƒªã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
                integration_prompt = f"{prefix}{joined_batch}"
                approx_integration = count_tokens(integration_prompt, model_hint) + count_tokens(sys_prompt, model_hint)
                if approx_integration + want_output + safety_margin > context_limit:
                    keep = context_limit - want_output - safety_margin - count_tokens(sys_prompt, model_hint)
                    integration_prompt = truncate_by_tokens(integration_prompt, max(1000, keep), model_hint)
                    approx_integration = count_tokens(integration_prompt, model_hint) + count_tokens(sys_prompt, model_hint)
                    st.caption(f"çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒªãƒ : å…¥åŠ›æ¨å®š ~{approx_integration:,} tok")

                try:
                    with st.spinner("æœ€çµ‚çµ±åˆè¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                        if is_gpt5(model):
                            final_resp = responses_generate(
                                client, model=model, temperature=float(temperature),
                                max_output_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=integration_prompt
                            )
                            final_text = responses_text(final_resp)
                            finish_final = getattr(final_resp, "finish_reason", None)
                            usage_final  = getattr(final_resp, "usage", None)
                        else:
                            final_resp = chat_complete_safely(
                                client, model=model, temperature=float(temperature),
                                limit_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=integration_prompt
                            )
                            final_text = extract_text_from_chat(final_resp)
                            try:
                                finish_final = final_resp.choices[0].finish_reason
                                usage_final  = final_resp.usage
                            except Exception:
                                finish_final, usage_final = None, None

                    st.markdown(final_text if (final_text and final_text.strip()) else "_ï¼ˆçµ±åˆçµæœãŒç©ºã§ã—ãŸï¼‰_")
                    if debug_mode:
                        cols = st.columns(3)
                        with cols[0]:
                            st.caption(f"finish_reason: **{finish_final}**")
                        with cols[1]:
                            try:
                                ct = getattr(usage_final, "completion_tokens", None) or usage_final.get("completion_tokens")
                                pt = getattr(usage_final, "prompt_tokens", None) or usage_final.get("prompt_tokens")
                                st.caption(f"tokens (c/p): **{ct}/{pt}**")
                            except Exception:
                                pass
                        with cols[2]:
                            st.caption(f"model: **{model}**")
                except Exception as e:
                    _render_openai_error(e, context={
                        "model": model, "temperature": temperature, "max_tokens": want_output,
                        "topn_snippets": take_n, "snippets_count": len(batch_summaries), "query": query
                    }, debug=debug_mode)

            else:
                # --- å˜ç™ºã§åã¾ã‚‹å ´åˆ ---
                snippets_text = _render_snippets(labelled_snips)
                user_prompt = user_prompt_tpl.format(query=query, snippets=snippets_text)
                approx_total = count_tokens(user_prompt, model_hint) + count_tokens(sys_prompt, model_hint)
                st.caption(f"ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: ~{approx_total:,} tok / å‡ºåŠ›ä¸Šé™ {want_output:,} tokï¼‰")

                try:
                    with st.spinner("è¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                        if is_gpt5(model):
                            raw = responses_generate(
                                client, model=model, temperature=float(temperature),
                                max_output_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=user_prompt
                            )
                            out_text = responses_text(raw)
                            finish = getattr(raw, "finish_reason", None)
                            usage  = getattr(raw, "usage", None)
                        else:
                            raw = chat_complete_safely(
                                client, model=model, temperature=float(temperature),
                                limit_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=user_prompt
                            )
                            out_text = extract_text_from_chat(raw)
                            try:
                                finish = raw.choices[0].finish_reason
                                usage  = raw.usage
                            except Exception:
                                finish, usage = None, None

                    if (not out_text or not out_text.strip()) and finish == "length":
                        st.info("ğŸ” å‡ºåŠ›ãŒæ‰“ã¡åˆ‡ã‚‰ã‚ŒãŸãŸã‚ã€ç¶šãã®ç”Ÿæˆã‚’åŠ ãˆã¦ã„ã¾ã™â€¦")
                        cont_prompt = user_prompt + "\n\nã€ç¶šãã®ã¿ã‚’ç°¡æ½”ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ã€‘"
                        if is_gpt5(model):
                            raw2 = responses_generate(
                                client, model=model, temperature=float(temperature),
                                max_output_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=cont_prompt
                            )
                            out_text2 = responses_text(raw2)
                        else:
                            raw2 = chat_complete_safely(
                                client, model=model, temperature=float(temperature),
                                limit_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=cont_prompt
                            )
                            out_text2 = extract_text_from_chat(raw2)
                        out_text = (out_text or "") + ("\n" + out_text2 if out_text2 else "")

                    st.markdown(out_text if (out_text and out_text.strip()) else "_ï¼ˆæœ¬æ–‡ãŒè¿”ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰_")
                    if debug_mode:
                        cols = st.columns(3)
                        with cols[0]:
                            st.caption(f"finish_reason: **{finish}**")
                        with cols[1]:
                            try:
                                ct = getattr(usage, "completion_tokens", None) or usage.get("completion_tokens")
                                pt = getattr(usage, "prompt_tokens", None) or usage.get("prompt_tokens")
                                st.caption(f"tokens (c/p): **{ct}/{pt}**")
                            except Exception:
                                pass
                        with cols[2]:
                            st.caption(f"model: **{model}**")
                except Exception as e:
                    _render_openai_error(e, context={
                        "model": model, "temperature": temperature, "max_tokens": want_output,
                        "topn_snippets": take_n, "snippets_count": len(labelled_snips), "query": query
                    }, debug=debug_mode)

    # ============== ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆæ—¢å®šã§ç•³ã‚€ï¼‰ ==============
    if "text" in show_order:
        st.divider()
        with st.expander("ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰", expanded=False):
            for i, row in df.head(200).iterrows():
                colA, colB = st.columns([4,1])
                with colA:
                    st.markdown(
                        f"**{row.get('file')}**  year={row.get('year')}  p.{row.get('page')}  "
                        f"score={row.get('score')}",
                        help=row.get("chunk_id")
                    )
                    st.markdown(row.get("text",""), unsafe_allow_html=True)
                with colB:
                    copy_button(text=str(row.get("file")), label="year/file ã‚’ã‚³ãƒ”ãƒ¼", key=f"cpy_{i}")

else:
    st.info("å·¦ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã¨æ¡ä»¶ã‚’é¸ã³ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€æ¤œç´¢ã‚’å®Ÿè¡Œã€ã—ã¦ãã ã•ã„ã€‚")
