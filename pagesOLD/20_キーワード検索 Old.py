# pages/20_ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py
# ------------------------------------------------------------
# ğŸ” meta.jsonl æ¨ªæ–­æ¤œç´¢ + ï¼ˆä»»æ„ï¼‰OpenAI ç”Ÿæˆè¦ç´„
# - gpt-5 ç³»ï¼ˆç‰¹ã« gpt-5-miniï¼‰å‘ã‘ã«äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³è¦‹ç© & è‡ªå‹•ãƒãƒƒãƒè¦ç´„ã‚’å®Ÿè£…
# - ç”Ÿæˆå‰ã«ã€Œè¦ç´„ã‚’å®Ÿè¡Œã€ãƒã‚§ãƒƒã‚¯ + ãƒœã‚¿ãƒ³ã§æ˜ç¤ºç¢ºèªï¼ˆå¸¸ã«ï¼‰
# - ã‚ªãƒ¼ãƒãƒ¼æ™‚ã¯è­¦å‘Šè¡¨ç¤ºã€å‡¦ç†å†…å®¹ã‚’é€ä¸€UIã«æ˜ç¤º
# - gpt-5 ç³»ã¯ Responses APIï¼ˆmax_output_tokensï¼‰ã€ä»–ã¯ Chat Completions
# - temperature: gpt-5 ç³»ã¯ 1.0 å›ºå®šï¼ˆUIã¯ metric è¡¨ç¤ºï¼‰
# - ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¯æ—¢å®šã§ç•³ã¿è¡¨ç¤ºï¼ˆst.expanderï¼‰
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Iterable, Any, Tuple
from datetime import datetime
import re
import json
import unicodedata
import os

import numpy as np  # noqa
import pandas as pd
import streamlit as st
from openai import OpenAI

# ============== ãƒ‘ã‚¹ ==============
APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
VS_ROOT  = DATA_DIR / "vectorstore"

# ============== æ—¥æœ¬èªæ­£è¦åŒ– ==============
CJK = r"\u3040-\u309F\u30A0-\u30FF\uFF65-\uFF9F\u4E00-\u9FFF\u3400-\u4DBF"
PUNC = r"ã€ã€‚ãƒ»ï¼Œï¼ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€Œã€ã€ã€ã€ˆã€‰ã€Šã€‹ã€ã€‘"
_cjk_cjk_space = re.compile(fr"(?<=[{CJK}])\s+(?=[{CJK}])")
_space_before_punc = re.compile(fr"\s+(?=[{PUNC}])")
_space_after_open = re.compile(fr"(?<=[ï¼ˆï¼»ï½›ã€Œã€ã€ˆã€Šã€])\s+")
_space_before_close = re.compile(fr"\s+(?=[ï¼‰ï¼½ï½ã€ã€ã€‰ã€‹ã€‘])")
_multi_space = re.compile(r"[ \t\u3000]+")

def normalize_ja_text(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _cjk_cjk_space.sub("", s)
    s = _space_before_punc.sub("", s)
    s = _space_after_open.sub("", s)
    s = _space_before_close.sub("", s)
    s = _multi_space.sub(" ", s)
    return s.strip()

# ============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆç”Ÿæˆç”¨ï¼‰ ==============
def strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")

# ---- token helpers ----
def _encoding_for(model_hint: str):
    import tiktoken
    try:
        return tiktoken.encoding_for_model(model_hint)
    except Exception:
        return tiktoken.get_encoding("cl100k_base")

def _count_tokens(text: str, model_hint: str = "gpt-5-mini") -> int:
    try:
        enc = _encoding_for(model_hint)
        return len(enc.encode(text or ""))
    except Exception:
        return max(1, int(len(text or "") / 4))

def _truncate_by_tokens(text: str, max_tokens: int, model_hint: str = "gpt-5-mini") -> str:
    try:
        enc = _encoding_for(model_hint)
        toks = enc.encode(text or "")
        if len(toks) <= max_tokens:
            return text or ""
        return enc.decode(toks[:max_tokens])
    except Exception:
        max_chars = max(100, max_tokens * 4)
        return (text or "")[:max_chars]

def _is_gpt5(model_name: str) -> bool:
    return (model_name or "").lower().startswith("gpt-5")

# ---- JSONL ----
def iter_jsonl(path: Path) -> Iterable[Dict]:
    if not path.exists():
        return
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s:
                continue
            try:
                yield json.loads(s)
            except Exception:
                continue

# ============== UI åŸºæœ¬ ==============
st.set_page_config(page_title="20 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmetaæ¨ªæ–­ï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmeta.jsonl æ¨ªæ–­ï¼‰")

with st.sidebar:
    st.header("æ¤œç´¢å¯¾è±¡")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
    base_dir = VS_ROOT / backend
    if not base_dir.exists():
        st.error(f"vectorstore/{backend} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« 03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs = sorted([p for p in base_dir.iterdir() if p.is_dir()])
    shard_ids = [p.name for p in shard_dirs]
    sel_shards = st.multiselect("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰", shard_ids, default=shard_ids)

    st.divider()
    st.subheader("çµã‚Šè¾¼ã¿ï¼ˆä»»æ„ï¼‰")
    year_min = st.number_input("å¹´ï¼ˆä¸‹é™ï¼‰", value=0, step=1, help="0 ã§ç„¡åŠ¹")
    year_max = st.number_input("å¹´ï¼ˆä¸Šé™ï¼‰", value=9999, step=1, help="9999 ã§ç„¡åŠ¹")
    file_filter = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", value="").strip()

    st.divider()
    st.subheader("è¡¨ç¤ºè¨­å®š")
    max_rows = st.number_input("æœ€å¤§è¡¨ç¤ºä»¶æ•°", min_value=50, max_value=5000, value=500, step=50)
    snippet_len = st.slider("ã‚¹ãƒ‹ãƒšãƒƒãƒˆé•·ï¼ˆå‰å¾Œåˆè¨ˆï¼‰", min_value=80, max_value=800, value=240, step=20)
    show_cols = st.multiselect(
        "è¡¨ç¤ºã‚«ãƒ©ãƒ ",
        ["file","year","page","shard_id","chunk_id","chunk_index","score","text"],
        default=["file","year","page","shard_id","score","text"]
    )

    # ============== ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ ==============
    st.divider()
    st.subheader("ğŸ§  ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆOpenAIï¼‰")

    # secrets.toml å„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    def _get_openai_key() -> str | None:
        try:
            return (
                st.secrets.get("OPENAI_API_KEY")
                or (st.secrets.get("openai") or {}).get("api_key")
                or os.getenv("OPENAI_API_KEY")
            )
        except Exception:
            return os.getenv("OPENAI_API_KEY")

    OPENAI_API_KEY = _get_openai_key()
    has_key = bool(OPENAI_API_KEY)

    gen_enabled = st.checkbox("ãƒ’ãƒƒãƒˆè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹", value=True if has_key else False, disabled=not has_key)
    if not has_key:
        st.warning("OPENAI_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ã€ç”Ÿæˆã¯ç„¡åŠ¹ã§ã™ã€‚", icon="âš ï¸")

    model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«",
        ["gpt-5-mini", "gpt-5", "gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"],
        index=0,
        disabled=not gen_enabled
    )

    # gpt-5 ç³»ã¯ temperature=1 å›ºå®šï¼ˆSliderã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§ metric è¡¨ç¤ºï¼‰
    if _is_gpt5(model):
        temperature = 1.0
        st.metric(label="temperature", value="1.0")
        st.caption("ğŸ”’ gpt-5 ç³»ãƒ¢ãƒ‡ãƒ«ã§ã¯ temperature=1 ã«å›ºå®šã•ã‚Œã¾ã™ã€‚")
    else:
        temperature = st.slider("temperature", 0.0, 1.0, 0.2, 0.05, disabled=not gen_enabled)

    # ä¸Šé™ã¯åºƒã‚ã«ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã—ãªã„å€¤ã¯ API ãŒå¼¾ãï¼‰
    max_tokens = st.slider("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™", 128, 32000, 2000, 128, disabled=not gen_enabled)
    topn_snippets = st.slider("ç”Ÿæˆã«ä½¿ã†ä¸Šä½ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°", 5, 200, 30, 5, disabled=not gen_enabled)

    auto_batch = st.checkbox("ã‚ªãƒ¼ãƒãƒ¼æ™‚ã¯è‡ªå‹•ãƒãƒƒãƒè¦ç´„ã§å‡¦ç†ã™ã‚‹ï¼ˆæ¨å¥¨ï¼‰", value=True, disabled=not gen_enabled)
    verbose_log = st.checkbox("ãƒãƒƒãƒå‡¦ç†ã®è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º", value=True, disabled=not gen_enabled)

    sys_prompt = st.text_area(
        "System Prompt",
        value="ã‚ãªãŸã¯äº‹å®Ÿã«å¿ å®Ÿãªãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ ¹æ‹ ã®ã‚ã‚‹è¨˜è¿°ã®ã¿ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
        height=80,
        disabled=not gen_enabled
    )
    user_prompt_tpl = st.text_area(
        "User Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ{query}, {snippets} ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰",
        value=(
            "ä»¥ä¸‹ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã™ã€‚"
            "ã“ã®æƒ…å ±ã€ã®ã¿ã€‘ã‚’æ ¹æ‹ ã«ã€ã‚¯ã‚¨ãƒªã€{query}ã€ã«ã¤ã„ã¦è¦ç‚¹ã‚’ç®‡æ¡æ›¸ãâ†’çŸ­ã„ã¾ã¨ã‚ã®é †ã§æ•´ç†ã—ã¦ãã ã•ã„ã€‚"
            "\n\n# ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆ\n{snippets}"
        ),
        height=140,
        disabled=not gen_enabled
    )

# ============== æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ==============
st.markdown("### ã‚¯ã‚¨ãƒª")
c1, c2 = st.columns([3,2])
with c1:
    query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šã§ AND / OR æŒ‡å®šï¼‰", value="")
with c2:
    bool_mode = st.radio("ãƒ¢ãƒ¼ãƒ‰", ["AND", "OR"], index=0, horizontal=True)

c3, c4, c5, c6 = st.columns(4)
with c3:
    use_regex = st.checkbox("æ­£è¦è¡¨ç¾", value=False)
with c4:
    case_sensitive = st.checkbox("å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥", value=False)
with c5:
    normalize_query = st.checkbox("æ—¥æœ¬èªã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–ï¼ˆæ¨å¥¨ï¼‰", value=True)
with c6:
    norm_body = st.checkbox("æœ¬æ–‡ã‚‚æ­£è¦åŒ–ã—ã¦æ¤œç´¢", value=True, help="å–ã‚Šè¾¼ã¿æ™‚ã«æ­£è¦åŒ–ã—ã¦ã„ãªã„ã‚³ãƒ¼ãƒ‘ã‚¹å‘ã‘")

go = st.button("æ¤œç´¢ã‚’å®Ÿè¡Œ", type="primary")

# ============== æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ==============
def compile_terms(q: str, use_regex: bool, case_sensitive: bool) -> List[re.Pattern]:
    if normalize_query:
        q = normalize_ja_text(q)
    terms = [t for t in q.split() if t]
    if not terms:
        return []
    flags = 0 if case_sensitive else re.IGNORECASE
    pats = []
    for t in terms:
        if use_regex:
            try:
                pats.append(re.compile(t, flags))
            except re.error:
                pats.append(re.compile(re.escape(t), flags))
        else:
            pats.append(re.compile(re.escape(t), flags))
    return pats

def make_snippet(text: str, pats: List[re.Pattern], total_len: int = 240) -> str:
    s_pos, e_pos = 0, 0
    for p in pats:
        m = p.search(text)
        if m:
            s_pos, e_pos = m.start(), m.end()
            break
    if e_pos == 0:
        s_pos, e_pos = 0, min(len(text), total_len)
    margin = total_len // 2
    left = max(0, s_pos - margin)
    right = min(len(text), e_pos + margin)
    snippet = text[left:right]
    for p in pats:
        try:
            snippet = p.sub(lambda m: f"<mark>{m.group(0)}</mark>", snippet)
        except re.error:
            pass
    if left > 0:
        snippet = "â€¦" + snippet
    if right < len(text):
        snippet = snippet + "â€¦"
    return snippet

def copy_button(text: str, label: str, key: str):
    payload = json.dumps(text, ensure_ascii=False)
    html = f"""
    <button id="{key}" style="
        padding:6px 10px;border-radius:8px;border:1px solid #dadce0;
        background:#fff;cursor:pointer;font-size:0.9rem;">ğŸ“‹ {label}</button>
    <script>
      const btn = document.getElementById("{key}");
      if (btn) {{
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({payload});
            const old = btn.innerText;
            btn.innerText = "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(()=>{{ btn.innerText = old; }}, 1200);
          }} catch(e) {{
            alert("ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: " + e);
          }}
        }});
      }}
    </script>
    """
    st.components.v1.html(html, height=38)

# ---------- OpenAI å‘¼ã³å‡ºã— ----------
def _use_mct(model_name: str) -> bool:
    m = (model_name or "").lower()
    return m.startswith("gpt-5") or m.startswith("o")

def _chat_complete_safely(client: OpenAI, *, model: str, temperature: float,
                          limit_tokens: int, system_prompt: str, user_prompt: str):
    def _call(use_mct: bool):
        payload = {
            "model": model,
            "temperature": float(temperature),
            "messages": [
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": user_prompt},
            ],
        }
        if use_mct:
            payload["max_completion_tokens"] = int(limit_tokens)
        else:
            payload["max_tokens"] = int(limit_tokens)
        return client.chat.completions.create(**payload)
    prefer_mct = _use_mct(model)
    try:
        return _call(prefer_mct)
    except Exception:
        return _call(not prefer_mct)

def _extract_text_from_chat(resp_obj) -> str:
    try:
        content = resp_obj.choices[0].message.content
        return content or ""
    except Exception:
        return ""

# Responses API for gpt-5ï¼ˆSDKå·®ç•°ã®ãŸã‚ response_format ã¯æ¸¡ã•ãªã„ï¼‰
def _responses_generate(client: OpenAI, *, model: str, temperature: float,
                        max_output_tokens: int, system_prompt: str, user_prompt: str):
    return client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system_prompt.strip()},
            {"role": "user",   "content": user_prompt},
        ],
        temperature=float(temperature),
        max_output_tokens=int(max_output_tokens),
    )

def _responses_text(resp) -> str:
    try:
        txt = resp.output_text
        if isinstance(txt, str) and txt.strip():
            return txt
    except Exception:
        pass
    try:
        out = getattr(resp, "output", None)
        if out:
            for item in out:
                if getattr(item, "type", "") == "message":
                    for c in getattr(item, "content", []) or []:
                        t = getattr(c, "text", None)
                        if isinstance(t, str) and t.strip():
                            return t
                t = getattr(item, "text", None)
                if isinstance(t, str) and t.strip():
                    return t
    except Exception:
        pass
    return ""

# ============== ãƒãƒƒãƒåˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆgpt-5-miniå‰æï¼‰ ==============
def _gpt5mini_limits():
    # æ—¢å®šï¼šcontext 128kã€ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒãƒ¼ã‚¸ãƒ³ 2k
    return 128_000, 2_000

def _estimate_prompt_tokens(model: str, sys_prompt: str, user_prompt_prefix: str) -> int:
    return _count_tokens(sys_prompt, model) + _count_tokens(user_prompt_prefix, model)

def _split_into_batches(snippets: List[str], model: str, input_budget_tokens: int) -> List[List[str]]:
    batches: List[List[str]] = []
    current: List[str] = []
    cur_tokens = 0
    for s in snippets:
        t = _count_tokens(s, model)
        if t > input_budget_tokens:
            s = _truncate_by_tokens(s, input_budget_tokens, model)
            t = _count_tokens(s, model)
        if cur_tokens + t <= input_budget_tokens:
            current.append(s); cur_tokens += t
        else:
            if current:
                batches.append(current)
            current = [s]; cur_tokens = t
    if current:
        batches.append(current)
    return batches

def _render_snippets(snips: List[str]) -> str:
    return "\n\n".join(snips)

# ============== å®Ÿè¡Œ ==============
if go:
    if not sel_shards:
        st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    pats = compile_terms(query, use_regex=use_regex, case_sensitive=case_sensitive)
    if not pats:
        st.warning("æ¤œç´¢èªãŒç©ºã§ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    rows: List[Dict[str,Any]] = []
    total_scanned = 0

    for sid in sel_shards:
        meta_path = base_dir / sid / "meta.jsonl"
        for obj in iter_jsonl(meta_path):
            total_scanned += 1
            yr = obj.get("year", None)
            if isinstance(yr, int):
                if year_min and yr < year_min: continue
                if year_max and year_max < 9999 and yr > year_max: continue
            if file_filter and file_filter.lower() not in str(obj.get("file","")).lower():
                continue

            text = str(obj.get("text",""))
            text_for_match = normalize_ja_text(text) if norm_body else text

            ok = all(p.search(text_for_match) for p in pats) if bool_mode == "AND" \
                 else any(p.search(text_for_match) for p in pats)
            if not ok:
                continue

            score = sum(len(list(p.finditer(text_for_match))) for p in pats)
            rows.append({
                "file": obj.get("file"),
                "year": obj.get("year"),
                "page": obj.get("page"),
                "shard_id": obj.get("shard_id", sid),
                "chunk_id": obj.get("chunk_id"),
                "chunk_index": obj.get("chunk_index"),
                "score": int(score),
                "text": make_snippet(text, pats, total_len=int(snippet_len)),
            })

            if len(rows) >= int(max_rows):
                break
        if len(rows) >= int(max_rows):
            break

    if not rows:
        st.warning("ãƒ’ãƒƒãƒˆãªã—ã€‚æ¤œç´¢èªã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    df = pd.DataFrame(rows).sort_values(["score","year","file","page"], ascending=[False, True, True, True])

    st.success(f"ãƒ’ãƒƒãƒˆ {len(df):,d} ä»¶ / èµ°æŸ» {total_scanned:,d} ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆä¸Šä½ã®ã¿è¡¨ç¤ºï¼‰")

    show_order = [c for c in show_cols if c in df.columns]
    non_text_cols = [c for c in show_order if c != "text"]
    st.dataframe(df[non_text_cols], use_container_width=True, height=420)

    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ“¥ CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name="keyword_hits.csv", mime="text/csv")

    # ============== ç”Ÿæˆï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚ˆã‚Šå‰ã«è¡¨ç¤ºï¼‰ ==============
    if gen_enabled:
        st.divider()
        st.subheader("ğŸ§  ç”Ÿæˆè¦ç´„ï¼ˆOpenAIï¼‰")

        # 1) ä¸Šä½ N ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆHTMLé™¤å» & 1ã¤ã²ã¨ã¤ã« Source ãƒ©ãƒ™ãƒ«ï¼‰
        take_n = int(topn_snippets)
        selected = df.head(take_n).copy()

        labelled_snips: List[str] = []
        for _, r in selected.iterrows():
            src = f"{r.get('file')} p.{r.get('page')} (score={r.get('score')})"
            snip = strip_html(str(r.get("text","")))
            labelled_snips.append(f"---\n# Source: {src}\n{snip}")

        # 2) äº‹å‰è¦‹ç©ï¼ˆã¾ãšã¯è¡¨ç¤ºã ã‘è¡Œã†ï¼‰
        model_hint = model
        context_limit, safety_margin = _gpt5mini_limits() if _is_gpt5(model) else (128_000, 1_000)
        user_prefix = user_prompt_tpl.replace("{snippets}", "").format(query=query, snippets="")
        prompt_overhead = _estimate_prompt_tokens(model_hint, sys_prompt, user_prefix)
        snippets_tokens = sum(_count_tokens(s, model_hint) for s in labelled_snips)
        want_output = int(max_tokens)
        needed_total = prompt_overhead + snippets_tokens + want_output + safety_margin

        st.caption(f"è¦‹ç©: prompt+snips={prompt_overhead + snippets_tokens:,} / "
                   f"å‡ºåŠ›ä¸Šé™={want_output:,} / safety={safety_margin:,} / "
                   f"context_limit~{context_limit:,}")

        will_overflow = needed_total > context_limit
        if will_overflow:
            over = needed_total - context_limit
            st.error(f"âš ï¸ å…¥åŠ›ãŒå¤§ãã™ãã¾ã™ï¼ˆæ¨å®š {needed_total:,} tok ãŒä¸Šé™ {context_limit:,} tok ã‚’ {over:,} tok è¶…éï¼‰ã€‚", icon="âš ï¸")
            if not auto_batch:
                st.info("å¯¾å‡¦: â‘  ç”Ÿæˆã«ä½¿ã†ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°ã‚’æ¸›ã‚‰ã™ â‘¡ å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’ä¸‹ã’ã‚‹ â‘¢ ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ï¼ˆå¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰")

        # 3) å®Ÿè¡Œå‰ç¢ºèª UIï¼ˆå¸¸ã«ç¢ºèªï¼‰
        with st.form("run_summary_form"):
            agree = st.checkbox("ã“ã®æ¡ä»¶ã§è¦ç´„ã‚’å®Ÿè¡Œã—ã¦ã‚ˆã„ï¼ˆå®Ÿè¡Œå‰ã®æœ€çµ‚ç¢ºèªï¼‰", value=False)
            run_summary = st.form_submit_button("ğŸ§  è¦ç´„ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)
        if not (agree and run_summary):
            st.info("è¦ç´„ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦ã€ğŸ§  è¦ç´„ã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            # ==== ã“ã“ã‹ã‚‰å®Ÿéš›ã®ç”Ÿæˆ ====
            client = OpenAI(api_key=OPENAI_API_KEY)

            if will_overflow and not auto_batch:
                st.stop()

            # --- è‡ªå‹•ãƒãƒƒãƒè¦ç´„ ---
            if will_overflow and auto_batch:
                st.info("ğŸª„ è‡ªå‹•ãƒãƒƒãƒè¦ç´„ã‚’é–‹å§‹ã—ã¾ã™ï¼šã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¤‡æ•°ãƒãƒƒãƒã«åˆ†å‰² â†’ å„ãƒãƒƒãƒè¦ç´„ â†’ æœ€çµ‚çµ±åˆã€‚")
                per_batch_budget = max(1000, context_limit - want_output - safety_margin - prompt_overhead)
                batches = _split_into_batches(labelled_snips, model_hint, per_batch_budget)
                st.caption(f"ãƒãƒƒãƒæ•°: {len(batches)} / ãƒãƒƒãƒå…¥åŠ›ãƒã‚¸ã‚§ãƒƒãƒˆ~{per_batch_budget:,} tok")

                batch_summaries: List[str] = []
                for bi, batch in enumerate(batches, start=1):
                    batch_snips = _render_snippets(batch)
                    user_prompt = user_prompt_tpl.format(query=query, snippets=batch_snips)
                    approx_in = _count_tokens(user_prompt, model_hint) + _count_tokens(sys_prompt, model_hint)
                    if verbose_log:
                        st.write(f"Batch {bi}/{len(batches)}: å…¥åŠ›æ¨å®š ~{approx_in:,} tok / å‡ºåŠ›ä¸Šé™ {want_output:,} tok")
                    with st.spinner(f"Batch {bi}/{len(batches)} ã‚’è¦ç´„ä¸­â€¦"):
                        if _is_gpt5(model):
                            resp = _responses_generate(
                                client, model=model, temperature=float(temperature),
                                max_output_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=user_prompt
                            )
                            text = _responses_text(resp)
                            finish = getattr(resp, "finish_reason", None)
                            usage  = getattr(resp, "usage", None)
                        else:
                            resp = _chat_complete_safely(
                                client, model=model, temperature=float(temperature),
                                limit_tokens=want_output, system_prompt=sys_prompt,
                                user_prompt=user_prompt
                            )
                            text = _extract_text_from_chat(resp)
                            try:
                                finish = resp.choices[0].finish_reason
                                usage = resp.usage
                            except Exception:
                                finish, usage = None, None
                    if verbose_log:
                        try:
                            ct = getattr(usage, "completion_tokens", None) or usage.get("completion_tokens")
                            pt = getattr(usage, "prompt_tokens", None) or usage.get("prompt_tokens")
                            st.caption(f"Batch {bi}: finish={finish} / tokens(c/p)={ct}/{pt}")
                        except Exception:
                            st.caption(f"Batch {bi}: finish={finish}")
                    batch_summaries.append(f"[Batch {bi} è¦ç´„]\n{text.strip()}")

                # æœ€çµ‚çµ±åˆ
                st.info("ğŸ§© ãƒãƒƒãƒè¦ç´„ã‚’çµ±åˆã—ã¦ã„ã¾ã™â€¦")
                joined_batch = "\n\n".join(batch_summaries)
                prefix = "ä»¥ä¸‹ã¯è¤‡æ•°ãƒãƒƒãƒã®è¦ç´„ã§ã™ã€‚é‡è¤‡ã‚’çµ±åˆã—ã€çŸ›ç›¾ã‚’è§£æ¶ˆã—ã€æœ€çµ‚ã®ç°¡æ½”ãªæ—¥æœ¬èªã‚µãƒãƒªã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
                integration_prompt = f"{prefix}{joined_batch}"
                approx_integration = _count_tokens(integration_prompt, model_hint) + _count_tokens(sys_prompt, model_hint)
                if approx_integration + want_output + safety_margin > context_limit:
                    keep = context_limit - want_output - safety_margin - _count_tokens(sys_prompt, model_hint)
                    integration_prompt = _truncate_by_tokens(integration_prompt, max(1000, keep), model_hint)
                    approx_integration = _count_tokens(integration_prompt, model_hint) + _count_tokens(sys_prompt, model_hint)
                    st.caption(f"çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒªãƒ : å…¥åŠ›æ¨å®š ~{approx_integration:,} tok")

                with st.spinner("æœ€çµ‚çµ±åˆè¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                    if _is_gpt5(model):
                        final_resp = _responses_generate(
                            client, model=model, temperature=float(temperature),
                            max_output_tokens=want_output, system_prompt=sys_prompt,
                            user_prompt=integration_prompt
                        )
                        final_text = _responses_text(final_resp)
                        finish_final = getattr(final_resp, "finish_reason", None)
                        usage_final  = getattr(final_resp, "usage", None)
                    else:
                        final_resp = _chat_complete_safely(
                            client, model=model, temperature=float(temperature),
                            limit_tokens=want_output, system_prompt=sys_prompt,
                            user_prompt=integration_prompt
                        )
                        final_text = _extract_text_from_chat(final_resp)
                        try:
                            finish_final = final_resp.choices[0].finish_reason
                            usage_final  = final_resp.usage
                        except Exception:
                            finish_final, usage_final = None, None

                st.markdown(final_text if final_text.strip() else "_ï¼ˆçµ±åˆçµæœãŒç©ºã§ã—ãŸï¼‰_")
                cols = st.columns(3)
                with cols[0]:
                    st.caption(f"finish_reason: **{finish_final}**")
                with cols[1]:
                    try:
                        ct = getattr(usage_final, "completion_tokens", None) or usage_final.get("completion_tokens")
                        pt = getattr(usage_final, "prompt_tokens", None) or usage_final.get("prompt_tokens")
                        st.caption(f"tokens (c/p): **{ct}/{pt}**")
                    except Exception:
                        pass
                with cols[2]:
                    st.caption(f"model: **{model}**")

            else:
                # --- å˜ç™ºã§åã¾ã‚‹å ´åˆ ---
                snippets_text = _render_snippets(labelled_snips)
                user_prompt = user_prompt_tpl.format(query=query, snippets=snippets_text)
                approx_total = _count_tokens(user_prompt, model_hint) + _count_tokens(sys_prompt, model_hint)
                st.caption(f"ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: ~{approx_total:,} tok / å‡ºåŠ›ä¸Šé™ {want_output:,} tokï¼‰")

                with st.spinner("è¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                    if _is_gpt5(model):
                        raw = _responses_generate(
                            client, model=model, temperature=float(temperature),
                            max_output_tokens=want_output, system_prompt=sys_prompt,
                            user_prompt=user_prompt
                        )
                        out_text = _responses_text(raw)
                        finish = getattr(raw, "finish_reason", None)
                        usage  = getattr(raw, "usage", None)
                    else:
                        raw = _chat_complete_safely(
                            client, model=model, temperature=float(temperature),
                            limit_tokens=want_output, system_prompt=sys_prompt,
                            user_prompt=user_prompt
                        )
                        out_text = _extract_text_from_chat(raw)
                        try:
                            finish = raw.choices[0].finish_reason
                            usage  = raw.usage
                        except Exception:
                            finish, usage = None, None

                if (not out_text or not out_text.strip()) and finish == "length":
                    st.info("ğŸ” å‡ºåŠ›ãŒæ‰“ã¡åˆ‡ã‚‰ã‚ŒãŸãŸã‚ã€ç¶šãã®ç”Ÿæˆã‚’åŠ ãˆã¦ã„ã¾ã™â€¦")
                    cont_prompt = user_prompt + "\n\nã€ç¶šãã®ã¿ã‚’ç°¡æ½”ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ã€‘"
                    if _is_gpt5(model):
                        raw2 = _responses_generate(
                            client, model=model, temperature=float(temperature),
                            max_output_tokens=want_output, system_prompt=sys_prompt,
                            user_prompt=cont_prompt
                        )
                        out_text2 = _responses_text(raw2)
                    else:
                        raw2 = _chat_complete_safely(
                            client, model=model, temperature=float(temperature),
                            limit_tokens=want_output, system_prompt=sys_prompt,
                            user_prompt=cont_prompt
                        )
                        out_text2 = _extract_text_from_chat(raw2)
                    out_text = (out_text or "") + ("\n" + out_text2 if out_text2 else "")

                st.markdown(out_text if (out_text and out_text.strip()) else "_ï¼ˆæœ¬æ–‡ãŒè¿”ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰_")
                cols = st.columns(3)
                with cols[0]:
                    st.caption(f"finish_reason: **{finish}**")
                with cols[1]:
                    try:
                        ct = getattr(usage, "completion_tokens", None) or usage.get("completion_tokens")
                        pt = getattr(usage, "prompt_tokens", None) or usage.get("prompt_tokens")
                        st.caption(f"tokens (c/p): **{ct}/{pt}**")
                    except Exception:
                        pass
                with cols[2]:
                    st.caption(f"model: **{model}**")

    # ============== ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆæ—¢å®šã§ç•³ã‚€ï¼‰ ==============
    if "text" in show_order:
        st.divider()
        with st.expander("ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰", expanded=False):
            for i, row in df.head(200).iterrows():
                colA, colB = st.columns([4,1])
                with colA:
                    st.markdown(
                        f"**{row.get('file')}**  year={row.get('year')}  p.{row.get('page')}  "
                        f"score={row.get('score')}",
                        help=row.get("chunk_id")
                    )
                    st.markdown(row.get("text",""), unsafe_allow_html=True)
                with colB:
                    copy_button(text=str(row.get("file")), label="year/file ã‚’ã‚³ãƒ”ãƒ¼", key=f"cpy_{i}")

else:
    st.info("å·¦ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã¨æ¡ä»¶ã‚’é¸ã³ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€æ¤œç´¢ã‚’å®Ÿè¡Œã€ã—ã¦ãã ã•ã„ã€‚")
