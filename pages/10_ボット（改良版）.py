# ============================================
# å¤‰æ›´ç‚¹ï¼ˆã“ã®ç‰ˆã§ã®ä¿®æ­£ãƒ»è¿½åŠ ï¼‰
# --------------------------------------------
# 1) OpenAIã‚­ãƒ¼æœªè¨­å®šæ™‚ã®å®‰å…¨åŒ–:
#    - åŸ‹ã‚è¾¼ã¿ backend=openai ã‹ã¤ OPENAI_API_KEY æœªè¨­å®šãªã‚‰è‡ªå‹•ã§ 'local' ã«åˆ‡æ›¿ã€‚
# 2) vdb.search() ã®ã‚¹ã‚³ã‚¢å‘ãã‚’æ˜ç¤º:
#    - return_="similarity" ã‚’å¸¸ã«æŒ‡å®šï¼ˆã€Œå¤§ãã„ã»ã©è‰¯ã„ã€å‰æã§ãƒ’ãƒ¼ãƒ—/ã‚½ãƒ¼ãƒˆï¼‰ã€‚
# 3) å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã®å®‰å…¨æ¯”è¼ƒ:
#    - NFKC/å¤§å°æ–‡å­—/åŒºåˆ‡ã‚Š(\\ â†’ /) æ­£è¦åŒ–ã—ã¦ä¸€è‡´åˆ¤å®šï¼ˆ_norm_pathï¼‰ã€‚
# 4) ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ [[files: ...]] æŠ½å‡º/é™¤å»ã®å …ç‰¢åŒ–:
#    - ä½™ç™½è¨±å®¹ãƒ»å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ã®æ­£è¦è¡¨ç¾ã«å¼·åŒ–ã€‚
# 5) send_now åˆ¤å®šã®å®‰å…¨åŒ–:
#    - go = go_click or bool(locals().get("send_now"))
# 6) ã‚³ãƒ¡ãƒ³ãƒˆã§ã€Œscore ã¯ similarityã€ã‚’æ˜ç¤ºã—å°†æ¥ã®æ··åœ¨ã‚’æŠ‘æ­¢ã€‚
# ============================================

# pages/09_ãƒœãƒƒãƒˆ.py
# ------------------------------------------------------------
# ğŸ’¬ Internal Bot (RAG, No-FAISS) â€” ä»¥å‰ã®ãƒœãƒƒãƒˆé¢¨UI + ã‚·ãƒ£ãƒ¼ãƒ‰æ¨ªæ–­æ¤œç´¢ç‰ˆï¼ˆå‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šå¯¾å¿œï¼‰
# - data/vectorstore/<backend>/<shard_id>/ ã‚’æ¨ªæ–­ã—ã¦ top-k ãƒãƒ¼ã‚¸
# - æ—¥æœ¬èªã®ã€Œ1æ–‡å­—ã”ã¨ç©ºç™½ã€å•é¡Œã«å¯¾å¿œï¼ˆnormalize_ja_textï¼‰
# - OpenAI ã‚­ãƒ¼æœªè¨­å®šæ™‚ã¯è‡ªå‹•ã§ Retrieve-only ã«åˆ‡æ›¿
# - vdb.search ã®æˆ»ã‚Šå€¤ãŒ (row_idx, score, meta) / (score, meta) ã©ã¡ã‚‰ã§ã‚‚å—ã‘ã‚‹
# - å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼è³ªå•å†… [[files: 2025/a.pdf, 2024/b.pdf]] ã§æŒ‡å®šå¯èƒ½
# ------------------------------------------------------------

from __future__ import annotations
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any
import random
import heapq
import unicodedata
import re

import streamlit as st
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

from rag_utils import EmbeddingStore, NumpyVectorDB, build_prompt

# ========= ãƒ‘ã‚¹ =========
APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
VS_ROOT  = DATA_DIR / "vectorstore"

# ========= æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ– =========
CJK = r"\u3040-\u309F\u30A0-\u30FF\uFF65-\uFF9F\u4E00-\u9FFF\u3400-\u4DBF"
PUNC = r"ã€ã€‚ãƒ»ï¼Œï¼ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€Œã€ã€ã€ã€ˆã€‰ã€Šã€‹ã€ã€‘"

_cjk_cjk_space = re.compile(fr"(?<=[{CJK}])\s+(?=[{CJK}])")
_space_before_punc = re.compile(fr"\s+(?=[{PUNC}])")
_space_after_open = re.compile(fr"(?<=[ï¼ˆï¼»ï½›ã€Œã€ã€ˆã€Šã€])\s+")
_space_before_close = re.compile(fr"\s+(?=[ï¼‰ï¼½ï½ã€ã€ã€‰ã€‹ã€‘])")
_multi_space = re.compile(r"[ \t\u3000]+")

def normalize_ja_text(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _cjk_cjk_space.sub("", s)
    s = _space_before_punc.sub("", s)
    s = _space_after_open.sub("", s)
    s = _space_before_close.sub("", s)
    s = _multi_space.sub(" ", s)
    return s.strip()

# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _count_tokens(text: str, model_hint: str = "cl100k_base") -> int:
    try:
        import tiktoken
        try:
            enc = tiktoken.encoding_for_model(model_hint)
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception:
        return max(1, int(len(text) / 4))

def _fmt_source(meta: Dict[str, Any]) -> str:
    f = str(meta.get("file", "") or "")
    p = meta.get("page", None)
    cid = str(meta.get("chunk_id", "") or "")
    base = f"{f} p.{int(p)}" if (f and p is not None) else (f or "(unknown)")
    return f"{base} ({cid})" if cid else base

def _format_contexts_for_prompt(hits: List[Tuple[int, float, Dict[str, Any]]]) -> List[str]:
    labeled = []
    for i, (idx, score, meta) in enumerate(hits, 1):
        txt = str(meta.get("text", "") or "")
        file_label = _fmt_source(meta)
        labeled.append(f"[S{i}] {txt}\n[meta: {file_label} / score={float(score):.3f}]")
    return labeled

def _list_shard_dirs(backend: str) -> List[Path]:
    base = VS_ROOT / backend
    if not base.exists(): return []
    return sorted([p for p in base.iterdir() if p.is_dir()])

def _norm_path(s: str) -> str:
    """å¹´/ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸€è‡´ã‚’å®‰å®šã•ã›ã‚‹ãŸã‚ã®æ­£è¦åŒ–"""
    s = unicodedata.normalize("NFKC", s or "")
    s = s.strip().replace("\\", "/")
    return s.lower()

# =========================
# ã‚µãƒ³ãƒ—ãƒ«è³ªå•
# =========================
SAMPLES = {
    "è£œåŠ©é‡‘": [
        "ã“ã®è³‡æ–™ã®è£œåŠ©å¯¾è±¡çµŒè²»ãƒ»å¯¾è±¡å¤–çµŒè²»ã®è¦ä»¶ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚[å‡ºå…¸ã‚’æ˜è¨˜]",
        "ã“ã®è³‡æ–™ã®è£œåŠ©å¯¾è±¡è€…ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚[å‡ºå…¸ã‚’æ˜è¨˜]",
        "ç”³è«‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ä¸»è¦ãªç· åˆ‡æ—¥ã‚’ä¸€è¦§ã§æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ä¸­å°ä¼æ¥­è€…ãƒ»å°è¦æ¨¡äº‹æ¥­è€…ã®å®šç¾©ã‚’æ¯”è¼ƒè¡¨ã§ç¤ºã—ã¦ãã ã•ã„ã€‚",
    ],
    "ç’°å¢ƒã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³": [
        "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ”¹å®šã®èƒŒæ™¯ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚",
        "ç’°å¢ƒå ±å‘Šã®è¨˜è¼‰äº‹é …ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
    ],
    "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼": [
        "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼ã®å‚åŠ è€…ã¯ã€‚",
        "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼ã«ãŠã„ã¦æ”¾å°„ç·šã®å¥åº·ç®¡ç†ã«ã¤ã„ã¦å‡ºãŸæ„è¦‹ã¯ï¼Ÿã€‚",
        "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼ã«ãŠã„ã¦æ½®æµç™ºé›»ã«ã¤ã„ã¦å‡ºãŸæ„è¦‹ã¯ï¼Ÿ"
    ],
     "æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆ": [
        "2023å¹´10æœˆ30æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã®å‡ºå¸­å§”å“¡",
        "2024å¹´1æœˆ22æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã®æ”¿åºœã‹ã‚‰ã®å‡ºå¸­è€…ã¯",
        "2024å¹´1æœˆ23æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã®æ”¿åºœã‹ã‚‰ã®å‡ºå¸­è€…ã‚’å…¨ã¦æ•™ãˆã¦ï¼ˆæ…é‡ã«è€ƒãˆã¦2024å¹´1æœˆ22 æ—¥ã§ã¯ãªã2024å¹´1æœˆ23æ—¥ã§ã™ï¼‰",
        "2023å¹´10æœˆ30æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã§è­°è«–ã•ã‚ŒãŸç‚ºæ›¿å¸‚å ´å‹•å‘ã¯"
    ],
}
ALL_SAMPLES = [q for qs in SAMPLES.values() for q in qs]

# =========================
# Streamlit æº–å‚™ï¼ˆä»¥å‰ã®ãƒœãƒƒãƒˆé¢¨UIï¼‰
# =========================
load_dotenv()
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Internal Bot (RAG, Shards)")

if "q" not in st.session_state:
    st.session_state.q = ""

def _set_question(text: str):
    st.session_state.q = text

with st.sidebar:
    st.header("è¨­å®š")

    # åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
    embed_backend_label = st.radio(
        "åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆä½œæˆæ™‚ã¨ä¸€è‡´ï¼‰",
        ["local (sentence-transformers)", "openai"],
        index=1
    )
    embed_backend = "openai" if embed_backend_label.startswith("openai") else "local"

    # æ¤œç´¢ä»¶æ•°
    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    # å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«
    label_to_value = {"ç°¡æ½”":"concise","æ¨™æº–":"standard","è©³ç´°":"detailed","è¶…è©³ç´°":"very_detailed"}
    detail_label = st.selectbox("è©³ã—ã•", list(label_to_value.keys()), index=2)
    detail = label_to_value[detail_label]

    cite = st.checkbox("å‡ºå…¸ã‚’è§’æ‹¬å¼§ã§å¼•ç”¨ï¼ˆ[S1] ç­‰ï¼‰", value=True)
    max_tokens = st.slider("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°", 256, 4000, 1200, 64)
    temperature = st.slider("temperature", 0.0, 1.0, 0.2, 0.05)

    answer_backend = st.radio("å›ç­”ãƒ¢ãƒ‡ãƒ«", ["OpenAI", "Retrieve-only"], index=0)
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)

    # ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠ
    st.divider()
    st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰")
    shard_dirs_all = _list_shard_dirs(embed_backend)
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # --- å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰: ä¾‹ "2025/foo.pdf, 2024/bar.pdf"
    st.caption("ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§æ¤œç´¢ã—ãŸã„å ´åˆã¯ã€å¹´/ãƒ•ã‚¡ã‚¤ãƒ«å ã§ã‚«ãƒ³ãƒåŒºåˆ‡ã‚ŠæŒ‡å®šï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    # OpenAI ã‚­ãƒ¼ãŒç„¡ã„ãªã‚‰ã“ã“ã§è­¦å‘Šè¡¨ç¤º
    has_key = bool(os.getenv("OPENAI_API_KEY"))
    if answer_backend == "OpenAI" and not has_key:
        st.error("OPENAI_API_KEY ãŒ .env ã«ã‚ã‚Šã¾ã›ã‚“ï¼ˆè‡ªå‹•ã§ã€Retrieve-onlyã€ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ï¼‰ã€‚")

    st.divider()
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample = ""
    if cat != "ï¼ˆæœªé¸æŠï¼‰":
        sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES[cat])
    else:
        st.caption("ã‚«ãƒ†ã‚´ãƒªã‚’é¸ã¶ã‹ã€ä¸‹ã®ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥ã‚’ä½¿ãˆã¾ã™ã€‚")

    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button("â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ", use_container_width=True,
                  disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")), on_click=lambda: _set_question(sample))
    with cols_demo[1]:
        st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥", use_container_width=True,
                  on_click=lambda: _set_question(random.choice(ALL_SAMPLES)))

    send_now = st.button("ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡", use_container_width=True,
                         disabled=(st.session_state.q.strip() == ""))

    st.divider()
    # æ–™é‡‘è¨ˆç®—
    st.subheader("ğŸ’µ æ–™é‡‘è¨ˆç®—ï¼ˆç·¨é›†å¯ï¼‰")
    fx_rate = st.number_input("ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ (JPY/USD)", min_value=50.0, max_value=500.0, value=150.0, step=0.5)
    chat_in_price = st.number_input("Chat å…¥åŠ›å˜ä¾¡ (USD / 1K tok)", min_value=0.0, value=0.00015, step=0.00001, format="%.5f")
    chat_out_price = st.number_input("Chat å‡ºåŠ›å˜ä¾¡ (USD / 1K tok)", min_value=0.0, value=0.00060, step=0.00001, format="%.5f")
    emb_price = st.number_input("Embedding å˜ä¾¡ (USD / 1K tok)", min_value=0.0, value=0.00002, step=0.00001, format="%.5f")

# å…¥åŠ›æ¬„
q = st.text_area("è³ªå•ã‚’å…¥åŠ›", value=st.session_state.q, placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦", height=100)
if q != st.session_state.q:
    st.session_state.q = q

go_click = st.button("é€ä¿¡", type="primary")
go = go_click or bool(locals().get("send_now"))

# =========================
# å®Ÿè¡Œ
# =========================
if go and st.session_state.q.strip():

    # --- åŸ‹ã‚è¾¼ã¿ backend ã®å®‰å…¨åŒ–ï¼ˆOpenAIã‚­ãƒ¼æœªè¨­å®šãªã‚‰ local ã«åˆ‡æ›¿ï¼‰
    if embed_backend == "openai" and not os.getenv("OPENAI_API_KEY"):
        st.warning("åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒ openai ã§ã™ãŒ OPENAI_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ã€'local' ã«è‡ªå‹•åˆ‡æ›¿ã—ã¾ã™ã€‚")
        embed_backend = "local"

    # --- ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æŒ‡å®šã®å–ã‚Šå‡ºã—: [[files: 2025/aaa.pdf, 2024/bbb.pdf]]
    inline = re.search(r"\[\[\s*files\s*:\s*([^\]]+)\]\]", st.session_state.q, flags=re.IGNORECASE)
    inline_files = set()
    if inline:
        inline_files = {s.strip() for s in inline.group(1).split(",") if s.strip()}

    # UIå…¥åŠ›ã¨çµ±åˆï¼ˆã©ã¡ã‚‰ã‹/ä¸¡æ–¹OKï¼‰â†’ æ­£è¦åŒ–
    effective_whitelist_raw = set(file_whitelist) | set(inline_files)
    effective_whitelist = {_norm_path(x) for x in effective_whitelist_raw}

    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¿ã‚°ã¯æœ¬æ–‡ã‹ã‚‰é™¤å» â†’ æ­£è¦åŒ–ã¸
    clean_q = re.sub(r"\[\[\s*files\s*:[^\]]+\]\]", "", st.session_state.q, flags=re.IGNORECASE).strip()

    # â˜… ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ï¼ˆæ—¥æœ¬èªã®1å­—ç©ºç™½å¯¾ç­–ï¼‰
    question = normalize_ja_text(clean_q)

    # ä½¿ã†ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ«ãƒ¼ãƒˆï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰æ¨ªæ–­ï¼‰
    vs_backend_dir = VS_ROOT / embed_backend
    if not vs_backend_dir.exists():
        st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã« **ãƒ™ã‚¯ãƒˆãƒ«åŒ–** ã‚’åŒã˜ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs_all = _list_shard_dirs(embed_backend)  # å¿µã®ãŸã‚å†å–å¾—ï¼ˆå®Ÿè¡Œæ™‚å¤‰æ›´å¯¾ç­–ï¼‰
    selected = [vs_backend_dir / s for s in target_shards] if target_shards else [vs_backend_dir / p.name for p in shard_dirs_all]
    shard_dirs = [p for p in selected if p.is_dir() and (p / "vectors.npy").exists()]

    if not shard_dirs:
        st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    try:
        # --- æ¤œç´¢ ---
        with st.spinner("æ¤œç´¢ä¸­â€¦"):
            estore = EmbeddingStore(backend=embed_backend)
            emb_tokens = _count_tokens(question, model_hint="text-embedding-3-small") if embed_backend == "openai" else 0
            qv = estore.embed([question]).astype("float32")  # shape=(1, d)

            # å„ã‚·ãƒ£ãƒ¼ãƒ‰ã§ top_k ã‚’å–ã‚Šã€å…¨ä½“ã§ãƒãƒ¼ã‚¸ï¼ˆæœ€å°ãƒ’ãƒ¼ãƒ—ï¼‰
            K = int(top_k)
            heap: List[Tuple[float, Tuple[int, Dict[str, Any]]]] = []  # (score, (row_idx, meta))

            for shp in shard_dirs:
                try:
                    vdb = NumpyVectorDB(shp)  # metric ã®æ—¢å®šã¯ rag_utils å´ã«ä¾å­˜
                    # é¡ä¼¼åº¦ï¼ˆå¤§ãã„ã»ã©è‰¯ã„ï¼‰ã‚’è¿”ã™å¥‘ç´„ã§å–å¾—ï¼ˆå°†æ¥ã®è·é›¢å®Ÿè£…ã¨æ··åœ¨ã—ãªã„ã‚ˆã†æ˜ç¤ºï¼‰
                    hits = vdb.search(qv, top_k=K, return_="similarity")
                    for h in hits:
                        # æˆ»ã‚Šå€¤ã®æºã‚Œã«å¯¾å¿œ
                        if isinstance(h, tuple) and len(h) == 3:
                            row_idx, score, meta = h
                        elif isinstance(h, tuple) and len(h) == 2:
                            score, meta = h
                            row_idx = -1
                        else:
                            continue

                        md = dict(meta or {})
                        md["shard_id"] = shp.name

                        # â–¼ å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯ã“ã“ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ­£è¦åŒ–å¾Œã®å®Œå…¨ä¸€è‡´ï¼‰
                        if effective_whitelist:
                            if _norm_path(str(md.get("file", ""))) not in effective_whitelist:
                                continue

                        sc = float(score)  # similarityï¼ˆå¤§ãã„ã»ã©è‰¯ã„ï¼‰å‰æ

                        if len(heap) < K:
                            heapq.heappush(heap, (sc, (row_idx, md)))
                        else:
                            if sc > heap[0][0]:
                                heapq.heapreplace(heap, (sc, (row_idx, md)))
                except Exception as e:
                    st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")

            raw_hits = [(rid, sc, md) for sc, (rid, md) in sorted(heap, key=lambda x: x[0], reverse=True)]

        if not raw_hits:
            if effective_whitelist:
                st.warning("æŒ‡å®šã•ã‚ŒãŸå‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                           "ãƒ•ã‚¡ã‚¤ãƒ«åã¨å¹´ï¼ˆä¾‹: 2025/foo.pdfï¼‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚„ Top-K ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ç”»é¢è¡¨ç¤ºç”¨
        contexts_display = []
        for i, (row_idx, score, meta) in enumerate(raw_hits, 1):
            txt = str(meta.get("text", "") or "")
            src_label = _fmt_source(meta)
            contexts_display.append((i, txt, src_label, float(score)))

        # --- å›ç­”ç”Ÿæˆ or Retrieve-only ---
        chat_prompt_tokens = 0
        chat_completion_tokens = 0
        answer = None

        # OpenAI ã‚­ãƒ¼ãŒç„¡ã„ãªã‚‰è‡ªå‹•ã§ Retrieve-only ã«åˆ‡æ›¿
        use_answer_backend = "Retrieve-only" if (answer_backend == "OpenAI" and not os.getenv("OPENAI_API_KEY")) else answer_backend

        if use_answer_backend == "OpenAI":
            with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                labeled_contexts = _format_contexts_for_prompt(raw_hits)
                prompt = build_prompt(
                    question,
                    labeled_contexts,
                    sys_inst=sys_inst,
                    style_hint=detail,
                    cite=cite,
                    strict=True,
                )
                client = OpenAI()
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    temperature=float(temperature),
                    max_tokens=int(max_tokens),
                    messages=[
                        {"role": "system", "content": "Follow the user's instruction carefully and answer in Japanese when possible."},
                        {"role": "user", "content": prompt},
                    ],
                )
                answer = resp.choices[0].message.content
                try:
                    chat_prompt_tokens = int(resp.usage.prompt_tokens or 0)
                    chat_completion_tokens = int(resp.usage.completion_tokens or 0)
                except Exception:
                    chat_prompt_tokens = _count_tokens(prompt, model_hint="gpt-4o-mini")
                    chat_completion_tokens = _count_tokens(answer or "", model_hint="gpt-4o-mini")

            st.subheader("ğŸ§  å›ç­”")
            st.write(answer)
        else:
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")

        # --- æ–™é‡‘è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰ ---
        with st.container():
            emb_cost_usd = (emb_tokens / 1000.0) * float(emb_price) if embed_backend == "openai" else 0.0
            chat_cost_usd = 0.0
            if use_answer_backend == "OpenAI":
                chat_cost_usd = (chat_prompt_tokens / 1000.0) * float(chat_in_price) + \
                                (chat_completion_tokens / 1000.0) * float(chat_out_price)
            total_usd = emb_cost_usd + chat_cost_usd
            total_jpy = total_usd * float(fx_rate)

            st.markdown("### ğŸ’´ ä½¿ç”¨æ–™ã®æ¦‚ç®—")
            cols = st.columns(3)
            with cols[0]:
                st.metric("åˆè¨ˆ (JPY)", f"{total_jpy:,.2f} å††")
                st.caption(f"ç‚ºæ›¿ {float(fx_rate):.2f} JPY/USD")
            with cols[1]:
                st.write("**å†…è¨³ (USD)**")
                st.write(f"- Embedding: `${emb_cost_usd:.6f}` ({emb_tokens} tok)")
                if use_answer_backend == "OpenAI":
                    st.write(f"- Chat å…¥åŠ›: `${(chat_prompt_tokens/1000.0)*float(chat_in_price):.6f}` ({chat_prompt_tokens} tok)")
                    st.write(f"- Chat å‡ºåŠ›: `${(chat_completion_tokens/1000.0)*float(chat_out_price):.6f}` ({chat_completion_tokens} tok)")
                st.write(f"- åˆè¨ˆ: `${total_usd:.6f}`")
            with cols[2]:
                st.write("**å˜ä¾¡ (USD / 1K tok)**")
                st.write(f"- Embedding: `${float(emb_price):.5f}`")
                st.write(f"- Chat å…¥åŠ›: `${float(chat_in_price):.5f}`")
                st.write(f"- Chat å‡ºåŠ›: `${float(chat_out_price):.5f}`")

        # --- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰ ---
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, txt, src_label, score in contexts_display:
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt  # ä½“æ„Ÿè»½é‡åŒ–
                st.markdown(f"**[S{i}] score={score:.3f}**  `{src_label}`\n\n{snippet}")

    except Exception as e:
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚„å›ç­”è¨­å®šã€å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª¿æ•´ã§ãã¾ã™ã€‚")
