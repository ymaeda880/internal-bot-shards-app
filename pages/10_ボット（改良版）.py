# pages/10_ãƒœãƒƒãƒˆï¼ˆæ–°ï¼‰.py

# ============================================
# å¤‰æ›´ç‚¹ï¼ˆã“ã®ç‰ˆã§ã®ä¿®æ­£ãƒ»è¿½åŠ ï¼‰
# --------------------------------------------
# 1) ãƒ¢ãƒ‡ãƒ«é¸æŠã®çµ±ä¸€:
#    - chat/Responses ä¸¡ç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ pricing.py ã®ä¸€è¦§ã‹ã‚‰é¸æŠå¯èƒ½ã«ã€‚
#    - æ—¢å®šãƒ¢ãƒ‡ãƒ«ã‚’ gpt-5-mini ã«è¨­å®šã€‚
# 2) Responsesç³»ï¼ˆgpt-5*, gpt-4.1*ï¼‰ã®API/æŒ™å‹•ã«è‡ªå‹•å¯¾å¿œï¼ˆé‡è¦ãƒã‚°ä¿®æ­£ï¼‰:
#    - âœ… roleåˆ†é›¢ã—ãŸ input ã‚’ä½¿ç”¨ï¼ˆ[{"role":"system"}, {"role":"user"}]ï¼‰ã€‚
#    - âœ… temperature ã‚’é€ã‚‰ãªã„ï¼ˆéå¯¾å¿œã®ãŸã‚ï¼‰ã€‚
#    - âœ… max_tokens ã§ã¯ãªã max_output_tokens ã‚’ä½¿ç”¨ã€‚
#    - âœ… å‡ºåŠ›æŠ½å‡ºã‚’å …ç‰¢åŒ–ï¼ˆresp.output_text â†’ ãƒã‚¹ãƒˆæ§‹é€ ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ã€‚
#    - âœ… UI ã® temperature ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•ã§ disabled ã«ã€‚
# 3) æ–™é‡‘è¨ˆç®—ã®ä¸€å…ƒåŒ–:
#    - config/pricing.py ã® MODEL_PRICES_USDï¼ˆUSD/1Mï¼‰/ EMBEDDING_PRICES_USDï¼ˆUSD/1Kï¼‰/
#      DEFAULT_USDJPY ã‚’ä½¿ç”¨ã€‚
#    - ğŸ†• ã“ã®ãƒšãƒ¼ã‚¸å†…ã§ MODEL_PRICES_USD ã‚’ /1K ã«å¤‰æ›ã—ã¦è¡¨ç¤ºãƒ»è¨ˆç®—ï¼ˆMODEL_PRICES_PER_1Kï¼‰ã€‚
# 4) åŒç‚¹ã‚¹ã‚³ã‚¢æ™‚ã®ãƒ’ãƒ¼ãƒ—æ¯”è¼ƒãƒã‚°å›é¿ï¼ˆã‚¿ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¯ï¼‰ç¶™ç¶šã€‚
# 5) OpenAIã‚­ãƒ¼ã®å–å¾—ã¯ secrets.toml â†’ env ã‚’ç¶™ç¶šã€‚
# 6) æ—¢å­˜æ©Ÿèƒ½ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š [[files: ...]]ã€ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠã€RAGæ¤œç´¢ã€Retrieve-only ç­‰ï¼‰ã¯ç¶­æŒã€‚
# 7) ã‚µãƒ³ãƒ—ãƒ«è³ªå•UIã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ç¶­æŒï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ»é¸æŠãƒ»ãƒ©ãƒ³ãƒ€ãƒ ãƒ»å³é€ä¿¡ï¼‰ã€‚
# 8) ğŸ†• å›ç­”å“è³ªæ”¹å–„: build_prompt(strict=False) ã«ã—ã¦éåº¦ãªã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã‚’æŠ‘åˆ¶ã€‚
# ============================================

from __future__ import annotations
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any
import random
import heapq
from itertools import count
import unicodedata
import re

import streamlit as st
import numpy as np
from openai import OpenAI

from lib.rag_utils import EmbeddingStore, NumpyVectorDB, build_prompt
from config.path_config import PATHS  # VS_ROOT ã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
# ä¾¡æ ¼ã¨ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’ pricing.py ã‹ã‚‰ä½¿ç”¨ï¼ˆMODEL_PRICES_USD ã¯ USD / 1M tokensï¼‰
from config.pricing import MODEL_PRICES_USD, EMBEDDING_PRICES_USD, DEFAULT_USDJPY

# ========= /1M â†’ /1K å¤‰æ› =========
# â€» config/pricing.py ã¯ä»–æ‰€ã§ã‚‚ä½¿ã‚ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã®ã¿ /1K ã«æ›ç®—ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚
MODEL_PRICES_PER_1K: Dict[str, Dict[str, float]] = {
    m: {
        "in": float(p.get("in", 0.0)) / 1000.0,
        "out": float(p.get("out", 0.0)) / 1000.0,
    }
    for m, p in MODEL_PRICES_USD.items()
}

# ========= ãƒ‘ã‚¹ =========
VS_ROOT: Path = PATHS.vs_root  # ä¾‹: <project>/data/vectorstore

# ========= æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ– =========
CJK = r"\u3040-\u309F\u30A0-\u30FF\uFF65-\uFF9F\u4E00-\u9FFF\u3400-\u4DBF"
PUNC = r"ã€ã€‚ãƒ»ï¼Œï¼ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€Œã€ã€ã€ã€ˆã€‰ã€Šã€‹ã€ã€‘"
_cjk_cjk_space = re.compile(fr"(?<=[{CJK}])\s+(?=[{CJK}])")
_space_before_punc = re.compile(fr"\s+(?=[{PUNC}])")
_space_after_open = re.compile(fr"(?<=[ï¼ˆï¼»ï½›ã€Œã€ã€ˆã€Šã€])\s+")
_space_before_close = re.compile(fr"\s+(?=[ï¼‰ï¼½ï½ã€ã€ã€‰ã€‹ã€‘])")
_multi_space = re.compile(r"[ \t\u3000]+")

def normalize_ja_text(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _cjk_cjk_space.sub("", s)
    s = _space_before_punc.sub("", s)
    s = _space_after_open.sub("", s)
    s = _space_before_close.sub("", s)
    s = _multi_space.sub(" ", s)
    return s.strip()

# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _count_tokens(text: str, model_hint: str = "cl100k_base") -> int:
    try:
        import tiktoken
        try:
            enc = tiktoken.encoding_for_model(model_hint)
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception:
        return max(1, int(len(text) / 4))

def _fmt_source(meta: Dict[str, Any]) -> str:
    f = str(meta.get("file", "") or "")
    p = meta.get("page", None)
    cid = str(meta.get("chunk_id", "") or "")
    base = f"{f} p.{int(p)}" if (f and p is not None) else (f or "(unknown)")
    return f"{base} ({cid})" if cid else base

def _list_shard_dirs(backend: str) -> List[Path]:
    base = VS_ROOT / backend
    if not base.exists():
        return []
    return sorted([p for p in base.iterdir() if p.is_dir()])

def _norm_path(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.strip().replace("\\", "/")
    return s.lower()

def _get_openai_api_key() -> str | None:
    # secrets.toml -> env
    try:
        ok = st.secrets.get("openai", {}).get("api_key")
        if ok:
            return str(ok)
    except Exception:
        pass
    return os.getenv("OPENAI_API_KEY")

# ---------- ãƒ¢ãƒ‡ãƒ«åˆ†é¡ ----------
# Responses API ç³»ï¼ˆtemperature éå¯¾å¿œã€max_output_tokens ã‚’ä½¿ã†ï¼‰
RESPONSES_MODELS = [m for m in MODEL_PRICES_PER_1K.keys() if m.startswith("gpt-5") or m.startswith("gpt-4.1")]
# Chat Completions API ç³»ï¼ˆtemperature / max_tokens ä½¿ç”¨å¯ï¼‰
CHAT_MODELS = [m for m in MODEL_PRICES_PER_1K.keys() if m.startswith("gpt-4o") or m.startswith("gpt-3.5")]

def _use_responses_api(model_name: str) -> bool:
    return model_name in RESPONSES_MODELS

# =========================
# ã‚µãƒ³ãƒ—ãƒ«è³ªå•
# =========================
SAMPLES = {
    "è£œåŠ©é‡‘": [
        "ã“ã®è³‡æ–™ã®è£œåŠ©å¯¾è±¡çµŒè²»ãƒ»å¯¾è±¡å¤–çµŒè²»ã®è¦ä»¶ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚[å‡ºå…¸ã‚’æ˜è¨˜]",
        "ã“ã®è³‡æ–™ã®è£œåŠ©å¯¾è±¡è€…ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚[å‡ºå…¸ã‚’æ˜è¨˜]",
        "ç”³è«‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ä¸»è¦ãªç· åˆ‡æ—¥ã‚’ä¸€è¦§ã§æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ä¸­å°ä¼æ¥­è€…ãƒ»å°è¦æ¨¡äº‹æ¥­è€…ã®å®šç¾©ã‚’æ¯”è¼ƒè¡¨ã§ç¤ºã—ã¦ãã ã•ã„ã€‚",
    ],
    "ç’°å¢ƒã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³": [
        "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ”¹å®šã®èƒŒæ™¯ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚",
        "ç’°å¢ƒå ±å‘Šã®è¨˜è¼‰äº‹é …ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
    ],
    "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼": [
        "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼ã®å‚åŠ è€…ã¯ã€‚",
        "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼ã«ãŠã„ã¦æ”¾å°„ç·šã®å¥åº·ç®¡ç†ã«ã¤ã„ã¦å‡ºãŸæ„è¦‹ã¯ï¼Ÿã€‚",
        "ä»¤å’Œï¼—å¹´åº¦ç’°å¢ƒçœè¡Œæ”¿äº‹æ¥­ãƒ¬ãƒ’ã‚™ãƒ¥ãƒ¼ã«ãŠã„ã¦æ½®æµç™ºé›»ã«ã¤ã„ã¦å‡ºãŸæ„è¦‹ã¯ï¼Ÿ"
    ],
     "æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆ": [
        "2023å¹´10æœˆ30æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã®å‡ºå¸­å§”å“¡",
        "2024å¹´1æœˆ22æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã®æ”¿åºœã‹ã‚‰ã®å‡ºå¸­è€…ã¯",
        "2024å¹´1æœˆ23æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã®æ”¿åºœã‹ã‚‰ã®å‡ºå¸­è€…ã‚’å…¨ã¦æ•™ãˆã¦ï¼ˆæ…é‡ã«è€ƒãˆã¦2024å¹´1æœˆ22 æ—¥ã§ã¯ãªã2024å¹´1æœˆ23æ—¥ã§ã™ï¼‰",
        "2023å¹´10æœˆ30æ—¥ã®æ”¿ç­–å§”å“¡ä¼šé‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã§è­°è«–ã•ã‚ŒãŸç‚ºæ›¿å¸‚å ´å‹•å‘ã¯"
    ],
}
ALL_SAMPLES = [q for qs in SAMPLES.values() for q in qs]

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Internal Bot (RAG, Shards)")

if "q" not in st.session_state:
    st.session_state.q = ""

def _set_question(text: str):
    st.session_state.q = text

with st.sidebar:
    st.header("è¨­å®š")

    # åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
    embed_backend_label = st.radio(
        "åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆä½œæˆæ™‚ã¨ä¸€è‡´ï¼‰",
        ["local (sentence-transformers)", "openai"],
        index=1
    )
    embed_backend = "openai" if embed_backend_label.startswith("openai") else "local"

    # å›ç­”ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆpricing.py ã®ä¸€è¦§ã‹ã‚‰ï¼‰
    st.markdown("### å›ç­”ãƒ¢ãƒ‡ãƒ«")
    all_models_sorted = sorted(MODEL_PRICES_PER_1K.keys(), key=lambda x: (0 if x.startswith("gpt-5") else 1, x))
    default_idx = all_models_sorted.index("gpt-5-mini") if "gpt-5-mini" in all_models_sorted else 0
    chat_model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", all_models_sorted, index=default_idx)

    # æ¤œç´¢ä»¶æ•°
    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    # å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«
    label_to_value = {"ç°¡æ½”":"concise","æ¨™æº–":"standard","è©³ç´°":"detailed","è¶…è©³ç´°":"very_detailed"}
    detail_label = st.selectbox("è©³ã—ã•", list(label_to_value.keys()), index=2)
    detail = label_to_value[detail_label]

    cite = st.checkbox("å‡ºå…¸ã‚’è§’æ‹¬å¼§ã§å¼•ç”¨ï¼ˆ[S1] ç­‰ï¼‰", value=True)

    # æ¸©åº¦ï¼ˆResponsesç³»ã¯ç„¡åŠ¹åŒ–ï¼‰
    is_responses = _use_responses_api(chat_model)
    temperature = st.slider(
        "temperatureï¼ˆChatç³»ã®ã¿æœ‰åŠ¹ï¼‰", 0.0, 1.0, 0.2, 0.05,
        disabled=is_responses, help="gpt-5*, gpt-4.1* ã§ã¯ç„¡åŠ¹ï¼ˆå›ºå®šï¼‰"
    )

    # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç›®å®‰ï¼‰", 256, 4000, 1200, 64)

    answer_backend = st.radio("å›ç­”ç”Ÿæˆ", ["OpenAI", "Retrieve-only"], index=0)
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)

    # ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠ
    st.divider()
    st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰")
    shard_dirs_all = _list_shard_dirs(embed_backend)
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«
    st.caption("ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§æ¤œç´¢ã—ãŸã„å ´åˆ: å¹´/ãƒ•ã‚¡ã‚¤ãƒ«å ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    # OpenAIã‚­ãƒ¼ç¢ºèª
    has_key = bool(_get_openai_api_key())
    if answer_backend == "OpenAI" and not has_key:
        st.error("OpenAI APIã‚­ãƒ¼ãŒ secrets.toml / ç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ï¼ˆè‡ªå‹•ã§ã€Retrieve-onlyã€ã«åˆ‡æ›¿ï¼‰ã€‚")

    # ğŸ§ª ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    st.divider()
    st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample = ""
    if cat != "ï¼ˆæœªé¸æŠï¼‰":
        sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES[cat])
    else:
        st.caption("ã‚«ãƒ†ã‚´ãƒªã‚’é¸ã¶ã‹ã€ä¸‹ã®ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥ã‚’ä½¿ãˆã¾ã™ã€‚")

    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button("â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ", use_container_width=True,
                  disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")), on_click=lambda: _set_question(sample))
    with cols_demo[1]:
        st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥", use_container_width=True,
                  on_click=lambda: _set_question(random.choice(ALL_SAMPLES)))
    send_now = st.button("ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡", use_container_width=True,
                         disabled=(st.session_state.q.strip() == ""))

    # æ–™é‡‘è¨ˆç®—ï¼ˆ/1K æ›ç®—æ¸ˆã¿ã®å˜ä¾¡ã‚’é©ç”¨ï¼‰
    st.divider()
    st.subheader("ğŸ’µ æ–™é‡‘è¨ˆç®—ï¼ˆ/1K tok æ›ç®—ï¼‰")
    usd_jpy = float(st.number_input("ç‚ºæ›¿ (JPY/USD)", min_value=50.0, max_value=500.0,
                                    value=float(DEFAULT_USDJPY), step=0.5))
    chat_in_price = float(MODEL_PRICES_PER_1K.get(chat_model, {}).get("in", 0.0))
    chat_out_price = float(MODEL_PRICES_PER_1K.get(chat_model, {}).get("out", 0.0))
    default_emb_model = "text-embedding-3-large"
    emb_price = float(EMBEDDING_PRICES_USD.get(default_emb_model, 0.0))  # ã™ã§ã« /1K

# å…¥åŠ›æ¬„
q = st.text_area("è³ªå•ã‚’å…¥åŠ›", value=st.session_state.q,
                 placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦", height=100)
if q != st.session_state.q:
    st.session_state.q = q

go_click = st.button("é€ä¿¡", type="primary")
go = go_click or bool(locals().get("send_now"))

# =========================
# å®Ÿè¡Œ
# =========================
if go and st.session_state.q.strip():

    # åŸ‹ã‚è¾¼ã¿ backend ã®å®‰å…¨åŒ–
    if embed_backend == "openai" and not _get_openai_api_key():
        st.warning("åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒ openai ã§ã™ãŒ APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€'local' ã«è‡ªå‹•åˆ‡æ›¿ã—ã¾ã™ã€‚")
        embed_backend = "local"

    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«: [[files: ...]]
    inline = re.search(r"\[\[\s*files\s*:\s*([^\]]+)\]\]", st.session_state.q, flags=re.IGNORECASE)
    inline_files = set()
    if inline:
        inline_files = {s.strip() for s in inline.group(1).split(",") if s.strip()}

    effective_whitelist = {_norm_path(x) for x in (set(file_whitelist) | set(inline_files))}
    clean_q = re.sub(r"\[\[\s*files\s*:[^\]]+\]\]", "", st.session_state.q, flags=re.IGNORECASE).strip()

    # ã‚¯ã‚¨ãƒªæ­£è¦åŒ–
    question = normalize_ja_text(clean_q)

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ«ãƒ¼ãƒˆ
    vs_backend_dir = VS_ROOT / embed_backend
    if not vs_backend_dir.exists():
        st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã« **ãƒ™ã‚¯ãƒˆãƒ«åŒ–** ã‚’åŒã˜ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs_all = _list_shard_dirs(embed_backend)
    selected = [vs_backend_dir / s for s in target_shards] if target_shards else [vs_backend_dir / p.name for p in shard_dirs_all]
    shard_dirs = [p for p in selected if p.is_dir() and (p / "vectors.npy").exists()]

    if not shard_dirs:
        st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    try:
        # --- æ¤œç´¢ ---
        with st.spinner("æ¤œç´¢ä¸­â€¦"):
            # OpenAI ã‚­ãƒ¼ã‚’ env ã«æ³¨å…¥ï¼ˆEmbeddingStore ãŒ env ã‚’å‚ç…§ã™ã‚‹å‰æï¼‰
            api_key = _get_openai_api_key()
            if api_key and "OPENAI_API_KEY" not in os.environ:
                os.environ["OPENAI_API_KEY"] = api_key

            estore = EmbeddingStore(backend=embed_backend)
            # OpenAIåŸ‹ã‚è¾¼ã¿æ™‚ã ã‘æ¦‚ç®—ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆæ¸¬ï¼ˆæ–™é‡‘ã®ç›®å®‰ï¼‰
            emb_tokens = _count_tokens(question, model_hint="text-embedding-3-large") if embed_backend == "openai" else 0
            qv = estore.embed([question]).astype("float32")  # shape=(1, d)

            # å„ã‚·ãƒ£ãƒ¼ãƒ‰ top-k â†’ å…¨ä½“ãƒãƒ¼ã‚¸ï¼ˆã‚¹ã‚³ã‚¢å¤§ã»ã©è‰¯ã„ï¼‰
            K = int(top_k)
            heap_: List[Tuple[float, int, int, Dict[str, Any]]] = []
            tiebreak = count()

            for shp in shard_dirs:
                try:
                    vdb = NumpyVectorDB(shp)
                    hits = vdb.search(qv, top_k=K, return_="similarity")
                    for h in hits:
                        if isinstance(h, tuple) and len(h) == 3:
                            row_idx, score, meta = h
                        elif isinstance(h, tuple) and len(h) == 2:
                            score, meta = h
                            row_idx = -1
                        else:
                            continue

                        md = dict(meta or {})
                        md["shard_id"] = shp.name

                        if effective_whitelist:
                            if _norm_path(str(md.get("file", ""))) not in effective_whitelist:
                                continue

                        sc = float(score)
                        tb = next(tiebreak)

                        if len(heap_) < K:
                            heapq.heappush(heap_, (sc, tb, row_idx, md))
                        else:
                            if sc > heap_[0][0]:
                                heapq.heapreplace(heap_, (sc, tb, row_idx, md))
                except Exception as e:
                    st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")

            raw_hits = [(rid, sc, md) for (sc, _tb, rid, md) in sorted(heap_, key=lambda x: x[0], reverse=True)]

        if not raw_hits:
            if effective_whitelist:
                st.warning("æŒ‡å®šã•ã‚ŒãŸå‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¹´/ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: 2025/foo.pdfï¼‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚„ Top-K ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # è¡¨ç¤ºç”¨
        contexts_display = []
        for i, (row_idx, score, meta) in enumerate(raw_hits, 1):
            txt = str(meta.get("text", "") or "")
            src_label = _fmt_source(meta)
            contexts_display.append((i, txt, src_label, float(score)))

        # --- å›ç­”ç”Ÿæˆ or Retrieve-only ---
        chat_prompt_tokens = 0
        chat_completion_tokens = 0
        answer = None

        use_answer_backend = "Retrieve-only" if (answer_backend == "OpenAI" and not _get_openai_api_key()) else answer_backend

        if use_answer_backend == "OpenAI":
            with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                labeled_contexts = [
                    f"[S{i}] {meta.get('text','')}\n[meta: {_fmt_source(meta)} / score={float(score):.3f}]"
                    for i, (_rid, score, meta) in enumerate(raw_hits, 1)
                ]
                # â˜… strict=False ã«å¤‰æ›´ã—ã¦ã€RAGæ–‡è„ˆå†…ã®æƒ…å ±ã‚’ç©æ¥µæ´»ç”¨
                prompt = build_prompt(
                    question,
                    labeled_contexts,
                    sys_inst=sys_inst,
                    style_hint=detail,
                    cite=cite,
                    strict=False,
                )

                client = OpenAI(api_key=_get_openai_api_key() or "")

                if _use_responses_api(chat_model):
                    # ---------- Responses APIï¼ˆroleåˆ†é›¢ãƒ»max_output_tokensï¼‰ ----------
                    try:
                        resp = client.responses.create(
                            model=chat_model,
                            input=[
                                {"role": "system", "content": sys_inst},
                                {"role": "user", "content": prompt},
                            ],
                            max_output_tokens=int(max_tokens),
                        )
                    except TypeError:
                        # SDK å·®ç•°å¯¾ç­–ï¼šæœ€å°å¼•æ•°ã§å†è©¦è¡Œ
                        resp = client.responses.create(
                            model=chat_model,
                            input=[
                                {"role": "system", "content": sys_inst},
                                {"role": "user", "content": prompt},
                            ],
                        )

                    # å‡ºåŠ›æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å«ã‚€ï¼‰
                    try:
                        answer = resp.output_text
                    except Exception:
                        try:
                            answer = resp.output[0].content[0].text  # ä¸€éƒ¨SDKç³»
                        except Exception:
                            answer = str(resp)

                    # ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³
                    try:
                        usage = getattr(resp, "usage", None)
                        if usage:
                            chat_prompt_tokens = int(getattr(usage, "input_tokens", 0) or 0)
                            chat_completion_tokens = int(getattr(usage, "output_tokens", 0) or 0)
                    except Exception:
                        pass

                else:
                    # ---------- Chat Completions API ----------
                    resp = client.chat.completions.create(
                        model=chat_model,
                        temperature=float(temperature),
                        max_tokens=int(max_tokens),
                        messages=[
                            {"role": "system", "content": "Follow the user's instruction carefully and answer in Japanese when possible."},
                            {"role": "user", "content": prompt},
                        ],
                    )
                    answer = resp.choices[0].message.content
                    try:
                        chat_prompt_tokens = int(resp.usage.prompt_tokens or 0)
                        chat_completion_tokens = int(resp.usage.completion_tokens or 0)
                    except Exception:
                        chat_prompt_tokens = _count_tokens(prompt, model_hint="cl100k_base")
                        chat_completion_tokens = _count_tokens(answer or "", model_hint="cl100k_base")

            st.subheader("ğŸ§  å›ç­”")
            st.write(answer)
        else:
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")

        # --- æ–™é‡‘è¨ˆç®—ï¼ˆ/1K å˜ä¾¡ã§è¨ˆç®—ï¼‰ ---
        with st.container():
            emb_cost_usd = ((emb_tokens / 1000.0) * emb_price) if (embed_backend == "openai") else 0.0
            chat_cost_usd = 0.0
            if use_answer_backend == "OpenAI":
                chat_cost_usd = (chat_prompt_tokens / 1000.0) * chat_in_price + \
                                (chat_completion_tokens / 1000.0) * chat_out_price
            total_usd = emb_cost_usd + chat_cost_usd
            total_jpy = total_usd * usd_jpy

            st.markdown("### ğŸ’´ ä½¿ç”¨æ–™ã®æ¦‚ç®—ï¼ˆ/1K tok å˜ä¾¡ï¼‰")
            cols = st.columns(3)
            with cols[0]:
                st.metric("åˆè¨ˆ (JPY)", f"{total_jpy:,.2f} å††")
                st.caption(f"ç‚ºæ›¿ {usd_jpy:.2f} JPY/USD")
            with cols[1]:
                st.write("**å†…è¨³ (USD)**")
                st.write(f"- Embedding: `${emb_cost_usd:.6f}` ({emb_tokens} tok @ {emb_price:.5f}/1K)")
                if use_answer_backend == "OpenAI":
                    st.write(f"- Chat å…¥åŠ›: `${(chat_prompt_tokens/1000.0)*chat_in_price:.6f}` ({chat_prompt_tokens} tok @ {chat_in_price:.5f}/1K)")
                    st.write(f"- Chat å‡ºåŠ›: `${(chat_completion_tokens/1000.0)*chat_out_price:.6f}` ({chat_completion_tokens} tok @ {chat_out_price:.5f}/1K)")
                st.write(f"- åˆè¨ˆ: `${total_usd:.6f}`")
            with cols[2]:
                st.write("**å˜ä¾¡ (USD / 1K tok)**")
                st.write(f"- Embedding: `${emb_price:.5f}`ï¼ˆ{default_emb_model}ï¼‰")
                st.write(f"- Chat å…¥åŠ›: `${chat_in_price:.5f}`ï¼ˆ{chat_model}ï¼‰")
                st.write(f"- Chat å‡ºåŠ›: `${chat_out_price:.5f}`ï¼ˆ{chat_model}ï¼‰")

        # --- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ---
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, (_rid, score, meta) in enumerate(raw_hits, 1):
                txt = str(meta.get("text", "") or "")
                src_label = _fmt_source(meta)
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt
                st.markdown(f"**[S{i}] score={float(score):.3f}**  `{src_label}`\n\n{snippet}")

    except Exception as e:
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚„å›ç­”è¨­å®šã€å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª¿æ•´ã§ãã¾ã™ã€‚")
