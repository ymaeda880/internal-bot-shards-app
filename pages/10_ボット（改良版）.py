# pages/10_ãƒœãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰.py
# ============================================
# ã“ã®ç‰ˆã¯ã€Œgpt-5 / gpt-4.1ã€å°‚ç”¨ï¼ˆResponses API ã®ã¿ï¼‰
# åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¯ OpenAI ã®ã¿ï¼ˆlocal ã‚’å»ƒæ­¢ï¼‰
# ============================================

# ============================================
# ã“ã®ç‰ˆã¯ã€Œgpt-5 / gpt-4.1ã€å°‚ç”¨ï¼ˆResponses API ã®ã¿ï¼‰
# - Chat Completions åˆ†å²ã¨ temperature UI ã‚’å‰Šé™¤ã—ã¦ç°¡ç´ åŒ–
# - max_output_tokens / usage.input_tokens, output_tokens ã®ã¿ä½¿ç”¨
# ============================================

# ============================================
# å¤‰æ›´ç‚¹ï¼ˆã“ã®ç‰ˆã§ã®ä¿®æ­£ãƒ»è¿½åŠ ï¼‰
# --------------------------------------------
# 1) ãƒ¢ãƒ‡ãƒ«é¸æŠã®çµ±ä¸€ï¼ˆpricing ã®ä¸€è¦§ã‚’ä½¿ç”¨ã€æ—¢å®š gpt-5-miniï¼‰
# 2) Responsesç³»ï¼ˆgpt-5*, gpt-4.1*ï¼‰ã«è‡ªå‹•å¯¾å¿œï¼ˆroleåˆ†é›¢ã€max_output_tokensã€æ¸©åº¦ç„¡åŠ¹ ç­‰ï¼‰
# 3) æ–™é‡‘è¨ˆç®—ã¯ lib/costs.py ã«é›†ç´„ï¼ˆã“ã®ãƒšãƒ¼ã‚¸ã¯å‘¼ã¶ã ã‘ï¼‰
# 4) åŒç‚¹ã‚¹ã‚³ã‚¢æ™‚ã®ãƒ’ãƒ¼ãƒ—æ¯”è¼ƒãƒã‚°å›é¿ï¼ˆã‚¿ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¯ï¼‰
# 5) OpenAIã‚­ãƒ¼: secrets.toml â†’ env ã‚’ç¶™ç¶š
# 6) æ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼ˆ[[files: ...]]ã€ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠã€RAGæ¤œç´¢ã€Retrieve-onlyï¼‰
# 7) ã‚µãƒ³ãƒ—ãƒ«è³ªå•UIç¶­æŒ
# 8) å›ç­”å“è³ªæ”¹å–„: build_prompt(strict=False)
# 9) ç‚ºæ›¿å…¥åŠ›ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å‰Šé™¤ã€‚å¸¸ã« DEFAULT_USDJPY ã‚’ä½¿ç”¨ã€‚
# ============================================

from __future__ import annotations
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any
import random
import heapq
from itertools import count
import unicodedata
import re

import streamlit as st
import numpy as np
from openai import OpenAI

from lib.rag_utils import EmbeddingStore, NumpyVectorDB
from config.path_config import PATHS
from lib.text_normalize import normalize_ja_text
from config.sample_questions import SAMPLES, ALL_SAMPLES

from lib.costs import (
    MODEL_PRICES_USD, EMBEDDING_PRICES_USD, DEFAULT_USDJPY,
    ChatUsage, estimate_chat_cost, estimate_embedding_cost, usd_to_jpy,
)

from lib.prompts.bot_prompt import build_prompt

# ========= /1M â†’ /1K å¤‰æ›ï¼ˆè¡¨ç¤ºç”¨ï¼‰ =========
MODEL_PRICES_PER_1K: Dict[str, Dict[str, float]] = {
    m: {"in": float(p.get("in", 0.0)) / 1000.0, "out": float(p.get("out", 0.0)) / 1000.0}
    for m, p in MODEL_PRICES_USD.items()
}

# ========= ãƒ‘ã‚¹ =========
VS_ROOT: Path = PATHS.vs_root  # ä¾‹: <project>/data/vectorstore

# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _count_tokens(text: str, model_hint: str = "cl100k_base") -> int:
    try:
        import tiktoken
        try:
            enc = tiktoken.encoding_for_model(model_hint)
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception:
        return max(1, int(len(text) / 4))

def _fmt_source(meta: Dict[str, Any]) -> str:
    f = str(meta.get("file", "") or "")
    p = meta.get("page", None)
    cid = str(meta.get("chunk_id", "") or "")
    base = f"{f} p.{int(p)}" if (f and p is not None) else (f or "(unknown)")
    return f"{base} ({cid})" if cid else base

def _list_shard_dirs_openai() -> List[Path]:
    base = VS_ROOT / "openai"
    if not base.exists():
        return []
    return sorted([p for p in base.iterdir() if p.is_dir()])

def _norm_path(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.strip().replace("\\", "/")
    return s.lower()

def _get_openai_api_key() -> str | None:
    try:
        ok = st.secrets.get("openai", {}).get("api_key")
        if ok:
            return str(ok)
    except Exception:
        pass
    return os.getenv("OPENAI_API_KEY")

# ---------- ãƒ¢ãƒ‡ãƒ«å€™è£œï¼ˆResponseså°‚ç”¨ï¼‰ ----------
RESPONSES_MODELS = [m for m in MODEL_PRICES_PER_1K.keys() if m.startswith("gpt-5") or m.startswith("gpt-4.1")]

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Internal Bot (RAG, Shards)")

if "q" not in st.session_state:
    st.session_state.q = ""

def _set_question(text: str):
    st.session_state.q = text

with st.sidebar:
    st.header("è¨­å®š")

    # å›ç­”ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆgpt-5 / gpt-4.1 é™å®šï¼‰
    st.markdown("### å›ç­”ãƒ¢ãƒ‡ãƒ«ï¼ˆResponses APIï¼‰")
    all_models_sorted = sorted(RESPONSES_MODELS, key=lambda x: (0 if x.startswith("gpt-5") else 1, x))
    default_idx = all_models_sorted.index("gpt-5-mini") if "gpt-5-mini" in all_models_sorted else 0
    chat_model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", all_models_sorted, index=default_idx)

    # æ¤œç´¢ä»¶æ•°
    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    # å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«
    label_to_value = {"ç°¡æ½”":"concise","æ¨™æº–":"standard","è©³ç´°":"detailed","è¶…è©³ç´°":"very_detailed"}
    detail_label = st.selectbox("è©³ã—ã•", list(label_to_value.keys()), index=2)
    detail = label_to_value[detail_label]

    cite = st.checkbox("å‡ºå…¸ã‚’è§’æ‹¬å¼§ã§å¼•ç”¨ï¼ˆ[S1] ç­‰ï¼‰", value=True)

    # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ï¼ˆResponses ã¯ max_output_tokensï¼‰
    # max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç›®å®‰ï¼‰", 256, 4000, 1200, 64)
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç›®å®‰ï¼‰", 1000, 40000, 12000, 500)

    answer_backend = st.radio("å›ç­”ç”Ÿæˆ", ["OpenAI", "Retrieve-only"], index=0)
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)

    # ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠï¼ˆOpenAI åŸ‹ã‚è¾¼ã¿å°‚ç”¨ï¼‰
    st.divider()
    st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆOpenAIï¼‰")
    shard_dirs_all = _list_shard_dirs_openai()
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«
    st.caption("ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§æ¤œç´¢ã—ãŸã„å ´åˆ: å¹´/ãƒ•ã‚¡ã‚¤ãƒ«å ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    # OpenAIã‚­ãƒ¼ç¢ºèªï¼ˆå¿…é ˆï¼‰
    has_key = bool(_get_openai_api_key())
    if not has_key:
        st.error("OpenAI APIã‚­ãƒ¼ãŒ secrets.toml / ç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ã€‚åŸ‹ã‚è¾¼ã¿ã¨å›ç­”ç”Ÿæˆã®åŒæ–¹ã«å¿…é ˆã§ã™ã€‚")

    # ğŸ§ª ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    st.divider()
    st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample = ""
    if cat != "ï¼ˆæœªé¸æŠï¼‰":
        sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES[cat])
    else:
        st.caption("ã‚«ãƒ†ã‚´ãƒªã‚’é¸ã¶ã‹ã€ä¸‹ã®ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥ã‚’ä½¿ãˆã¾ã™ã€‚")

    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button("â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ", use_container_width=True,
                  disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")), on_click=lambda: _set_question(sample))
    with cols_demo[1]:
        st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥", use_container_width=True,
                  on_click=lambda: _set_question(random.choice(ALL_SAMPLES)))
    send_now = st.button("ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡", use_container_width=True,
                         disabled=(st.session_state.q.strip() == ""))

# å…¥åŠ›æ¬„
q = st.text_area("è³ªå•ã‚’å…¥åŠ›", value=st.session_state.q,
                 placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦", height=100)
if q != st.session_state.q:
    st.session_state.q = q

go_click = st.button("é€ä¿¡", type="primary")
go = go_click or bool(locals().get("send_now"))

# =========================
# å®Ÿè¡Œ
# =========================
if go and st.session_state.q.strip():
    # OpenAI ã‚­ãƒ¼å¿…é ˆï¼ˆåŸ‹ã‚è¾¼ã¿ã‚¯ã‚¨ãƒªç”Ÿæˆã«å¿…è¦ï¼‰
    api_key = _get_openai_api_key()
    if not api_key:
        st.stop()

    try:
        # --- æ¤œç´¢ ---
        with st.spinner("æ¤œç´¢ä¸­â€¦"):
            if "OPENAI_API_KEY" not in os.environ:
                os.environ["OPENAI_API_KEY"] = api_key

            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ«ãƒ¼ãƒˆï¼ˆOpenAI å›ºå®šï¼‰
            vs_backend_dir = VS_ROOT / "openai"
            if not vs_backend_dir.exists():
                st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã« **ãƒ™ã‚¯ãƒˆãƒ«åŒ–** ã‚’ OpenAI ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()

            shard_dirs_all = _list_shard_dirs_openai()
            selected = [vs_backend_dir / s for s in target_shards] if target_shards else [vs_backend_dir / p.name for p in shard_dirs_all]
            shard_dirs = [p for p in selected if p.is_dir() and (p / "vectors.npy").exists()]
            if not shard_dirs:
                st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()

            # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«: [[files: ...]]
            inline = re.search(r"\[\[\s*files\s*:\s*([^\]]+)\]\]", st.session_state.q, flags=re.IGNORECASE)
            inline_files = set()
            if inline:
                inline_files = {s.strip() for s in inline.group(1).split(",") if s.strip()}

            effective_whitelist = {_norm_path(x) for x in (set(file_whitelist) | set(inline_files))}
            clean_q = re.sub(r"\[\[\s*files\s*:[^\]]+\]\]", "", st.session_state.q, flags=re.IGNORECASE).strip()

            # ã‚¯ã‚¨ãƒªæ­£è¦åŒ– & åŸ‹ã‚è¾¼ã¿
            question = normalize_ja_text(clean_q)
            estore = EmbeddingStore(backend="openai")
            emb_tokens = _count_tokens(question, model_hint="text-embedding-3-large")
            qv = estore.embed([question]).astype("float32")  # shape=(1, d)

            # å„ã‚·ãƒ£ãƒ¼ãƒ‰ top-k â†’ å…¨ä½“ãƒãƒ¼ã‚¸
            K = int(top_k)
            heap_: List[Tuple[float, int, int, Dict[str, Any]]] = []
            tiebreak = count()

            for shp in shard_dirs:
                try:
                    vdb = NumpyVectorDB(shp)
                    hits = vdb.search(qv, top_k=K, return_="similarity")
                    for h in hits:
                        if isinstance(h, tuple) and len(h) == 3:
                            row_idx, score, meta = h
                        elif isinstance(h, tuple) and len(h) == 2:
                            score, meta = h
                            row_idx = -1
                        else:
                            continue

                        md = dict(meta or {})
                        md["shard_id"] = shp.name

                        if effective_whitelist:
                            if _norm_path(str(md.get("file", ""))) not in effective_whitelist:
                                continue

                        sc = float(score)
                        tb = next(tiebreak)

                        if len(heap_) < K:
                            heapq.heappush(heap_, (sc, tb, row_idx, md))
                        else:
                            if sc > heap_[0][0]:
                                heapq.heapreplace(heap_, (sc, tb, row_idx, md))
                except Exception as e:
                    st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")

            raw_hits = [(rid, sc, md) for (sc, _tb, rid, md) in sorted(heap_, key=lambda x: x[0], reverse=True)]

        if not raw_hits:
            if effective_whitelist:
                st.warning("æŒ‡å®šã•ã‚ŒãŸå‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¹´/ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: 2025/foo.pdfï¼‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚„ Top-K ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # --- å›ç­”ç”Ÿæˆï¼ˆResponses APIï¼‰ ---
        chat_prompt_tokens = 0
        chat_completion_tokens = 0
        answer = None

        use_answer_backend = "Retrieve-only" if (answer_backend == "OpenAI" and not api_key) else answer_backend

        if use_answer_backend == "OpenAI":
            with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                labeled_contexts = [
                    f"[S{i}] {meta.get('text','')}\n[meta: {_fmt_source(meta)} / score={float(score):.3f}]"
                    for i, (_rid, score, meta) in enumerate(raw_hits, 1)
                ]
                prompt = build_prompt(
                    question,
                    labeled_contexts,
                    sys_inst=sys_inst,
                    style_hint=detail,
                    cite=cite,
                    strict=False,
                )

                client = OpenAI(api_key=api_key or "")
                try:
                    resp = client.responses.create(
                        model=chat_model,
                        input=[
                            {"role": "system", "content": sys_inst},
                            {"role": "user", "content": prompt},
                        ],
                        max_output_tokens=int(max_tokens),
                    )
                except TypeError:
                    resp = client.responses.create(
                        model=chat_model,
                        input=[
                            {"role": "system", "content": sys_inst},
                            {"role": "user", "content": prompt},
                        ],
                    )

                try:
                    answer = resp.output_text
                except Exception:
                    try:
                        answer = resp.output[0].content[0].text
                    except Exception:
                        answer = str(resp)

                try:
                    usage = getattr(resp, "usage", None)
                    if usage:
                        chat_prompt_tokens = int(getattr(usage, "input_tokens", 0) or 0)
                        chat_completion_tokens = int(getattr(usage, "output_tokens", 0) or 0)
                except Exception:
                    pass

            st.subheader("ğŸ§  å›ç­”")
            st.write(answer)
        else:
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")

        # --- æ–™é‡‘è¨ˆç®—ï¼ˆlib/costs.py ã‚’ä½¿ç”¨ï¼‰ ---
        with st.container():
            emb_cost_usd = estimate_embedding_cost("text-embedding-3-large", emb_tokens)["usd"]
            chat_cost_usd = 0.0
            if use_answer_backend == "OpenAI":
                chat_cost_usd = estimate_chat_cost(
                    chat_model,
                    ChatUsage(input_tokens=chat_prompt_tokens, output_tokens=chat_completion_tokens)
                )["usd"]

            total_usd = emb_cost_usd + chat_cost_usd
            total_jpy = usd_to_jpy(total_usd, rate=DEFAULT_USDJPY)

            st.markdown("### ğŸ’´ ä½¿ç”¨æ–™ã®æ¦‚ç®—ï¼ˆlib/costs ã«ã‚ˆã‚‹é›†è¨ˆï¼‰")
            cols = st.columns(3)
            with cols[0]:
                st.metric("åˆè¨ˆ (JPY)", f"{total_jpy:,.2f} å††")
                st.caption(f"ç‚ºæ›¿ {DEFAULT_USDJPY:.2f} JPY/USD")
            with cols[1]:
                st.write("**å†…è¨³ (USD)**")
                st.write(f"- Embedding: `${emb_cost_usd:.6f}`  ({emb_tokens} tok)")
                if use_answer_backend == "OpenAI":
                    st.write(f"- Chat åˆè¨ˆ: `${chat_cost_usd:.6f}` "
                             f"(in={chat_prompt_tokens} tok / out={chat_completion_tokens} tok)")
                st.write(f"- åˆè¨ˆ: `${total_usd:.6f}`")
            with cols[2]:
                emb_price_per_1k = float(EMBEDDING_PRICES_USD.get("text-embedding-3-large", 0.0)) / 1000.0
                st.write("**å˜ä¾¡ (USD / 1K tok)**")
                st.write(f"- Embedding: `${emb_price_per_1k:.5f}`ï¼ˆtext-embedding-3-largeï¼‰")
                st.write(f"- Chat å…¥åŠ›: `${MODEL_PRICES_PER_1K.get(chat_model,{}).get('in',0.0):.5f}`ï¼ˆ{chat_model}ï¼‰")
                st.write(f"- Chat å‡ºåŠ›: `${MODEL_PRICES_PER_1K.get(chat_model,{}).get('out',0.0):.5f}`ï¼ˆ{chat_model}ï¼‰")

        # --- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ---
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, (_rid, score, meta) in enumerate(raw_hits, 1):
                txt = str(meta.get("text", "") or "")
                src_label = _fmt_source(meta)
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt
                st.markdown(f"**[S{i}] score={float(score):.3f}**  `{src_label}`\n\n{snippet}")

    except Exception as e:
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚„å›ç­”è¨­å®šã€å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª¿æ•´ã§ãã¾ã™ã€‚")
