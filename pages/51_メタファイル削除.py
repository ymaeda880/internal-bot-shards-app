# pages/51_ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤.py
# ------------------------------------------------------------
# ğŸ—‘ï¸ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ãƒšãƒ¼ã‚¸
# - å‰Šé™¤ / åˆæœŸåŒ– / ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— / å¾©å…ƒ
# - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆï¼šPATHS.backup_root / <backend> / <shard_id> / <timestamp>
# - è¿½åŠ æ©Ÿèƒ½:
#   1) ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
#   2) å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã¿å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
#   3) ã€Œæœªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ—¥æ•°ã€ã—ãã„å€¤ã§ä¸€æ‹¬ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
#   4) ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨å‰Šé™¤â†’ç©ºãƒ•ã‚©ãƒ«ãƒ€å†ä½œæˆï¼‰
#   5) å®Œå…¨åˆæœŸåŒ–ã«ã‚‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼†DELETEç¢ºèª
# ------------------------------------------------------------
from __future__ import annotations
from pathlib import Path
from datetime import datetime, timezone
import json
import shutil
import os
import urllib.parse
import unicodedata
import numpy as np
import pandas as pd
import streamlit as st

from config.config import PATHS  # âœ… vs_root / backup_root ã‚’é›†ä¸­ç®¡ç†
from lib.vectorstore_utils import iter_jsonl  # æ—¢å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

# ============================================================
# åŸºæœ¬ãƒ‘ã‚¹ï¼ˆconfig ã«åˆã‚ã›ã‚‹ï¼‰
# ============================================================
VS_ROOT: Path      = PATHS.vs_root
BACKUP_ROOT: Path  = PATHS.backup_root

# ============================================================
# UI è¨­å®š
# ============================================================
st.set_page_config(page_title="51 ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤", page_icon="ğŸ—‘ï¸", layout="wide")
st.title("ğŸ—‘ï¸ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰å˜ä½ï¼‰")
st.caption(f"Backup base: `{BACKUP_ROOT}` / VectorStore: `{VS_ROOT}`")
st.info("ã“ã®ãƒšãƒ¼ã‚¸ã¯ **å‰Šé™¤ãƒ»åˆæœŸåŒ–ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—/å¾©å…ƒ** ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚ä½œæ¥­å‰ã«å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")

# ============================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠ
# ============================================================
with st.sidebar:
    st.header("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True, key="sb_backend")
    base_backend_dir = VS_ROOT / backend
    if not base_backend_dir.exists():
        st.error(f"{base_backend_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs = sorted([p for p in base_backend_dir.iterdir() if p.is_dir()])
    shard_ids = [p.name for p in shard_dirs]
    shard_id = st.selectbox("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰", shard_ids if shard_ids else ["(ãªã—)"], key="sb_shard")
    if not shard_ids:
        st.error("ã‚·ãƒ£ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

# ============================================================
# å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ‘ã‚¹
# ============================================================
base_dir = base_backend_dir / shard_id
meta_path = base_dir / "meta.jsonl"
vec_path  = base_dir / "vectors.npy"
pf_path   = base_dir / "processed_files.json"

# ============================================================
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆã“ã®ãƒšãƒ¼ã‚¸ã§å®Ÿè£…ï¼‰
# ============================================================
def _timestamp() -> str:
    return datetime.now(timezone.utc).astimezone().strftime("%Y%m%d-%H%M%S")

def _backup_dir_for(backend: str, shard_id: str, ts: str | None = None) -> Path:
    if ts is None:
        ts = _timestamp()
    return BACKUP_ROOT / backend / shard_id / ts

def backup_all_local(src_dir: Path, backend: str, shard_id: str) -> tuple[list[str], Path]:
    """
    src_dirï¼ˆ= VS_ROOT/backend/shardï¼‰ã‹ã‚‰ meta.jsonl / vectors.npy / processed_files.json ã‚’
    BACKUP_ROOT/backend/shard/<timestamp>/ ã«ã‚³ãƒ”ãƒ¼ã€‚å­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘ã‚³ãƒ”ãƒ¼ã€‚
    """
    ts_dir = _backup_dir_for(backend, shard_id)
    ts_dir.mkdir(parents=True, exist_ok=True)
    copied: list[str] = []
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        src = src_dir / name
        if src.exists():
            shutil.copy2(src, ts_dir / name)
            copied.append(name)
    return copied, ts_dir

def list_backup_dirs_local(backend: str, shard_id: str) -> list[Path]:
    root = BACKUP_ROOT / backend / shard_id
    if not root.exists():
        return []
    return sorted([p for p in root.iterdir() if p.is_dir()], key=lambda p: p.name, reverse=True)

def preview_backup_local(bdir: Path) -> pd.DataFrame:
    rows = []
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        p = bdir / name
        if p.exists():
            size = p.stat().st_size
            rows.append({"name": name, "size(bytes)": size, "path": str(p)})
    return pd.DataFrame(rows)

def restore_from_backup_local(dst_dir: Path, bdir: Path) -> tuple[list[str], list[str]]:
    restored, missing = [], []
    dst_dir.mkdir(parents=True, exist_ok=True)
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        src = bdir / name
        if src.exists():
            shutil.copy2(src, dst_dir / name)
            restored.append(name)
        else:
            missing.append(name)
    return restored, missing

def backup_age_days_local(backend: str, shard_id: str) -> float | None:
    import time
    bdirs = list_backup_dirs_local(backend, shard_id)
    if not bdirs:
        return None
    latest = bdirs[0]
    mtimes = []
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        p = latest / name
        if p.exists():
            mtimes.append(p.stat().st_mtime)
    if not mtimes:
        mtimes.append(latest.stat().st_mtime)
    age_sec = max(time.time() - max(mtimes), 0.0)
    return age_sec / 86400.0

# ============================================================
# processed_files.json æœ€é©åŒ–ï¼šæ§‹é€ ä¿æŒã§ã®é¸æŠå‰Šé™¤
#  - å¯¾å¿œã‚¹ã‚­ãƒ¼ãƒï¼š
#      * {"done":[...]}
#      * [...]
#      * [{"done":[...]}, {"done":[...]}]   â† ã‚ˆãã‚ã‚‹ã‚±ãƒ¼ã‚¹
#  - è¦ç´ ã¯ str / dictï¼ˆfile/path/name/relpath/source/original/orig/pdfï¼‰ã„ãšã‚Œã§ã‚‚OK
#  - ç…§åˆã¯ ãƒ•ãƒ«/ç›¸å¯¾(shard/filename)/basename/stem + æ­£è¦åŒ–ï¼ˆNFKCãƒ»URLdecodeãƒ»åŒºåˆ‡ã‚Šçµ±ä¸€ãƒ»lowerï¼‰
# ============================================================
def _canon(s: str) -> str:
    if not s:
        return ""
    s = urllib.parse.unquote(s)
    s = unicodedata.normalize("NFKC", s).strip()
    s = s.replace("\\", "/")
    s = os.path.normpath(s).replace("\\", "/")
    return s.lower()

def _entry_to_pathlike(entry) -> str:
    if isinstance(entry, str):
        return entry
    if isinstance(entry, dict):
        for k in ("file", "path", "name", "relpath", "source", "original", "orig", "pdf"):
            v = entry.get(k)
            if isinstance(v, str) and v.strip():
                return v
    return ""

def _load_pf_struct(pf_path: Path):
    if not pf_path.exists():
        return "empty", None, []
    try:
        root = json.loads(pf_path.read_text(encoding="utf-8"))
    except Exception:
        return "unknown", None, []

    if isinstance(root, dict) and isinstance(root.get("done"), list):
        return "object_done", root, [root["done"]]

    if isinstance(root, list):
        done_lists = []
        all_str = True
        for e in root:
            if isinstance(e, dict) and isinstance(e.get("done"), list):
                done_lists.append(e["done"])
                all_str = False
            elif not isinstance(e, str):
                all_str = False
        if done_lists:
            return "array_of_done_objects", root, done_lists
        if all_str:
            return "array", root, [root]

    return "unknown", root, []

def _save_pf_struct(pf_path: Path, schema: str, root_obj):
    if schema in ("object_done", "array", "array_of_done_objects") and root_obj is not None:
        pf_path.write_text(json.dumps(root_obj, ensure_ascii=False, indent=2), encoding="utf-8")
        return

    items: list[str] = []
    if isinstance(root_obj, dict) and isinstance(root_obj.get("done"), list):
        for e in root_obj["done"]:
            s = _entry_to_pathlike(e)
            if s:
                items.append(s)
    elif isinstance(root_obj, list):
        for e in root_obj:
            if isinstance(e, dict) and isinstance(e.get("done"), list):
                for x in e["done"]:
                    s = _entry_to_pathlike(x)
                    if s:
                        items.append(s)
            else:
                s = _entry_to_pathlike(e)
                if s:
                    items.append(s)
    items = sorted(set(items))
    pf_path.write_text(json.dumps({"done": items}, ensure_ascii=False, indent=2), encoding="utf-8")

def remove_from_processed_files_selective(pf_path: Path, removed_files: list[str]) -> tuple[int, int, int, list[str]]:
    schema, root, list_refs = _load_pf_struct(pf_path)
    if not list_refs:
        return (0, 0, 0, [])

    t_full = {_canon(x) for x in removed_files}
    t_base = {os.path.basename(x) for x in t_full}
    t_stem = {os.path.splitext(b)[0] for b in t_base}

    def _match(entry) -> bool:
        raw = _entry_to_pathlike(entry)
        cn = _canon(raw)
        if not cn:
            return False
        base = os.path.basename(cn)
        stem = os.path.splitext(base)[0]
        return (
            (cn in t_full) or
            (base in t_base) or
            (stem in t_stem) or
            any(cn.endswith("/" + t) for t in t_full)
        )

    before_total = sum(len(lst) for lst in list_refs)
    removed_show: list[str] = []

    for lst in list_refs:
        new_lst = []
        for e in lst:
            if _match(e):
                raw = _entry_to_pathlike(e)
                removed_show.append(raw if raw else json.dumps(e, ensure_ascii=False)[:120])
            else:
                new_lst.append(e)
        lst.clear()
        lst.extend(new_lst)

    _save_pf_struct(pf_path, schema, root)

    after_total = sum(len(lst) for lst in list_refs)
    removed_count = before_total - after_total
    return (before_total, after_total, removed_count, removed_show[:10])

# ============================================================
# ğŸ” ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ‹¡å¼µæ©Ÿèƒ½ï¼‰
# ============================================================
st.subheader("ğŸ›¡ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ‹¡å¼µï¼‰")

col_a, col_b, col_c = st.columns(3)
with col_a:
    if st.button("âš¡ å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", use_container_width=True, key="bak_one"):
        copied, bdir = backup_all_local(base_dir, backend, shard_id)
        if copied:
            st.success(f"[{backend}/{shard_id}] ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir}")
        else:
            st.warning(f"[{backend}/{shard_id}] ã‚³ãƒ”ãƒ¼å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¿å­˜å…ˆ: {bdir}")

with col_b:
    if st.button("âš¡ ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", use_container_width=True, key="bak_all"):
        summary = []
        for sid in shard_ids:
            sdir = base_backend_dir / sid
            copied, bdir = backup_all_local(sdir, backend, sid)
            summary.append((sid, len(copied), bdir))
        ok = [f"- {sid}: {n}é …ç›® -> {bdir}" for sid, n, bdir in summary]
        st.success("å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†:\n" + "\n".join(ok))

with col_c:
    threshold = st.selectbox("æœªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ—¥æ•° ä»¥ä¸Šãªã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", [1,2,3,7,14,30], index=2, key="bak_thr")
    if st.button("ğŸ—“ æ¡ä»¶ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ", use_container_width=True, key="bak_cond"):
        triggered, skipped = [], []
        for sid in shard_ids:
            age = backup_age_days_local(backend, sid)
            if age is None or age >= float(threshold):
                sdir = base_backend_dir / sid
                copied, bdir = backup_all_local(sdir, backend, sid)
                triggered.append((sid, age, len(copied), bdir))
            else:
                skipped.append((sid, age))
        msg = ""
        if triggered:
            msg += "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œï¼ˆé–¾å€¤è¶…é or æœªå®Ÿæ–½ï¼‰:\n" + "\n".join(
                f"- {sid}: age={('None' if age is None else f'{age:.2f}d')} -> {n}é …ç›® @ {bdir}"
                for sid, age, n, bdir in triggered
            )
        if skipped:
            if msg: msg += "\n\n"
            msg += "ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé–¾å€¤æœªæº€ï¼‰:\n" + "\n".join(f"- {sid}: age={age:.2f}d" for sid, age in skipped)
        st.info(msg or "å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

st.divider()

# ============================================================
# ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
# ============================================================
st.subheader("ğŸ“„ ç¾çŠ¶ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
rows = [dict(obj) for obj in iter_jsonl(meta_path)] if meta_path.exists() else []
if not rows:
    st.warning("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã«ã¯ meta.jsonl ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    df = pd.DataFrame(rows)
    if "file" not in df.columns:
        df["file"] = None
    st.caption(f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}")
    st.dataframe(df.head(500), use_container_width=True, height=420)

st.divider()

# ============================================================
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå€‹åˆ¥ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
# ============================================================
st.subheader("ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå€‹åˆ¥ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
bdirs = list_backup_dirs_local(backend, shard_id)
if bdirs:
    sel_bdir_prev = st.selectbox("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", bdirs, format_func=lambda p: p.name, key="prev_bdir")
    if sel_bdir_prev:
        st.dataframe(preview_backup_local(sel_bdir_prev), use_container_width=True, height=180)
else:
    st.caption("ã¾ã ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.divider()

# ============================================================
# é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆprocessed_files ã®æ‰±ã„ã‚’ãƒ©ã‚¸ã‚ªã§é¸æŠï¼‰
# ============================================================
st.subheader("ğŸ§¹ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤")
if rows:
    files = sorted(pd.Series([r.get("file") for r in rows if r.get("file")]).unique().tolist())
    c1, c2 = st.columns([2,1])
    with c1:
        target_files = st.multiselect("å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆyear/file.pdf ãªã©ï¼‰", files, key="sel_targets")
    with c2:
        pf_mode = st.radio(
            "processed_files.json ã®å‡¦ç†",
            ["é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¶ˆã™ï¼ˆæ—¢å®šï¼‰", "å®Œå…¨ãƒªã‚»ãƒƒãƒˆï¼ˆå…¨å‰Šé™¤ï¼‰", "å¤‰æ›´ã—ãªã„"],
            index=0,
            key="pf_mode"
        )
        confirm_del = st.checkbox("å‰Šé™¤ã«åŒæ„ã—ã¾ã™", key="confirm_selective")

    if st.button("ğŸ§¹ å‰Šé™¤å®Ÿè¡Œ", type="primary", use_container_width=True,
                 disabled=not (target_files and confirm_del), key="btn_selective_delete"):
        try:
            # ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            copied, bdir = backup_all_local(base_dir, backend, shard_id)

            # meta.jsonl å†æ§‹ç¯‰ + vectors.npy åŒæœŸ
            keep_lines, keep_vec_indices = [], []
            removed_meta, valid_idx = 0, 0
            target_set = set(target_files)

            if meta_path.exists():
                with meta_path.open("r", encoding="utf-8") as f:
                    for raw in f:
                        s = raw.strip()
                        if not s:
                            continue
                        try:
                            obj = json.loads(s)
                        except Exception:
                            keep_lines.append(raw)  # å£Šã‚Œè¡Œã¯ä¿å…¨
                            continue
                        fname = obj.get("file") if isinstance(obj, dict) else None
                        if fname in target_set:
                            removed_meta += 1
                            valid_idx += 1
                            continue
                        keep_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
                        keep_vec_indices.append(valid_idx)
                        valid_idx += 1
                with meta_path.open("w", encoding="utf-8") as f:
                    f.writelines(keep_lines)

            removed_vecs = 0
            if vec_path.exists():
                vecs = np.load(vec_path)
                if vecs.ndim == 2:
                    new_vecs = vecs[keep_vec_indices] if keep_vec_indices else np.empty((0, vecs.shape[1]))
                    removed_vecs = vecs.shape[0] - new_vecs.shape[0]
                    np.save(vec_path, new_vecs)

            # processed_files.json ã®å‡¦ç†ï¼ˆãƒ©ã‚¸ã‚ªé¸æŠï¼‰
            pf_msg = ""
            if pf_mode == "å®Œå…¨ãƒªã‚»ãƒƒãƒˆï¼ˆå…¨å‰Šé™¤ï¼‰":
                if pf_path.exists():
                    pf_path.unlink()
                pf_msg = "- processed_files.json: å‰Šé™¤ï¼ˆå®Œå…¨ãƒªã‚»ãƒƒãƒˆï¼‰\n"

            elif pf_mode == "é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¶ˆã™ï¼ˆæ—¢å®šï¼‰":
                if pf_path.exists():
                    before, after, removed_pf, removed_list = remove_from_processed_files_selective(pf_path, target_files)
                    if removed_pf > 0:
                        st.success(
                            "processed_files.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ:\n"
                            f"- é™¤å¤–æ•°: {removed_pf} ä»¶ (before={before}, after={after})\n"
                            f"- é™¤å¤–ã•ã‚ŒãŸé …ç›®ã®ä¾‹: {removed_list}"
                        )
                    else:
                        st.warning(
                            "processed_files.json ã«ä¸€è‡´ã™ã‚‹é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                            "ï¼ˆãƒ•ãƒ«/ç›¸å¯¾/basename/stemãƒ»NFKCãƒ»URLãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ»åŒºåˆ‡ã‚Šçµ±ä¸€ã§ç…§åˆã—ã¦ã„ã¾ã™ï¼‰"
                        )
                else:
                    pf_msg = "- processed_files.json: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n"

            else:  # "å¤‰æ›´ã—ãªã„"
                pf_msg = "- processed_files.json: å¤‰æ›´ãªã—\n"

            st.success(
                "å‰Šé™¤å®Œäº† âœ…\n"
                f"- meta.jsonl: {removed_meta} è¡Œå‰Šé™¤\n"
                f"- vectors.npy: {removed_vecs} è¡Œå‰Šé™¤\n"
                f"{pf_msg}"
                f"- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir}"
            )
        except Exception as e:
            st.error(f"å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ—‚ï¸ ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã®å®Œå…¨å‰Šé™¤ï¼‰
# ============================================================
st.subheader("ğŸ—‚ï¸ ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨å‰Šé™¤ï¼‰")

def _dir_stats(d: Path) -> tuple[int, int]:
    if not d.exists():
        return (0, 0)
    n, total = 0, 0
    for p in d.rglob("*"):
        if p.is_file():
            n += 1
            try:
                total += p.stat().st_size
            except Exception:
                pass
    return n, total

if base_dir.exists():
    cnt, total = _dir_stats(base_dir)
    st.caption(f"ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: **{cnt:,}** / åˆè¨ˆã‚µã‚¤ã‚º: **{total:,} bytes**")
else:
    st.caption("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

colx, coly = st.columns([2,1])
with colx:
    do_backup_before_shard_delete = st.checkbox(
        "å‰Šé™¤å‰ã«æ¨™æº–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆmeta/vectors/processedï¼‰ã‚’ä½œæˆã™ã‚‹",
        value=True,
        key="sharddel_backup"
    )
    confirm_shard_del = st.checkbox(
        "ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ã«åŒæ„ã—ã¾ã™ï¼ˆå…ƒã«æˆ»ã›ã¾ã›ã‚“ï¼‰",
        key="sharddel_confirm"
    )
with coly:
    typed = st.text_input("ã‚¿ã‚¤ãƒ—ç¢ºèªï¼šDELETE ã¨å…¥åŠ›", value="", key="sharddel_typed")

if st.button("ğŸ—‚ï¸ ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ã‚’å®Ÿè¡Œ", type="secondary", use_container_width=True,
             disabled=not (confirm_shard_del and typed.strip().upper() == "DELETE"), key="sharddel_exec"):
    try:
        if do_backup_before_shard_delete and base_dir.exists():
            copied, bdir = backup_all_local(base_dir, backend, shard_id)
            st.info(f"äº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir} / ã‚³ãƒ”ãƒ¼: {', '.join(copied) if copied else 'ãªã—'}")

        if base_dir.exists():
            shutil.rmtree(base_dir)
        base_dir.mkdir(parents=True, exist_ok=True)
        st.success(f"ã‚·ãƒ£ãƒ¼ãƒ‰ `{backend}/{shard_id}` ã‚’å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å†ä½œæˆæ¸ˆã¿ï¼‰")
    except Exception as e:
        st.error(f"ã‚·ãƒ£ãƒ¼ãƒ‰å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ—‘ï¸ å®Œå…¨åˆæœŸåŒ–ï¼ˆ3ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‰Šé™¤ï¼šmeta.jsonl / vectors.npy / processed_files.jsonï¼‰
#    - å®Ÿè¡Œå‰ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå­˜åœ¨ã™ã‚‹å¯¾è±¡ã¨åˆè¨ˆã‚µã‚¤ã‚ºï¼‰
#    - å‰Šé™¤å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ—¢å®šã‚ªãƒ³ï¼‰
#    - äºŒé‡ç¢ºèªï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ + DELETE å…¥åŠ›ï¼‰
# ============================================================
st.subheader("ğŸ—‘ï¸ å®Œå…¨åˆæœŸåŒ–")

targets = [
    ("meta.jsonl", meta_path),
    ("vectors.npy", vec_path),
    ("processed_files.json", pf_path),
]
present = [(name, p, (p.stat().st_size if p.exists() and p.is_file() else 0)) for name, p in targets if p.exists()]
total_bytes = sum(s for _, _, s in present)

if present:
    lines = [f"- {name}: {p} ({size:,} bytes)" for name, p, size in present]
    st.caption("å‰Šé™¤å¯¾è±¡ï¼ˆå­˜åœ¨ã—ã¦ã„ã‚‹ã‚‚ã®ã®ã¿ï¼‰:\n" + "\n".join(lines))
    st.caption(f"åˆè¨ˆã‚µã‚¤ã‚º: **{total_bytes:,} bytes**")
else:
    st.caption("å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆmeta / vectors / processedï¼‰ã€‚")

col_init_l, col_init_r = st.columns([2, 1])
with col_init_l:
    do_backup_before_wipe = st.checkbox(
        "å‰Šé™¤å‰ã«æ¨™æº–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆmeta/vectors/processedï¼‰ã‚’ä½œæˆã™ã‚‹",
        value=True,
        key="wipe_backup"
    )
    confirm_wipe = st.checkbox(
        "å®Œå…¨åˆæœŸåŒ–ã«åŒæ„ã—ã¾ã™ï¼ˆå…ƒã«æˆ»ã›ã¾ã›ã‚“ï¼‰",
        key="wipe_confirm"
    )
with col_init_r:
    typed_init = st.text_input("ã‚¿ã‚¤ãƒ—ç¢ºèªï¼šDELETE ã¨å…¥åŠ›", value="", key="wipe_typed")

if st.button(
    "ğŸ—‘ï¸ åˆæœŸåŒ–å®Ÿè¡Œ",
    type="secondary",
    use_container_width=True,
    disabled=not (confirm_wipe and typed_init.strip().upper() == "DELETE"),
    key="wipe_execute"
):
    try:
        if do_backup_before_wipe:
            copied, bdir = backup_all_local(base_dir, backend, shard_id)
            st.info(f"äº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir} / ã‚³ãƒ”ãƒ¼: {', '.join(copied) if copied else 'ãªã—'}")

        deleted = []
        for name, p in targets:
            if p.exists():
                p.unlink()
                deleted.append(f"{name}: {p}")

        if deleted:
            st.success("å®Œå…¨åˆæœŸåŒ–ã—ã¾ã—ãŸ:\n" + "\n".join(f"- {x}" for x in deleted))
        else:
            st.info("å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    except Exception as e:
        st.error(f"å®Œå…¨åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒï¼ˆæ–°ãƒ«ãƒ¼ãƒˆï¼‰
# ============================================================
st.subheader("â™»ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ")
bdirs = list_backup_dirs_local(backend, shard_id)
if not bdirs:
    st.info("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
else:
    sel_bdir_restore = st.selectbox("å¾©å…ƒã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’é¸æŠ", bdirs, format_func=lambda p: p.name, key="restore_bdir")
    if sel_bdir_restore:
        st.dataframe(preview_backup_local(sel_bdir_restore), use_container_width=True, height=160)
        ok_restore = st.checkbox("å¾©å…ƒã«åŒæ„ã—ã¾ã™ï¼ˆç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸Šæ›¸ãã•ã‚Œã¾ã™ï¼‰", key="restore_ok")
        if st.button("â™»ï¸ å¾©å…ƒå®Ÿè¡Œ", type="primary", use_container_width=True, disabled=not ok_restore, key="restore_exec"):
            try:
                restored, missing = restore_from_backup_local(base_dir, sel_bdir_restore)
                msg = "å¾©å…ƒå®Œäº† âœ…\n" + "\n".join(f"- {x}" for x in restored)
                if missing:
                    msg += "\n\nå­˜åœ¨ã—ãªã‹ã£ãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é …ç›®:\n" + "\n".join(f"- {x}" for x in missing)
                st.success(msg)
            except Exception as e:
                st.error(f"å¾©å…ƒä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
