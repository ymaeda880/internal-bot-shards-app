# pages/03_pdfãƒ™ã‚¯ãƒˆãƒ«åŒ–.py
# ------------------------------------------------------------
# ğŸ“¥ data/pdf/<year>/*.pdf ã‚’å–ã‚Šè¾¼ã¿ã€data/vectorstore/<backend>/<year>/ ã«
#    vectors.npy / meta.jsonl ã‚’ â€œãƒšãƒ¼ã‚¸å˜ä½â€ ã§è¿½è¨˜ã™ã‚‹ã€‚
#    meta ã«ã¯ year ã¨ page ã‚’ä»˜ä¸ã€‚rag_utils.py ã® API ã«æº–æ‹ ã€‚
#    é‡è¦: meta.jsonl ã¸ã®è¿½è¨˜ã¯ NumpyVectorDB.add() ãŒè¡Œã†ãŸã‚ã€äºŒé‡è¿½è¨˜ã¯ã—ãªã„ã€‚
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from datetime import datetime
from typing import List, Tuple
import unicodedata
import re
import streamlit as st
import pdfplumber
import numpy as np

from rag_utils import split_text, EmbeddingStore, NumpyVectorDB, ProcessedFilesSimple

APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
PDF_ROOT = DATA_DIR / "pdf"           # data/pdf/<year>/*.pdf
VS_ROOT  = DATA_DIR / "vectorstore"   # data/vectorstore/<backend>/<year>/

# ---------- helpers ----------
CJK = r"\u3040-\u309F\u30A0-\u30FF\uFF65-\uFF9F\u4E00-\u9FFF\u3400-\u4DBF"
PUNC = r"ã€ã€‚ãƒ»ï¼Œï¼ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€Œã€ã€ã€ã€ˆã€‰ã€Šã€‹ã€ã€‘"

_cjk_cjk_space = re.compile(fr"(?<=[{CJK}])\s+(?=[{CJK}])")
_space_before_punc = re.compile(fr"\s+(?=[{PUNC}])")
_space_after_open = re.compile(fr"(?<=[ï¼ˆï¼»ï½›ã€Œã€ã€ˆã€Šã€])\s+")
_space_before_close = re.compile(fr"\s+(?=[ï¼‰ï¼½ï½ã€ã€ã€‰ã€‹ã€‘])")
_multi_space = re.compile(r"[ \t\u3000]+")

def normalize_ja_text(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _cjk_cjk_space.sub("", s)
    s = _space_before_punc.sub("", s)
    s = _space_after_open.sub("", s)
    s = _space_before_close.sub("", s)
    s = _multi_space.sub(" ", s)
    return s.strip()

def list_shards() -> List[str]:
    if not PDF_ROOT.exists():
        return []
    return sorted([p.name for p in PDF_ROOT.iterdir() if p.is_dir()])

def list_pdfs(shard_id: str) -> List[Path]:
    d = PDF_ROOT / shard_id
    if not d.exists():
        return []
    return sorted(d.glob("*.pdf"))

def ensure_vs_dir(backend: str, shard_id: str) -> Path:
    d = VS_ROOT / backend / shard_id
    d.mkdir(parents=True, exist_ok=True)
    return d

def get_vector_count(base_dir: Path) -> int:
    """vectors.npy ã®è¡Œæ•°ã‚’è¿”ã™ã€‚å­˜åœ¨ã—ãªã‘ã‚Œã° 0ã€‚"""
    p = base_dir / "vectors.npy"
    if not p.exists():
        return 0
    try:
        arr = np.load(p, mmap_mode="r")
        return int(arr.shape[0]) if arr.ndim == 2 else 0
    except Exception:
        return 0

# ---------- UI ----------
st.set_page_config(page_title="03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆãƒšãƒ¼ã‚¸å˜ä½ãƒ»ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰", page_icon="ğŸ§±", layout="wide")
st.title("ğŸ§± ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ˆ=ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰ã”ã¨ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆpage + yearä»˜ãï¼‰")

col1, col2, col3 = st.columns([1,1,2])
with col1:
    backend = st.radio("åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
with col2:
    chunk_size = st.number_input("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—ï¼‰", 200, 3000, 900, 50)
    overlap    = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—ï¼‰", 0, 600, 150, 10)
with col3:
    batch_size = st.number_input("åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒæ•°", 8, 512, 64, 8)
    st.caption("â€» OCRãŒå¿…è¦ãªPDFã¯ã€äº‹å‰ã«æ¤œç´¢å¯èƒ½PDFåŒ–ï¼ˆocrmypdf ç­‰ï¼‰ã—ã¦ãŠãã¨å®‰å®šã—ã¾ã™ã€‚")

shards = list_shards()
if not shards:
    st.warning("data/pdf/ é…ä¸‹ã«ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ˆ=ã‚·ãƒ£ãƒ¼ãƒ‰=å¹´åº¦ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¾‹: data/pdf/2025/*.pdf")
    st.stop()

with st.sidebar:
    st.subheader("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰")
    selected_shards = st.multiselect("è¤‡æ•°é¸æŠå¯", shards, default=shards)

run = st.button("é¸æŠã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å–ã‚Šè¾¼ã¿ï¼ˆè¿½è¨˜ï¼‰", type="primary")

# ---------- RUN ----------
if run:
    estore = EmbeddingStore(backend=backend)
    total_files = 0
    total_chunks = 0

    progress = st.progress(0.0, text="æº–å‚™ä¸­â€¦")

    for i_shard, shard_id in enumerate(selected_shards, start=1):
        st.markdown(f"### ğŸ“‚ ã‚·ãƒ£ãƒ¼ãƒ‰: `{shard_id}`")

        # ãƒ•ã‚©ãƒ«ãƒ€åã‚’ yearï¼ˆæ•°å€¤ï¼‰ã«
        try:
            year_val = int(shard_id)
        except ValueError:
            year_val = None

        vs_dir = ensure_vs_dir(backend, shard_id)
        tracker = ProcessedFilesSimple(vs_dir / "processed_files.json")
        vdb = NumpyVectorDB(vs_dir)

        shard_new_files = 0
        shard_new_chunks = 0

        pdf_files = list_pdfs(shard_id)
        if not pdf_files:
            st.info("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã«PDFãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            progress.progress(i_shard/len(selected_shards), text=f"{i_shard}/{len(selected_shards)} å®Œäº†")
            continue

        for pdf_path in pdf_files:
            name = pdf_path.name
            if tracker.is_done(name):
                continue

            try:
                with pdfplumber.open(str(pdf_path)) as pdf:
                    for page_no, page in enumerate(pdf.pages, start=1):
                        raw = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or ""
                        raw = raw.replace("\t", " ").replace("\xa0", " ")
                        text = " ".join(raw.split())
                        if not text:
                            continue

                        # split_text ã¯ (chunk, start, end)
                        spans: List[Tuple[str,int,int]] = split_text(
                            text, chunk_size=int(chunk_size), overlap=int(overlap)
                        )
                        if not spans:
                            continue

                        chunks = [s[0] for s in spans]
                        vectors: List[np.ndarray] = []
                        metas: List[dict] = []

                        for i in range(0, len(chunks), int(batch_size)):
                            batch = chunks[i:i+int(batch_size)]
                            vecs = estore.embed(batch, batch_size=int(batch_size)).astype("float32")
                            vectors.append(vecs)
                            for j, (ch, s, e) in enumerate(spans[i:i+int(batch_size)]):
                                metas.append({
                                    "file": f"{shard_id}/{name}",
                                    "year": year_val,
                                    "page": page_no,
                                    "chunk_id": f"{name}#p{page_no}-{i+j}",
                                    "chunk_index": i + j,
                                    "text": normalize_ja_text(ch),   # â˜…ã“ã“ã§æ­£è¦åŒ–
                                    "span_start": s,
                                    "span_end": e,
                                    "ingested_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
                                    "shard_id": shard_id
                                })

                        vec_mat = np.vstack(vectors) if len(vectors) > 1 else vectors[0]
                        vdb.add(vec_mat, metas)

                        shard_new_chunks += len(metas)

                tracker.mark_done(name)
                shard_new_files += 1

            except Exception as e:
                st.error(f"âŒ å–ã‚Šè¾¼ã¿å¤±æ•—: {name} : {e}")

        st.success(f"æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« {shard_new_files} ä»¶ / è¿½åŠ ãƒãƒ£ãƒ³ã‚¯ {shard_new_chunks} ä»¶")
        st.caption(f"ã‚·ãƒ£ãƒ¼ãƒ‰å†…ãƒ™ã‚¯ãƒˆãƒ«ç·æ•°ï¼ˆDBè¨ˆæ¸¬ï¼‰: {get_vector_count(vs_dir):,d}")

        total_files  += shard_new_files
        total_chunks += shard_new_chunks

        progress.progress(i_shard/len(selected_shards), text=f"{i_shard}/{len(selected_shards)} å®Œäº†")

    st.toast(f"âœ… å®Œäº†: æ–°è¦ {total_files} ãƒ•ã‚¡ã‚¤ãƒ« / {total_chunks} ãƒãƒ£ãƒ³ã‚¯ï¼ˆpage + yearä»˜ãï¼‰", icon="âœ…")
