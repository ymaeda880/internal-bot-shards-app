# pages/03_pdfãƒ™ã‚¯ãƒˆãƒ«åŒ–.py
# ------------------------------------------------------------
# ğŸ“¥ <SSD>/bot_data/pdf/<shard> ã‚’å–ã‚Šè¾¼ã¿ã€
#    ./data/vectorstore/<backend>/<shard>/ ã« vectors.npy / meta.jsonl ã‚’è¿½è¨˜ã€‚
#    meta ã«ã¯ year / page / embed_model ã‚’ä»˜ä¸ã€‚rag_utils.py ã® API ã«æº–æ‹ ã€‚
#    é‡è¦: meta.jsonl ã¸ã®è¿½è¨˜ã¯ NumpyVectorDB.add() ãŒè¡Œã†ãŸã‚ã€äºŒé‡è¿½è¨˜ã¯ã—ãªã„ã€‚
#    â€» OpenAI ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ text-embedding-3-large ã«å›ºå®šï¼ˆ3072 æ¬¡å…ƒï¼‰
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from datetime import datetime
from typing import List, Tuple
import unicodedata
import re
import json
import os

import streamlit as st
import pdfplumber
import numpy as np
import tiktoken

from config.config import PATHS, AVAILABLE_PRESETS, resolve_paths_for
from config import pricing
from lib.rag_utils import split_text, EmbeddingStore, NumpyVectorDB, ProcessedFilesSimple
from lib.vectorstore_utils import load_processed_files, save_processed_files  # æ—¢å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’æ´»ç”¨

# ============================================================
# å®šæ•°
# ============================================================
OPENAI_EMBED_MODEL = "text-embedding-3-large"  # â† å¤§å›ºå®šï¼ˆ3072 æ¬¡å…ƒï¼‰

# ============================================================
# æ—¥æœ¬èªæ­£è¦åŒ–ï¼ˆjapanese normalizationï¼‰
# ============================================================
CJK = r"\u3040-\u309F\u30A0-\u30FF\uFF65-\uFF9F\u4E00-\u9FFF\u3400-\u4DBF"
PUNC = r"ã€ã€‚ãƒ»ï¼Œï¼ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€Œã€ã€ã€ã€ˆã€‰ã€Šã€‹ã€ã€‘"
_cjk_cjk_space = re.compile(fr"(?<=[{CJK}])\s+(?=[{CJK}])")
_space_before_punc = re.compile(fr"\s+(?=[{PUNC}])")
_space_after_open = re.compile(fr"(?<=[ï¼ˆï¼»ï½›ã€Œã€ã€ˆã€Šã€])\s+")
_space_before_close = re.compile(fr"\s+(?=[ï¼‰ï¼½ï½ã€ã€ã€‰ã€‹ã€‘])")
_multi_space = re.compile(r"[ \t\u3000]+")

def normalize_ja_text(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _cjk_cjk_space.sub("", s)
    s = _space_before_punc.sub("", s)
    s = _space_after_open.sub("", s)
    s = _space_before_close.sub("", s)
    s = _multi_space.sub(" ", s)
    return s.strip()

# ============================================================
# tokenizerï¼ˆlarge ã«åˆã‚ã›ã‚‹ï¼‰
# ============================================================
enc = tiktoken.encoding_for_model(OPENAI_EMBED_MODEL)
def count_tokens(text: str) -> int:
    return len(enc.encode(text))

# ============================================================
# UI
# ============================================================
st.set_page_config(page_title="03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆãƒšãƒ¼ã‚¸å˜ä½ãƒ»ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰", page_icon="ğŸ§±", layout="wide")
st.title("ğŸ§± ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ˆ=ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰ã”ã¨ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆpage + yearä»˜ãï¼‰")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šlocation å¼·èª¿ + ãƒ©ã‚¸ã‚ªåˆ‡æ›¿ + æ•´å½¢è¡¨ç¤º ---
with st.sidebar:
    st.subheader("ğŸ““ ç¾åœ¨ã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³")
    idx0 = AVAILABLE_PRESETS.index(PATHS.preset) if PATHS.preset in AVAILABLE_PRESETS else 0
    ui_preset = st.radio(
        "Locationï¼ˆã“ã®å®Ÿè¡Œä¸­ã®ã¿åˆ‡æ›¿ï¼‰",
        AVAILABLE_PRESETS,
        index=idx0,
        horizontal=True,
        help="secrets.toml ã® [mounts] ã§å®šç¾©ã•ã‚ŒãŸãƒã‚¦ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚",
    )

EFFECTIVE = resolve_paths_for(ui_preset, PATHS.app_root) if ui_preset != PATHS.preset else PATHS

with st.sidebar:
    st.markdown(f"### ğŸ§­ Location: **{ui_preset}**")
    st.markdown("#### ğŸ“‚ è§£æ±ºãƒ‘ã‚¹ï¼ˆã‚³ãƒ”ãƒ¼å¯ï¼‰")
    st.text_input("ssd_path", str(EFFECTIVE.ssd_path), key="p_ssd", disabled=True)
    st.text_input("PDF_ROOT", str(EFFECTIVE.pdf_root), key="p_pdf", disabled=True)
    st.text_input("BACKUP_ROOT", str(EFFECTIVE.backup_root), key="p_bak", disabled=True)
    st.text_input("VS_ROOT", str(EFFECTIVE.vs_root), key="p_vs", disabled=True)

PDF_ROOT: Path = EFFECTIVE.pdf_root
VS_ROOT: Path  = EFFECTIVE.vs_root

# --- ãã®ä»– UI ---
col1, col2, col3 = st.columns([1, 1, 2])
with col1:
    backend = st.radio("åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
    if backend == "openai":
        st.caption(f"ğŸ”§ Embedding ãƒ¢ãƒ‡ãƒ«ã¯ **{OPENAI_EMBED_MODEL}ï¼ˆ3072æ¬¡å…ƒï¼‰å›ºå®š**")
with col2:
    chunk_size = st.number_input("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—ï¼‰", 200, 3000, 900, 50)
    overlap    = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—ï¼‰", 0, 600, 150, 10)
with col3:
    batch_size = st.number_input("åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒæ•°", 8, 512, 64, 8)
    st.caption("â€» OCRãŒå¿…è¦ãªPDFã¯ã€äº‹å‰ã«æ¤œç´¢å¯èƒ½PDFåŒ–ï¼ˆocrmypdf ç­‰ï¼‰ã—ã¦ãŠãã¨å®‰å®šã—ã¾ã™ã€‚")

st.info(
    "PDF å…¥åŠ›: `<ssd>/bot_data/pdf/<shard>`ï¼ˆlocation ã«ã‚ˆã‚Šè‡ªå‹•åˆ‡æ›¿ï¼‰\n"
    "å‡ºåŠ›: `./data/vectorstore/<backend>/<shard>/`ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã”ã¨ã«åˆ†é›¢ï¼‰"
)

# ============================================================
# ãƒ‘ã‚¹ãƒ˜ãƒ«ãƒ‘ï¼ˆpath helpersï¼‰
# ============================================================
def list_shards() -> List[str]:
    if not PDF_ROOT.exists():
        return []
    return sorted([p.name for p in PDF_ROOT.iterdir() if p.is_dir()])

def list_pdfs(shard_id: str) -> List[Path]:
    d = PDF_ROOT / shard_id
    if not d.exists():
        return []
    return sorted(d.glob("*.pdf"))

def ensure_vs_dir(backend: str, shard_id: str) -> Path:
    d = VS_ROOT / backend / shard_id
    d.mkdir(parents=True, exist_ok=True)
    return d

def get_vector_count(base_dir: Path) -> int:
    p = base_dir / "vectors.npy"
    if not p.exists():
        return 0
    try:
        arr = np.load(p, mmap_mode="r")
        return int(arr.shape[0]) if arr.ndim == 2 else 0
    except Exception:
        return 0

# ============================================================
# processed_files.json ã® canon åŒ–ï¼ˆshard/filenameï¼‰ã‚’ä¿è¨¼
# ============================================================
def migrate_processed_files_to_canonical(pf_json: Path, shard_id: str) -> None:
    """
    processed_files.json ã‚’ 'shard/filename' å½¢å¼ã«æ­£è¦åŒ–ï¼ˆcanonicalizeï¼‰ã™ã‚‹ã€‚
    - æ–‡å­—åˆ—è¡¨ç¾: 'file.pdf' â†’ 'shard/file.pdf'
    - dict è¡¨ç¾: file/path/name ã®ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ã«å…¥ã£ã¦ã„ã‚‹å ´åˆã«è£œå®Œ
    - é‡è¤‡ã¯é™¤å»
    """
    pf_list = load_processed_files(pf_json)
    if not pf_list:
        return

    changed = False
    canonical_entries = []

    for entry in pf_list:
        if isinstance(entry, str):
            val = entry
        elif isinstance(entry, dict):
            val = entry.get("file") or entry.get("path") or entry.get("name")
        else:
            continue

        if not val:
            continue

        if "/" not in val:
            val = f"{shard_id}/{val}"
            changed = True

        canonical_entries.append(val)

    # é‡è¤‡é™¤å»
    dedup = []
    seen = set()
    for v in canonical_entries:
        if v in seen:
            continue
        seen.add(v)
        dedup.append(v)

    if changed:
        save_processed_files(pf_json, dedup)

# ============================================================
# ã‚·ãƒ£ãƒ¼ãƒ‰ç¢ºèª
# ============================================================
shards = list_shards()
if not shards:
    st.warning(f"{PDF_ROOT} é…ä¸‹ã«ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ˆ=ã‚·ãƒ£ãƒ¼ãƒ‰=å¹´åº¦ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¾‹: {PDF_ROOT}/2025/*.pdf")
    st.stop()

with st.sidebar:
    st.subheader("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰")
    selected_shards = st.multiselect("è¤‡æ•°é¸æŠå¯", shards, default=shards)

st.info("PDF ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®å„ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã‚’ã‚·ãƒ£ãƒ¼ãƒ‰ã¨ã—ã¦å–ã‚Šè¾¼ã¿ã¾ã™ã€‚æ—¢ã«å–ã‚Šè¾¼ã‚“ã  PDF ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

run = st.button("é¸æŠã‚·ãƒ£ãƒ¼ãƒ‰å†…ã® PDF ã‚’å–ã‚Šè¾¼ã¿", type="primary")

# ============================================================
# å®Ÿè¡Œ
# ============================================================
if run:
    # âœ… EmbeddingStore ã¯ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ openai_model ã‚’å—ã‘å–ã‚‹ï¼ˆembed() ã« model= ã¯æ¸¡ã•ãªã„ï¼‰
    estore = EmbeddingStore(backend=backend, openai_model=OPENAI_EMBED_MODEL)
    total_files = 0
    total_chunks = 0

    # é€²æ—è¡¨ç¤ºã‚’å¼·åŒ–ï¼šå…¨ä½“ç”¨ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ»ç¾åœ¨çŠ¶æ³ãƒ†ã‚­ã‚¹ãƒˆ
    overall_progress = st.progress(0.0, text="æº–å‚™ä¸­â€¦")
    file_progress = st.progress(0.0, text="ãƒ•ã‚¡ã‚¤ãƒ«é€²æ—ï¼šå¾…æ©Ÿä¸­â€¦")
    status_current = st.empty()  # ç¾åœ¨ã‚·ãƒ£ãƒ¼ãƒ‰ / ãƒ•ã‚¡ã‚¤ãƒ« / ãƒšãƒ¼ã‚¸ã‚’é€æ¬¡è¡¨ç¤º

    num_shards = len(selected_shards)

    for i_shard, shard_id in enumerate(selected_shards, start=1):
        st.markdown(f"### ğŸ“‚ ã‚·ãƒ£ãƒ¼ãƒ‰: `{shard_id}`")

        try:
            year_val = int(shard_id)
        except ValueError:
            year_val = None

        vs_dir = ensure_vs_dir(backend, shard_id)
        tracker = ProcessedFilesSimple(vs_dir / "processed_files.json")
        vdb = NumpyVectorDB(vs_dir)

        # âœ… å–ã‚Šè¾¼ã¿å‰ã« PF ã‚’ canon åŒ–ï¼ˆæ—§ãƒ‡ãƒ¼ã‚¿ï¼šãƒ•ã‚¡ã‚¤ãƒ«åã ã‘ â†’ shard/filenameï¼‰
        migrate_processed_files_to_canonical(vs_dir / "processed_files.json", shard_id)

        shard_new_files = 0
        shard_new_chunks = 0

        pdf_files = list_pdfs(shard_id)
        n_files = len(pdf_files)
        if n_files == 0:
            st.info("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã« PDF ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            overall_progress.progress(i_shard / num_shards, text=f"å…¨ä½“ {i_shard}/{num_shards} ã‚·ãƒ£ãƒ¼ãƒ‰å®Œäº†")
            continue

        for i_file, pdf_path in enumerate(pdf_files, start=1):
            name = pdf_path.name
            key_full = f"{shard_id}/{name}"  # âœ… æ­£æº–ã‚­ãƒ¼ï¼ˆmeta.jsonl ã¨æƒãˆã‚‹ï¼‰

            # äº’æ›: æ—§ã‚­ãƒ¼ï¼ˆname ã®ã¿ï¼‰ã‚‚ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã«å«ã‚ã‚‹
            if tracker.is_done(key_full) or tracker.is_done(name):
                status_current.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: `{shard_id}` / **{name}**ï¼ˆæ—¢ã«å–ã‚Šè¾¼ã¿æ¸ˆã¿ï¼‰")
                # ãƒ•ã‚¡ã‚¤ãƒ«é€²æ—ã¯100%ã«ã—ã¦ã‹ã‚‰æ¬¡ã¸
                file_progress.progress(1.0, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{n_files} å®Œäº†: {name}")
                continue

            try:
                with pdfplumber.open(str(pdf_path)) as pdf:
                    total_pages = max(len(pdf.pages), 1)
                    status_current.info(f"ğŸ“¥ å–ã‚Šè¾¼ã¿é–‹å§‹: `{shard_id}` / **{name}**ï¼ˆ{i_file}/{n_files}ï¼‰ å…¨{total_pages}ãƒšãƒ¼ã‚¸")
                    file_progress.progress(0.0, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{n_files}: {name} - 0/{total_pages} ãƒšãƒ¼ã‚¸")

                    for page_no, page in enumerate(pdf.pages, start=1):
                        raw = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or ""
                        raw = raw.replace("\t", " ").replace("\xa0", " ")
                        text = " ".join(raw.split())
                        if not text:
                            # ç©ºãƒšãƒ¼ã‚¸ã§ã‚‚é€²æ—ã ã‘æ›´æ–°
                            file_progress.progress(page_no / total_pages, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{n_files}: {name} - {page_no}/{total_pages} ãƒšãƒ¼ã‚¸")
                            continue

                        # æ­£è¦åŒ– â†’ åˆ†å‰²
                        text = normalize_ja_text(text)
                        spans: List[Tuple[str, int, int]] = split_text(
                            text, chunk_size=int(chunk_size), overlap=int(overlap)
                        )
                        if not spans:
                            file_progress.progress(page_no / total_pages, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{n_files}: {name} - {page_no}/{total_pages} ãƒšãƒ¼ã‚¸")
                            continue

                        chunks = [s[0] for s in spans]
                        vectors: List[np.ndarray] = []
                        metas: List[dict] = []

                        for i in range(0, len(chunks), int(batch_size)):
                            batch = chunks[i:i + int(batch_size)]
                            vecs = estore.embed(batch, batch_size=int(batch_size)).astype("float32")
                            vectors.append(vecs)
                            for j, (ch, s, e) in enumerate(spans[i:i + int(batch_size)]):
                                metas.append({
                                    "file": key_full,
                                    "year": year_val,
                                    "page": page_no,
                                    "chunk_id": f"{name}#p{page_no}-{i+j}",
                                    "chunk_index": i + j,
                                    "text": ch,
                                    "span_start": s,
                                    "span_end": e,
                                    "chunk_len_tokens": count_tokens(ch),
                                    "ingested_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
                                    "shard_id": shard_id,
                                    "embed_model": OPENAI_EMBED_MODEL if backend == "openai" else "local-model",
                                })

                        vec_mat = np.vstack(vectors) if len(vectors) > 1 else vectors[0]
                        vdb.add(vec_mat, metas)
                        shard_new_chunks += len(metas)

                        # ãƒšãƒ¼ã‚¸åˆ°é”ã®ãŸã³ã«ãƒ•ã‚¡ã‚¤ãƒ«é€²æ—æ›´æ–°
                        file_progress.progress(page_no / total_pages, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{n_files}: {name} - {page_no}/{total_pages} ãƒšãƒ¼ã‚¸")

                # å–ã‚Šè¾¼ã¿å®Œäº†ã‚’æ­£æº–ã‚­ãƒ¼ã§è¨˜éŒ²
                tracker.mark_done(key_full)
                shard_new_files += 1
                status_current.success(f"âœ… å®Œäº†: `{shard_id}` / **{name}**ï¼ˆ{i_file}/{n_files}ï¼‰")

            except Exception as e:
                st.error(f"âŒ å–ã‚Šè¾¼ã¿å¤±æ•—: {name} : {e}")
                status_current.error(f"âŒ å¤±æ•—: `{shard_id}` / **{name}** - {e}")

            # ã‚·ãƒ£ãƒ¼ãƒ‰å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤çµ‚ã‚ã‚‹ã”ã¨ã«å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°
            overall_progress.progress(
                (i_shard - 1 + i_file / max(n_files, 1)) / num_shards,
                text=f"å…¨ä½“ {i_shard}/{num_shards} ã‚·ãƒ£ãƒ¼ãƒ‰å‡¦ç†ä¸­â€¦ï¼ˆ{shard_id}: {i_file}/{n_files} ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰"
            )

        st.success(f"æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« {shard_new_files} ä»¶ / è¿½åŠ ãƒãƒ£ãƒ³ã‚¯ {shard_new_chunks} ä»¶")
        st.caption(f"ã‚·ãƒ£ãƒ¼ãƒ‰å†…ãƒ™ã‚¯ãƒˆãƒ«ç·æ•°ï¼ˆDBè¨ˆæ¸¬ï¼‰: {get_vector_count(vs_dir):,d}")

        # ã‚·ãƒ£ãƒ¼ãƒ‰å®Œäº†æ™‚ã«å…¨ä½“é€²æ—ã‚’æ›´æ–°
        overall_progress.progress(i_shard / num_shards, text=f"å…¨ä½“ {i_shard}/{num_shards} ã‚·ãƒ£ãƒ¼ãƒ‰å®Œäº†")

        total_files  += shard_new_files
        total_chunks += shard_new_chunks

    st.toast(f"âœ… å®Œäº†: æ–°è¦ {total_files} ãƒ•ã‚¡ã‚¤ãƒ« / {total_chunks} ãƒãƒ£ãƒ³ã‚¯ï¼ˆpage + yearä»˜ãï¼‰", icon="âœ…")

    # ---------- æ–™é‡‘è¨ˆç®—ï¼ˆpricingï¼‰ ----------
    if total_chunks > 0:
        if backend == "openai":
            total_tokens = 0
            # é¸æŠã‚·ãƒ£ãƒ¼ãƒ‰ã® meta.jsonl ã‹ã‚‰ chunk_len_tokens ã‚’åˆç®—
            for shard_id in selected_shards:
                vs_dir = ensure_vs_dir(backend, shard_id)
                meta_path = vs_dir / "meta.jsonl"
                if not meta_path.exists():
                    continue
                with meta_path.open("r", encoding="utf-8") as f:
                    for line in f:
                        try:
                            obj = json.loads(line)
                        except Exception:
                            continue
                        total_tokens += int(obj.get("chunk_len_tokens", 0))

            model = OPENAI_EMBED_MODEL  # large å›ºå®š
            usd = pricing.estimate_embedding_cost_usd(total_tokens, model)
            jpy = pricing.estimate_embedding_cost_jpy(total_tokens, model)

            st.markdown("### ğŸ’° åŸ‹ã‚è¾¼ã¿ã‚³ã‚¹ãƒˆã®æ¦‚ç®—")
            st.write(f"- ãƒ¢ãƒ‡ãƒ«: **{model}**")
            st.write(f"- ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens:,}")
            st.write(f"- æ¦‚ç®—ã‚³ã‚¹ãƒˆ: `${usd:.4f}` â‰ˆ Â¥{jpy:,.0f}")
        else:
            st.markdown("### ğŸ’° åŸ‹ã‚è¾¼ã¿ã‚³ã‚¹ãƒˆã®æ¦‚ç®—")
            st.info("local backend ã®ãŸã‚ã‚³ã‚¹ãƒˆã¯ç™ºç”Ÿã—ã¾ã›ã‚“ã€‚")
