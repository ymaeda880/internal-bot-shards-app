# pages/52_ã‚·ãƒ£ãƒ¼ãƒ‰å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯.py
# ------------------------------------------------------------
# ğŸ§© ã‚·ãƒ£ãƒ¼ãƒ‰å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®¹é‡ãƒ»ä»¶æ•°ãƒ»æ•´åˆæ€§ï¼‰
# - data/vectorstore/<backend>/<shard_id>/ ã‚’èµ°æŸ»
# - vectors.npy ã® (è¡Œæ•°n, æ¬¡å…ƒd), ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º, meta.jsonlè¡Œæ•° ã‚’å–å¾—
# - ã—ãã„å€¤ã§ OK / WARN / NG ã‚’åˆ¤å®šï¼ˆç†ç”±ã¤ãï¼‰
# - RAMç›®å®‰ã‹ã‚‰ã€Œæ¨å¥¨æœ€å¤§ãƒ™ã‚¯ãƒˆãƒ«æ•°/ã‚·ãƒ£ãƒ¼ãƒ‰ã€ã‚’è‡ªå‹•è©¦ç®—ï¼ˆå¯ç·¨é›†ï¼‰
# - åˆè¨ˆã‚µã‚¤ã‚ºã¯ B/KB/MB/GB ã§äººé–“å‘ã‘è¡¨ç¤ºï¼ˆå°‘é‡ã§ã‚‚ 0.00GB ã«ãªã‚‰ãªã„ï¼‰
# ------------------------------------------------------------
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, List, Tuple
import math

import numpy as np
import streamlit as st

# ========== ãƒšãƒ¼ã‚¸è¨­å®š ==========
st.set_page_config(page_title="Shard Health Check", page_icon="ğŸ§©", layout="wide")

# ========== ãƒ‘ã‚¹è¦ç´„ï¼ˆæ—¢å­˜ã‚¢ãƒ—ãƒªã¨åŒä¸€æƒ³å®šï¼‰ ==========
APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
VS_ROOT  = DATA_DIR / "vectorstore"   # data/vectorstore/<backend>/<shard_id>/

# ========== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==========
def help_popover(title: str, content_md: str) -> None:
    """Streamlit 1.31+ ã§ã¯ popoverã€ãã‚Œã‚ˆã‚Šå¤ã„ç‰ˆã§ã¯ expander ã‚’ä½¿ã†"""
    pop = getattr(st, "popover", None)
    if callable(pop):
        with st.popover(title):
            st.markdown(content_md)
    else:
        with st.expander(title):
            st.markdown(content_md)

def list_shard_dirs(backend: str) -> List[Path]:
    base = VS_ROOT / backend
    if not base.exists():
        return []
    return sorted([p for p in base.iterdir() if p.is_dir()])

def sizeof_fmt(num: float) -> str:
    """äººé–“å‘ã‘ã®ã‚µã‚¤ã‚ºè¡¨è¨˜ï¼ˆB/KB/MB/GB/TBï¼‰"""
    for unit in ["B", "KB", "MB", "GB", "TB", "PB"]:
        if abs(num) < 1024.0:
            return f"{num:,.2f} {unit}"
        num /= 1024.0
    return f"{num:,.2f} EB"

def load_vector_shape(vec_path: Path) -> Tuple[int, int]:
    """mmapã§ shape ã ã‘å–å¾—ï¼ˆRAMã«è¼‰ã›ãªã„ï¼‰"""
    arr = np.load(vec_path, mmap_mode="r")
    shape = tuple(arr.shape)
    if len(shape) == 1:
        # æƒ³å®šå¤–ï¼ˆ(n,)ï¼‰ã®å ´åˆã¯ (n, 1) æ‰±ã„
        return (shape[0], 1)
    return (shape[0], shape[1])

def count_jsonl_lines(jsonl_path: Path) -> int:
    try:
        with jsonl_path.open("r", encoding="utf-8") as f:
            return sum(1 for _ in f)
    except FileNotFoundError:
        return 0

def estimate_ram_for_vectors(n: int, d: int, dtype_bytes: int = 4) -> float:
    """float32=4bytes åŸºæº–ã§ RAM æ¶ˆè²»è¦‹ç©ã‚Šï¼ˆGBï¼‰"""
    return n * d * dtype_bytes / (1024**3)

def judge_status(row: Dict[str, Any],
                 max_vectors_per_shard: int,
                 max_vectors_file_gb: float,
                 mismatch_tol_pct: float,
                 est_ram_limit_gb: float) -> Tuple[str, List[str]]:
    """
    è¿”ã‚Šå€¤: (status, reasons[])
    - status: "OK" | "WARN" | "NG"
    """
    reasons = []
    n = int(row["n_vectors"])
    d = int(row["dim"])
    size_gb = float(row["vectors_npy_gb"])
    meta = int(row["meta_rows"])
    mismatch = abs(n - meta)
    mismatch_pct = (mismatch / max(1, n)) * 100.0 if n > 0 else 0.0
    est_ram_gb = float(row["est_ram_gb"])

    # ãƒã‚§ãƒƒã‚¯1: ä»¶æ•°ã—ãã„å€¤
    if n > max_vectors_per_shard * 1.5:
        reasons.append(f"ãƒ™ã‚¯ãƒˆãƒ«æ•°ãŒä¸Šé™ã®150%è¶…ï¼ˆ{n:,} > {int(max_vectors_per_shard*1.5):,}ï¼‰")
    elif n > max_vectors_per_shard:
        reasons.append(f"ãƒ™ã‚¯ãƒˆãƒ«æ•°ãŒä¸Šé™è¶…ï¼ˆ{n:,} > {max_vectors_per_shard:,}ï¼‰")

    # ãƒã‚§ãƒƒã‚¯2: vectors.npy ã®ç‰©ç†ã‚µã‚¤ã‚º
    if size_gb > max_vectors_file_gb * 1.5:
        reasons.append(f"vectors.npy ãŒä¸Šé™ã®150%è¶…ï¼ˆ{size_gb:.2f}GB > {max_vectors_file_gb*1.5:.2f}GBï¼‰")
    elif size_gb > max_vectors_file_gb:
        reasons.append(f"vectors.npy ãŒä¸Šé™è¶…ï¼ˆ{size_gb:.2f}GB > {max_vectors_file_gb:.2f}GBï¼‰")

    # ãƒã‚§ãƒƒã‚¯3: n ã¨ meta.jsonl è¡Œæ•°ã®ä¸æ•´åˆ
    if mismatch_pct > mismatch_tol_pct * 2:
        reasons.append(f"metaä¸æ•´åˆãŒé–¾å€¤ã®2å€è¶…ï¼ˆ{mismatch_pct:.1f}% > {mismatch_tol_pct*2:.1f}%ï¼‰")
    elif mismatch_pct > mismatch_tol_pct:
        reasons.append(f"metaä¸æ•´åˆãŒé–¾å€¤è¶…ï¼ˆ{mismatch_pct:.1f}% > {mismatch_tol_pct:.1f}%ï¼‰")

    # ãƒã‚§ãƒƒã‚¯4: RAMè¦‹ç©ã‚Šï¼ˆå®‰å…¨åŸŸè¶…éï¼‰
    if est_ram_gb > est_ram_limit_gb * 1.5:
        reasons.append(f"RAMè¦‹ç©ã‚ŠãŒå®‰å…¨ä¸Šé™ã®150%è¶…ï¼ˆ{est_ram_gb:.2f}GB > {est_ram_limit_gb*1.5:.2f}GBï¼‰")
    elif est_ram_gb > est_ram_limit_gb:
        reasons.append(f"RAMè¦‹ç©ã‚ŠãŒå®‰å…¨ä¸Šé™è¶…ï¼ˆ{est_ram_gb:.2f}GB > {est_ram_limit_gb:.2f}GBï¼‰")

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ±ºå®š
    if any("150%è¶…" in r for r in reasons):
        status = "NG"
    elif reasons:
        status = "WARN"
    else:
        status = "OK"
    return status, reasons

def plan_split(n: int, max_per_shard: int) -> Tuple[int, List[Tuple[int, int]]]:
    """å¿…è¦åˆ†å‰²æ•°ã¨å„ã‚·ãƒ£ãƒ¼ãƒ‰ã®ä»¶æ•°å‰²å½“ï¼ˆã»ã¼å‡ç­‰ï¼‰"""
    if n <= max_per_shard:
        return 1, [(n, 0)]
    k = math.ceil(n / max_per_shard)
    base = n // k
    rem  = n % k
    plan = []
    for i in range(k):
        cnt = base + (1 if i < rem else 0)
        plan.append((cnt, i))
    return k, plan

# ========== ã‚¿ã‚¤ãƒˆãƒ«ï¼‹ãƒ˜ãƒ«ãƒ— ==========
c1, c2 = st.columns([1, 0.14])
with c1:
    st.title("ğŸ§© ã‚·ãƒ£ãƒ¼ãƒ‰å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®¹é‡ãƒ»ä»¶æ•°ãƒ»æ•´åˆæ€§ï¼‰")
with c2:
    help_popover(
        "RAMè¦‹ç©ã¨ã¯ï¼Ÿ",
        r"""
**RAMè¦‹ç©** = ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«å±•é–‹ã—ãŸã¨ãã®æ¦‚ç®—ï¼ˆGBï¼‰ã€‚

**è¨ˆç®—å¼**ï¼š
$$
RAM_{GB}=\frac{n\times d\times 4}{1024^3}
$$
- *n*: ãƒ™ã‚¯ãƒˆãƒ«æ•°ï¼ˆè¡Œæ•°ï¼‰  
- *d*: æ¬¡å…ƒæ•°ï¼ˆåˆ—æ•°ï¼‰  
- 4: float32ï¼ˆ1è¦ç´ ã‚ãŸã‚Š4ãƒã‚¤ãƒˆï¼‰

**ä¾‹**ï¼š300,000 Ã— 768 â‰’ 0.86 GB  
**ç›®çš„**ï¼šæ­è¼‰RAMã¨æ¯”è¼ƒã—ã€ã‚·ãƒ£ãƒ¼ãƒ‰ãŒå¤§ãã™ããªã„ã‹ã‚’åˆ¤å®šã€‚
"""
    )

# ========== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ==========
with st.sidebar:
    st.header("è¨­å®š")

    backend_label = st.radio(
        "åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆæ ¼ç´å…ˆï¼‰",
        ["local (sentence-transformers)", "openai"],
        index=1,
        help="data/vectorstore/<backend>/ ä»¥ä¸‹ã‚’èµ°æŸ»ã—ã¾ã™ã€‚",
    )
    backend = "openai" if backend_label.startswith("openai") else "local"

    # ãƒ¡ãƒ¢ãƒªå®‰å…¨åŸŸ
    st.markdown("### ãƒ¡ãƒ¢ãƒªå®‰å…¨åŸŸï¼ˆæ¨å¥¨ï¼‰")
    ram_gb = st.number_input(
        "æ­è¼‰RAM (GB)", min_value=4.0, max_value=1024.0, value=32.0, step=1.0,
        help="ç‰©ç†RAMå®¹é‡ã€‚Dockerã‚„ä»–ãƒ—ãƒ­ã‚»ã‚¹ã‚‚ä½¿ã†ãŸã‚ã€å®‰å…¨ä¿‚æ•°ã§ä½™è£•ã‚’è¦‹ã¾ã™ã€‚",
    )
    safety = st.slider(
        "å®‰å…¨ä¿‚æ•°ï¼ˆä½¿ç”¨ä¸Šé™ã®å‰²åˆï¼‰", 0.1, 0.9, 0.5, 0.05,
        help="RAMè¦‹ç©ã®åˆè¨ˆãŒã€æ­è¼‰RAMÃ—å®‰å…¨ä¿‚æ•°ã€ã‚’è¶…ãˆã‚‹ã¨WARN/NGåˆ¤å®šã«ãªã‚Šã¾ã™ã€‚",
    )
    est_ram_limit_gb = ram_gb * safety
    st.caption(f"æ¨å¥¨ã€æ¤œç´¢æ™‚ã«ä½¿ã£ã¦ã‚ˆã„ãƒ™ã‚¯ãƒˆãƒ«RAMåˆè¨ˆä¸Šé™ã€ â‰ˆ **{est_ram_limit_gb:.1f} GB**")

    # ã—ãã„å€¤
    st.markdown("### ã‚·ãƒ£ãƒ¼ãƒ‰ã—ãã„å€¤")
    max_vectors_file_gb = st.number_input(
        "vectors.npy 1ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨å¥¨ä¸Šé™ (GB)", min_value=0.1, max_value=64.0, value=2.0, step=0.1,
        help="å¤§ãã™ãã‚‹å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¯é…å¸ƒãƒ»è»¢é€ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã§ä¸åˆ©ã€ã¨ã„ã†é‹ç”¨ç›®å®‰ã€‚",
    )
    max_vectors_per_shard = st.number_input(
        "ãƒ™ã‚¯ãƒˆãƒ«æ•°/ã‚·ãƒ£ãƒ¼ãƒ‰ã®æ¨å¥¨ä¸Šé™ (ä»¶)", min_value=10_000, max_value=5_000_000, value=300_000, step=10_000,
        help="åŒæ™‚ãƒ­ãƒ¼ãƒ‰ã‚„å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’æƒ³å®šã—ãŸé‹ç”¨ä¸Šã®é–¾å€¤ã€‚",
    )
    mismatch_tol_pct = st.slider(
        "metaä¸æ•´åˆã®è¨±å®¹ç‡(%)", 0.0, 20.0, 2.0, 0.5,
        help="vectors.npy ã® nï¼ˆè¡Œæ•°ï¼‰ã¨ meta.jsonl ã®è¡Œæ•°ã®ã‚ºãƒ¬ãŒã“ã®å‰²åˆã‚’è¶…ãˆã‚‹ã¨WARN/NGã€‚",
    )

    st.markdown("### è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    sample_rows = st.number_input(
        "ç¢ºèªã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºï¼ˆè¡Œï¼‰", min_value=0, max_value=50, value=0, step=1,
        help="0=æŠ½å‡ºã—ãªã„ã€‚meta.jsonl ã®å†’é ­ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚",
    )

st.caption(f"ã‚¹ã‚­ãƒ£ãƒ³å¯¾è±¡: **{VS_ROOT} / {backend}**")

# ========== å…¥åŠ›æ¤œè¨¼ ==========
base_dir = VS_ROOT / backend
if not base_dir.exists():
    st.error(f"ãƒ™ã‚¯ãƒˆãƒ«ãƒ«ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_dir}")
    st.stop()

shards = list_shard_dirs(backend)
if not shards:
    st.warning("ã‚·ãƒ£ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã¾ãšã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ========== åé›† ==========
rows: List[Dict[str, Any]] = []
total_vectors = 0
total_ram_est_gb = 0.0
total_size_bytes = 0  # â† bytesã§åˆè¨ˆ

for shp in shards:
    vec = shp / "vectors.npy"
    meta = shp / "meta.jsonl"
    if not vec.exists():
        continue

    try:
        n, d = load_vector_shape(vec)
    except Exception as e:
        st.warning(f"{shp.name}: vectors.npy ã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼: {e}")
        n, d = 0, 0

    meta_rows = count_jsonl_lines(meta)
    size_bytes = vec.stat().st_size if vec.exists() else 0
    size_gb = size_bytes / (1024**3)

    est_ram_gb = estimate_ram_for_vectors(n, d, dtype_bytes=4)

    row = dict(
        shard_id=shp.name,
        n_vectors=n,
        dim=d,
        vectors_npy=size_bytes,
        vectors_npy_gb=size_gb,
        meta_rows=meta_rows,
        mismatch=n - meta_rows,
        est_ram_gb=est_ram_gb,
        path=str(shp),
    )

    status, reasons = judge_status(
        row,
        max_vectors_per_shard=int(max_vectors_per_shard),
        max_vectors_file_gb=float(max_vectors_file_gb),
        mismatch_tol_pct=float(mismatch_tol_pct),
        est_ram_limit_gb=float(est_ram_limit_gb),
    )
    row["status"] = status
    row["reasons"] = "; ".join(reasons) if reasons else ""

    rows.append(row)

    total_vectors += n
    total_ram_est_gb += est_ram_gb
    total_size_bytes += size_bytes  # â† bytesã§åŠ ç®—

# ========== è¡¨ç¤º ==========
st.subheader("ğŸ“Š ã‚·ãƒ£ãƒ¼ãƒ‰ä¸€è¦§")
if not rows:
    st.info("vectors.npy ã‚’å«ã‚€ã‚·ãƒ£ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()

# å±é™ºåº¦é«˜â†’ä½ã§ä¸¦ã¹æ›¿ãˆ
priority = {"NG": 0, "WARN": 1, "OK": 2}
rows_sorted = sorted(rows, key=lambda r: (priority.get(r["status"], 3), -r["n_vectors"]))

# ãƒ†ãƒ¼ãƒ–ãƒ«ä¸Šéƒ¨ã®è£œåŠ©ãƒ˜ãƒ«ãƒ—
hc1, hc2 = st.columns([1, 0.18])
with hc1:
    st.caption("åˆ—ã€RAMè¦‹ç©ã€ã¯ã€ã‚·ãƒ£ãƒ¼ãƒ‰å˜ä½“ã‚’ float32 ã§å±•é–‹ã—ãŸå ´åˆã®æ¦‚ç®—ãƒ¡ãƒ¢ãƒªé‡ã§ã™ã€‚")
with hc2:
    help_popover(
        "RAMè¦‹ç©ã¨ã¯ï¼Ÿ",
        r"""
**RAMè¦‹ç©**ï¼šãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«å±•é–‹ã—ãŸã¨ãã®æ¦‚ç®—ï¼ˆGBï¼‰ã€‚  
$$
RAM_{GB}=\frac{n\times d\times 4}{1024^3}
$$
ï¼ˆ4 = float32 ã®ãƒã‚¤ãƒˆæ•°ï¼‰

**ä¾‹**ï¼š300,000 Ã— 768 â‰’ 0.86 GB  
**æ¯”è¼ƒå¯¾è±¡**ï¼šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€æ­è¼‰RAMÃ—å®‰å…¨ä¿‚æ•°ã€ã€‚
"""
    )

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
def status_icon(s: str) -> str:
    return {"OK": "ğŸŸ¢ OK", "WARN": "ğŸŸ¡ WARN", "NG": "ğŸ”´ NG"}.get(s, s)

table = []
for r in rows_sorted:
    table.append({
        "ã‚·ãƒ£ãƒ¼ãƒ‰": r["shard_id"],
        "çŠ¶æ…‹": status_icon(r["status"]),
        "ãƒ™ã‚¯ãƒˆãƒ«æ•° n": f'{r["n_vectors"]:,}',
        "æ¬¡å…ƒ d": r["dim"],
        "vectors.npy": sizeof_fmt(float(r["vectors_npy"])) + f" ({r['vectors_npy_gb']:.2f} GB)",
        "meta.jsonl è¡Œæ•°": f'{r["meta_rows"]:,}',
        "ä¸ä¸€è‡´ (n - meta)": f'{r["mismatch"]:+,}',
        "RAMè¦‹ç©": f'{r["est_ram_gb"]:.2f} GB',
        "ç†ç”±": r["reasons"],
        "ãƒ‘ã‚¹": r["path"],
    })

st.dataframe(table, use_container_width=True)

# ===== åˆè¨ˆ / æ¦‚æ³ï¼ˆå°‘é‡ã§ã‚‚ 0.00GB ã«ãªã‚‰ãªã„è¡¨ç¤ºï¼‰ =====
st.markdown("### åˆè¨ˆ / æ¦‚æ³")
cols = st.columns(3)
with cols[0]:
    st.metric("ç·ãƒ™ã‚¯ãƒˆãƒ«æ•°", f"{total_vectors:,}")
with cols[1]:
    # bytes â†’ B/KB/MB/GBâ€¦ ã«è‡ªå‹•æ•´å½¢
    st.metric("vectors.npy åˆè¨ˆã‚µã‚¤ã‚º", sizeof_fmt(float(total_size_bytes)))
with cols[2]:
    # RAMåˆè¨ˆã¯ 1GB æœªæº€ãªã‚‰ MB ã§è¡¨ç¤º
    st.metric(
        "RAMè¦‹ç©ï¼ˆå…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰",
        f"{total_ram_est_gb*1024:.2f} MB" if total_ram_est_gb < 1.0 else f"{total_ram_est_gb:.2f} GB"
    )

# ========== è©³ç´°ï¼ˆä»»æ„ï¼‰ ==========
with st.expander("ğŸ§ª meta.jsonl ã®å…ˆé ­ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºèªï¼ˆä»»æ„ï¼‰", expanded=False):
    if sample_rows > 0:
        for shp in shards:
            meta = shp / "meta.jsonl"
            if not meta.exists():
                continue
            st.markdown(f"**{shp.name} / meta.jsonl**")
            try:
                with meta.open("r", encoding="utf-8") as f:
                    for i, line in enumerate(f):
                        if i >= sample_rows:
                            break
                        st.code(line.rstrip("\n"))
            except Exception as e:
                st.warning(f"{shp.name}: meta.jsonl èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        st.info("ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºã¯ 0 è¡Œã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")

# ========== æ¨å¥¨åˆ†å‰²ãƒ—ãƒ©ãƒ³ ==========
st.subheader("ğŸª“ å¤§ãã™ãã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®åˆ†å‰²ãƒ—ãƒ©ãƒ³ï¼ˆç›®å®‰ï¼‰")
need_split = [r for r in rows_sorted if r["n_vectors"] > int(max_vectors_per_shard)]
if not need_split:
    st.caption("ç¾çŠ¶ã®ã—ãã„å€¤ã§ã¯ã€åˆ†å‰²æ¨å¥¨ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    for r in need_split:
        n = int(r["n_vectors"])
        k, plan = plan_split(n, int(max_vectors_per_shard))
        st.markdown(f"**{r['shard_id']}** ã¯ **{n:,} ä»¶** â†’ æ¨å¥¨åˆ†å‰²æ•° **{k}**")
        st.caption("å‰²å½“ï¼ˆæ¦‚ç®—ï¼‰: " + ", ".join([f"shard_{i}: {cnt:,}" for (cnt, i) in plan]))
        with st.expander(f"åˆ†å‰²ã®é‹ç”¨ãƒ’ãƒ³ãƒˆ: {r['shard_id']}", expanded=False):
            st.markdown(
                "- å…ƒãƒ‡ãƒ¼ã‚¿ã®æ„å‘³ã®ã‚ã‚‹å¢ƒç•Œï¼ˆå¹´åˆ¥ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ãªã©ï¼‰ã§å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã¨é‹ç”¨ãŒå®‰å®šã—ã¾ã™ã€‚\n"
                "- ç”Ÿæˆæ™‚ã« new_shard_id ã‚’æŒ¯ã‚Šç›´ã—ã€`data/vectorstore/<backend>/<new_shard_id>/` ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚\n"
                "- æ—¢å­˜ã®æ¨ªæ–­æ¤œç´¢ãƒšãƒ¼ã‚¸ã¯è¤‡æ•° shard_id ã‚’é¸æŠå¯èƒ½ãªã®ã§ã€æ–°æ§‹æˆã§ã‚‚é‹ç”¨ã§ãã¾ã™ã€‚"
            )
