# pages/50_ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª.py
# ------------------------------------------------------------
# ğŸ—‚ï¸ vectorstore/(openai|local)/<shard_id>/meta.jsonl ãƒ“ãƒ¥ãƒ¼ã‚¢ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰å¯¾å¿œï¼‰
# - è¤‡æ•°ã‚·ãƒ£ãƒ¼ãƒ‰ã® meta.jsonl ã‚’æ¨ªæ–­ã—ã¦èª­ã¿è¾¼ã¿ãƒ»çµã‚Šè¾¼ã¿ãƒ»CSVå‡ºåŠ›
# - å˜ä¸€ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠæ™‚: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— / é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ / å®Œå…¨åˆæœŸåŒ– / å¾©å…ƒ
# - ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åã‚³ãƒ”ãƒ¼: year/file.pdf ã‚’1ã‚¯ãƒªãƒƒã‚¯ã§ clipboard ã«ã‚³ãƒ”ãƒ¼
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from datetime import datetime
from typing import Dict, Iterable, List, Optional, Tuple, Any
import json
import shutil
import re

import numpy as np
import pandas as pd
import streamlit as st

# ============================================================
# åŸºæœ¬ãƒ‘ã‚¹
# ============================================================
APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
VS_ROOT  = DATA_DIR / "vectorstore"

# ============================================================
# JSONL èª­ã¿è¾¼ã¿
# ============================================================
def iter_jsonl(path: Path) -> Iterable[Dict]:
    if not path.exists():
        return
    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f, start=1):
            s = line.strip()
            if not s:
                continue
            try:
                yield json.loads(s)
            except Exception as e:
                st.warning(f"[{path.name}] {i} è¡Œç›® JSONL ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")

# ============================================================
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–¢é€£
# ============================================================
def ensure_backup_dir(base: Path) -> Path:
    bdir = base / "backup" / datetime.now().strftime("%Y%m%d-%H%M%S")
    bdir.mkdir(parents=True, exist_ok=True)
    return bdir

def safe_copy(src: Path, dst_dir: Path) -> Optional[Path]:
    if src.exists():
        dst = dst_dir / src.name
        shutil.copy2(src, dst)
        return dst
    return None

def backup_all(base_dir: Path) -> Tuple[List[Path], Path]:
    bdir = ensure_backup_dir(base_dir)
    copied: List[Path] = []
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        p = base_dir / name
        dst = safe_copy(p, bdir)
        if dst is not None:
            copied.append(dst)
    return copied, bdir

def list_backup_dirs(base_dir: Path) -> List[Path]:
    broot = base_dir / "backup"
    if not broot.exists():
        return []
    dirs = [p for p in broot.iterdir() if p.is_dir()]
    def sort_key(p: Path):
        try:
            return datetime.strptime(p.name, "%Y%m%d-%H%M%S")
        except Exception:
            return datetime.fromtimestamp(p.stat().st_mtime)
    return sorted(dirs, key=sort_key, reverse=True)

def preview_backup(backup_dir: Path) -> pd.DataFrame:
    rows = []
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        p = backup_dir / name
        rows.append({
            "name": name,
            "exists": "âœ…" if p.exists() else "âŒ",
            "size": file_size_readable(p) if p.exists() else "N/A",
            "path": str(p)
        })
    return pd.DataFrame(rows)

def restore_from_backup(base_dir: Path, backup_dir: Path) -> Tuple[List[str], List[str]]:
    restored, missing = [], []
    for name in ["meta.jsonl", "vectors.npy", "processed_files.json"]:
        src = backup_dir / name
        dst = base_dir / name
        if src.exists():
            shutil.copy2(src, dst)
            restored.append(str(dst))
        else:
            missing.append(str(src))
    return restored, missing

# ============================================================
# processed_files.json ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def load_processed_files(p: Path) -> Optional[List[Any]]:
    if not p.exists():
        return None
    try:
        with p.open("r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else [data]
    except Exception as e:
        st.warning(f"processed_files.json èª­è¾¼å¤±æ•—: {e}")
        return None

def save_processed_files(p: Path, data: List[Any]) -> None:
    with p.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def extract_name(item: Any) -> Optional[str]:
    if isinstance(item, dict):
        for k in ("file", "filename", "path", "name"):
            v = item.get(k)
            if isinstance(v, str):
                return v
        return None
    if isinstance(item, str):
        return item
    return None

def filter_processed_list(items: List[Any], remove_files: List[str]) -> Tuple[List[Any], int]:
    rset = set(remove_files)
    kept, removed = [], 0
    for it in items:
        cand = extract_name(it)
        if isinstance(cand, str) and cand in rset:
            removed += 1
        else:
            kept.append(it)
    return kept, removed

def file_size_readable(p: Path) -> str:
    try:
        n = p.stat().st_size
    except Exception:
        return "N/A"
    for unit in ["B","KB","MB","GB","TB"]:
        if n < 1024:
            return f"{n:.0f} {unit}"
        n /= 1024
    return f"{n:.0f} PB"

# ============================================================
# ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã‚³ãƒ”ãƒ¼ï¼ˆJSåŸ‹ã‚è¾¼ã¿ï¼‰
# ============================================================
def copy_button(text: str, label: str, key: str):
    """
    ãƒ–ãƒ©ã‚¦ã‚¶ã® clipboard API ã‚’ä½¿ã£ã¦ text ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ãƒœã‚¿ãƒ³ã‚’æç”»ã€‚
    text ã¯ JSON ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦å®‰å…¨ã«åŸ‹ã‚è¾¼ã‚€ã€‚
    """
    payload = json.dumps(text, ensure_ascii=False)
    html = f"""
    <button id="{key}" style="
        padding:6px 10px;border-radius:8px;border:1px solid #dadce0;
        background:#fff;cursor:pointer;font-size:0.9rem;">ğŸ“‹ {label}</button>
    <script>
      const btn = document.getElementById("{key}");
      if (btn) {{
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({payload});
            const old = btn.innerText;
            btn.innerText = "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(()=>{{ btn.innerText = old; }}, 1200);
          }} catch(e) {{
            console.error(e);
            alert("ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: " + e);
          }}
        }});
      }}
    </script>
    """
    st.components.v1.html(html, height=38)

# ============================================================
# UI
# ============================================================
st.set_page_config(page_title="50 ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰å¯¾å¿œï¼‰", page_icon="ğŸ—‚ï¸", layout="wide")
st.title("ğŸ—‚ï¸ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šèª­ã¿è¾¼ã¿è¨­å®š ---
with st.sidebar:
    st.header("èª­ã¿è¾¼ã¿è¨­å®š")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
    base_backend_dir = VS_ROOT / backend
    if not base_backend_dir.exists():
        st.error(f"vectorstore/{backend} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs = sorted([p for p in base_backend_dir.iterdir() if p.is_dir()])
    shard_ids = [p.name for p in shard_dirs]
    sel_shards = st.multiselect("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", shard_ids, default=shard_ids)
    max_rows_read = st.number_input("å„ã‚·ãƒ£ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿æœ€å¤§è¡Œæ•°ï¼ˆ0=å…¨ä»¶ï¼‰", min_value=0, value=0, step=1000)

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
if not sel_shards:
    st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    st.stop()

all_rows: List[Dict] = []
for sid in sel_shards:
    bdir = base_backend_dir / sid
    meta_path = bdir / "meta.jsonl"
    read_cnt = 0
    for obj in iter_jsonl(meta_path):
        obj = dict(obj)
        obj.setdefault("shard_id", sid)
        obj.setdefault("file", obj.get("doc_id"))
        all_rows.append(obj)
        read_cnt += 1
        if max_rows_read and read_cnt >= max_rows_read:
            break

if not all_rows:
    st.warning("è¡¨ç¤ºã§ãã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

df = pd.DataFrame(all_rows)
for col in ["file","page","chunk_id","chunk_index","text","span_start","span_end","shard_id","year"]:
    if col not in df.columns:
        df[col] = None
df["chunk_len"] = df["text"].astype(str).str.len()

# å…ˆé ­ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
st.dataframe(df.head(500), use_container_width=True, height=560)
st.divider()

# ============================================================
# ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åã‚³ãƒ”ãƒ¼ï¼ˆyear/file.pdf ã‚’ãƒ¯ãƒ³ã‚¯ãƒªãƒƒãƒ—ï¼‰
# ============================================================
st.subheader("ğŸ“‹ å‚ç…§ç”¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚³ãƒ”ãƒ¼ï¼ˆyear/file.pdfï¼‰")
st.caption("09_ãƒœãƒƒãƒˆ.py ã®ã€å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã€æ¬„ã‚„ [[files: ...]] ã«è²¼ã‚‹ã®ã«ä¾¿åˆ©ã§ã™ã€‚")

# ä¸€æ„ãªãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆé¸æŠã‚·ãƒ£ãƒ¼ãƒ‰å†…ï¼‰
file_list = sorted(df["file"].dropna().unique().tolist())

# ã‚¯ã‚¤ãƒƒã‚¯æ¤œç´¢
colf1, colf2 = st.columns([2,1])
with colf1:
    q = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", value="")
with colf2:
    show_count = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=10, max_value=1000, value=100, step=10)

if q:
    q_norm = q.strip()
    filtered = [f for f in file_list if q_norm.lower() in str(f).lower()]
else:
    filtered = file_list

st.caption(f"ãƒ’ãƒƒãƒˆ: {len(filtered)} ä»¶")
if not filtered:
    st.info("ãƒ•ã‚£ãƒ«ã‚¿ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    # è¡Œã‚°ãƒªãƒƒãƒ‰ã§ã‚³ãƒ”ãƒ¼ã—ã‚„ã™ãä¸¦ã¹ã‚‹
    cols = st.columns(3)
    for i, f in enumerate(filtered[:int(show_count)]):
        with cols[i % 3]:
            st.write(f"`{f}`")
            copy_button(text=f, label="year/file.pdf ã‚’ã‚³ãƒ”ãƒ¼", key=f"copy_{i}")

st.divider()

# ============================================================
# ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆå˜ä¸€ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã¿ï¼‰
# ============================================================
st.subheader("ğŸ›  ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆå˜ä¸€ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠæ™‚ã®ã¿ï¼‰")
if len(sel_shards) != 1:
    st.info("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— / å‰Šé™¤ / å¾©å…ƒã¯ **å˜ä¸€ã‚·ãƒ£ãƒ¼ãƒ‰**ã‚’é¸æŠã—ãŸã¨ãã®ã¿æœ‰åŠ¹ã§ã™ã€‚")
    st.stop()

shard_id = sel_shards[0]
base_dir = base_backend_dir / shard_id
meta_path = base_dir / "meta.jsonl"
vec_path  = base_dir / "vectors.npy"
pf_path   = base_dir / "processed_files.json"

m1, m2 = st.columns(2)

# --- ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— ---
with m1:
    st.markdown("### ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ")
    if st.button("ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ", use_container_width=True):
        copied, bdir = backup_all(base_dir)
        st.success(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {bdir}\n" + "\n".join(f"- {p}" for p in copied))

# --- ğŸ§¹ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ ---
with m2:
    st.markdown("### ğŸ§¹ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã¿ï¼‰")
    st.caption("çµã‚Šè¾¼ã¿ã§é¸ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã€‚å®Ÿè¡Œå‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¨å¥¨ã€‚")

    sel_files = sorted(df["file"].dropna().unique().tolist())
    target_files = st.multiselect("å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«", sel_files)

    reset_pf_file = st.checkbox("processed_files.json ã‚‚ãƒªã‚»ãƒƒãƒˆï¼ˆå‰Šé™¤ï¼‰", value=True)
    confirm_del = st.checkbox("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿ã§å‰Šé™¤ã«åŒæ„ã—ã¾ã™ã€‚")

    if st.button("ğŸ§¹ å‰Šé™¤ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True,
                 disabled=not (target_files and confirm_del)):
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            copied, bdir = backup_all(base_dir)

            # meta.jsonl å†æ§‹ç¯‰ & vectors.npy æ›´æ–°
            keep_lines, keep_vec_indices = [], []
            removed_meta, valid_idx = 0, 0
            with meta_path.open("r", encoding="utf-8") as f:
                for raw in f:
                    s = raw.strip()
                    if not s:
                        continue
                    try:
                        obj = json.loads(s)
                    except:
                        keep_lines.append(raw)
                        continue
                    fname = obj.get("file") if isinstance(obj, dict) else None
                    if fname in set(target_files):
                        removed_meta += 1
                        valid_idx += 1
                        continue
                    else:
                        keep_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
                        keep_vec_indices.append(valid_idx)
                        valid_idx += 1
            with meta_path.open("w", encoding="utf-8") as f:
                f.writelines(keep_lines)

            removed_vecs = 0
            if vec_path.exists():
                vecs = np.load(vec_path)
                if vecs.ndim == 2:
                    new_vecs = vecs[keep_vec_indices] if keep_vec_indices else np.empty((0, vecs.shape[1]))
                    removed_vecs = vecs.shape[0] - new_vecs.shape[0]
                    np.save(vec_path, new_vecs)

            if reset_pf_file:
                if pf_path.exists():
                    pf_path.unlink()
            else:
                pf_list = load_processed_files(pf_path)
                if pf_list is not None:
                    kept, _ = filter_processed_list(pf_list, target_files)
                    save_processed_files(pf_path, kept)

            st.success(
                f"å‰Šé™¤å®Œäº† âœ…\n- meta.jsonl: {removed_meta} è¡Œå‰Šé™¤\n- vectors.npy: {removed_vecs} è¡Œå‰Šé™¤\n"
                + ("- processed_files.json: å‰Šé™¤ï¼ˆãƒªã‚»ãƒƒãƒˆï¼‰" if reset_pf_file else "- processed_files.json: å¯¾è±¡ã ã‘é™¤å¤–")
                + f"\n- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir}"
            )
        except Exception as e:
            st.error(f"å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# --- ğŸ—‘ï¸ å®Œå…¨åˆæœŸåŒ– ---
st.subheader("ğŸ—‘ï¸ ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å®Œå…¨åˆæœŸåŒ–")
st.caption("meta.jsonl / vectors.npy / processed_files.json ã‚’å…¨éƒ¨å‰Šé™¤ã—ã¾ã™ã€‚å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰ã€‚")
confirm_wipe = st.checkbox("å®Œå…¨åˆæœŸåŒ–ã«åŒæ„ã—ã¾ã™ã€‚")
if st.button("ğŸ—‘ï¸ å®Œå…¨åˆæœŸåŒ–ï¼ˆã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰", type="secondary", use_container_width=True, disabled=not confirm_wipe):
    deleted = []
    for name in ["meta.jsonl","vectors.npy","processed_files.json"]:
        p = base_dir / name
        if p.exists():
            p.unlink()
            deleted.append(str(p))
    if deleted:
        st.success("å‰Šé™¤ã—ã¾ã—ãŸ:\n" + "\n".join(f"- {x}" for x in deleted))
