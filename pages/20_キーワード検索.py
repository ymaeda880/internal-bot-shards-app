# ============================================
# å¤‰æ›´ç‚¹ï¼ˆã“ã®ç‰ˆã§ã®ä¿®æ­£ãƒ»è¿½åŠ ï¼‰
# --------------------------------------------
# 1) ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ï¼ˆOpenAIï¼‰:
#    - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã€ŒğŸ§  ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ã‚’è¿½åŠ ã—ã€æ¬¡ã‚’è¨­å®šå¯èƒ½ã«ã€‚
#      ãƒ»ãƒ’ãƒƒãƒˆè¦ç´„ã‚’ç”Ÿæˆï¼ˆæœ‰åŠ¹/ç„¡åŠ¹ï¼‰
#      ãƒ»ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ã€temperatureã€max_tokens
#      ãƒ»System Promptã€User Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ{query}, {snippets} ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰
#      ãƒ»ç”Ÿæˆã«ä½¿ã†ã‚¹ãƒ‹ãƒšãƒƒãƒˆä»¶æ•°ï¼ˆTop-Nï¼‰
# 2) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾®èª¿æ•´å¯èƒ½:
#    - User Promptï¼ˆä¾‹: ã€Œä»¥ä¸‹ã®ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’åŸºã«ã€{query}ã€ã«ã¤ã„ã¦è¦ç‚¹ã‚’ã¾ã¨ã‚ã¦ã€ï¼‰ã‚’ UI ã‹ã‚‰ç·¨é›†å¯èƒ½ã€‚
# 3) å®Ÿè¡Œéƒ¨ã«ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã‚’è¿½åŠ :
#    - æ¤œç´¢çµæœ DataFrame ä½œæˆå¾Œã€é¸æŠã•ã‚ŒãŸä¸Šä½ N ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’çµåˆã—ã¦ OpenAI ã«æŠ•å…¥ã€‚
#    - ç”Ÿæˆçµæœã‚’ç”»é¢å‡ºåŠ›ï¼ˆ"ğŸ§  ç”Ÿæˆè¦ç´„" ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã€‚
# 4) å®‰å…¨æ€§å‘ä¸Š:
#    - OPENAI_API_KEY æœªè¨­å®šæ™‚ã¯ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•çš„ã«ç„¡åŠ¹åŒ–ã—è­¦å‘Šã€‚
#    - ã‚¹ãƒ‹ãƒšãƒƒãƒˆä¸­ã® HTMLï¼ˆ<mark> ç­‰ï¼‰ã‚’é™¤å»ã—ã¦ã‹ã‚‰ LLM ã«æ¸¡ã™ã€‚
# ============================================

# pages/04_ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py
# ------------------------------------------------------------
# ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ / æ­£è¦è¡¨ç¾ã§ meta.jsonlï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’æ¨ªæ–­æ¤œç´¢
# - data/vectorstore/<backend>/<shard_id>/meta.jsonl ã‚’èª­ã¿è¾¼ã¿
# - AND/OR, å¤§æ–‡å­—å°æ–‡å­—, æ­£è¦è¡¨ç¾, æ—¥æœ¬èªã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–(NFKC + CJKé–“ã‚¹ãƒšãƒ¼ã‚¹é™¤å»)
# - ã‚·ãƒ£ãƒ¼ãƒ‰/å¹´/ãƒ•ã‚¡ã‚¤ãƒ«çµã‚Šè¾¼ã¿ã€çµæœãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã€CSVå‡ºåŠ›ã€year/file.pdf ã®ğŸ“‹ã‚³ãƒ”ãƒ¼
# - ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ OpenAI ã«æŠ•ã’ã¦è¦ç´„ã‚’ç”Ÿæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾®èª¿æ•´å¯ï¼‰
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Iterable, Any, Tuple
from datetime import datetime  # noqa
import re
import json
import unicodedata
import os

import numpy as np  # noqa
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# ============== ãƒ‘ã‚¹ ==============
APP_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = APP_ROOT / "data"
VS_ROOT  = DATA_DIR / "vectorstore"

# ============== æ—¥æœ¬èªæ­£è¦åŒ–ï¼ˆã‚¯ã‚¨ãƒª/æœ¬æ–‡ã®æºã‚Œå¯¾ç­–ï¼‰ ==============
CJK = r"\u3040-\u309F\u30A0-\u30FF\uFF65-\uFF9F\u4E00-\u9FFF\u3400-\u4DBF"
PUNC = r"ã€ã€‚ãƒ»ï¼Œï¼ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€Œã€ã€ã€ã€ˆã€‰ã€Šã€‹ã€ã€‘"

_cjk_cjk_space = re.compile(fr"(?<=[{CJK}])\s+(?=[{CJK}])")
_space_before_punc = re.compile(fr"\s+(?=[{PUNC}])")
_space_after_open = re.compile(fr"(?<=[ï¼ˆï¼»ï½›ã€Œã€ã€ˆã€Šã€])\s+")
_space_before_close = re.compile(fr"\s+(?=[ï¼‰ï¼½ï½ã€ã€ã€‰ã€‹ã€‘])")
_multi_space = re.compile(r"[ \t\u3000]+")

def normalize_ja_text(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _cjk_cjk_space.sub("", s)
    s = _space_before_punc.sub("", s)
    s = _space_after_open.sub("", s)
    s = _space_before_close.sub("", s)
    s = _multi_space.sub(" ", s)
    return s.strip()

# ============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆç”Ÿæˆç”¨ï¼‰ ==============
def strip_html(s: str) -> str:
    """ç°¡æ˜“ã« HTML ã‚¿ã‚°ã‚’é™¤å»ï¼ˆ<mark> ç­‰ï¼‰"""
    return re.sub(r"<[^>]+>", "", s or "")

def _count_tokens(text: str, model_hint: str = "gpt-4o-mini") -> int:
    try:
        import tiktoken
        try:
            enc = tiktoken.encoding_for_model(model_hint)
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception:
        return max(1, int(len(text) / 4))

# ============== JSONL èª­ã¿è¾¼ã¿ ==============
def iter_jsonl(path: Path) -> Iterable[Dict]:
    if not path.exists():
        return
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s:
                continue
            try:
                yield json.loads(s)
            except Exception:
                # å£Šã‚ŒãŸè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

# ============== UI åŸºæœ¬ ==============
load_dotenv()
st.set_page_config(page_title="04 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmetaæ¨ªæ–­ï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmeta.jsonl æ¨ªæ–­ï¼‰")

with st.sidebar:
    st.header("æ¤œç´¢å¯¾è±¡")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
    base_dir = VS_ROOT / backend
    if not base_dir.exists():
        st.error(f"vectorstore/{backend} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« 03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs = sorted([p for p in base_dir.iterdir() if p.is_dir()])
    shard_ids = [p.name for p in shard_dirs]
    sel_shards = st.multiselect("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰", shard_ids, default=shard_ids)

    st.divider()
    st.subheader("çµã‚Šè¾¼ã¿ï¼ˆä»»æ„ï¼‰")
    year_min = st.number_input("å¹´ï¼ˆä¸‹é™ï¼‰", value=0, step=1, help="0 ã§ç„¡åŠ¹")
    year_max = st.number_input("å¹´ï¼ˆä¸Šé™ï¼‰", value=9999, step=1, help="9999 ã§ç„¡åŠ¹")
    file_filter = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ / ä¾‹: budgetï¼‰", value="").strip()

    st.divider()
    st.subheader("è¡¨ç¤ºè¨­å®š")
    max_rows = st.number_input("æœ€å¤§è¡¨ç¤ºä»¶æ•°", min_value=50, max_value=5000, value=500, step=50)
    snippet_len = st.slider("ã‚¹ãƒ‹ãƒšãƒƒãƒˆé•·ï¼ˆå‰å¾Œåˆè¨ˆï¼‰", min_value=80, max_value=800, value=240, step=20)
    show_cols = st.multiselect(
        "è¡¨ç¤ºã‚«ãƒ©ãƒ ",
        ["file","year","page","shard_id","chunk_id","chunk_index","score","text"],
        default=["file","year","page","shard_id","score","text"]
    )

    st.divider()
    st.subheader("ğŸ§  ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆOpenAIï¼‰")
    has_key = bool(os.getenv("OPENAI_API_KEY"))
    gen_enabled = st.checkbox("ãƒ’ãƒƒãƒˆè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹", value=False, disabled=not has_key)
    if not has_key:
        st.warning("OPENAI_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ã€ç”Ÿæˆã¯ç„¡åŠ¹ã§ã™ã€‚", icon="âš ï¸")
    model = st.selectbox("ãƒ¢ãƒ‡ãƒ«", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0, disabled=not gen_enabled)
    temperature = st.slider("temperature", 0.0, 1.0, 0.2, 0.05, disabled=not gen_enabled)
    max_tokens = st.slider("max_tokens", 128, 4000, 1000, 64, disabled=not gen_enabled)
    topn_snippets = st.slider("ç”Ÿæˆã«ä½¿ã†ä¸Šä½ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°", 5, 200, 30, 5, disabled=not gen_enabled)

    sys_prompt = st.text_area(
        "System Prompt",
        value="ã‚ãªãŸã¯äº‹å®Ÿã«å¿ å®Ÿãªãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ ¹æ‹ ã®ã‚ã‚‹è¨˜è¿°ã®ã¿ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
        height=80,
        disabled=not gen_enabled
    )
    user_prompt_tpl = st.text_area(
        "User Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ{query}, {snippets} ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰",
        value=(
            "ä»¥ä¸‹ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã™ã€‚"
            "ã“ã®æƒ…å ±ã€ã®ã¿ã€‘ã‚’æ ¹æ‹ ã«ã€ã‚¯ã‚¨ãƒªã€{query}ã€ã«ã¤ã„ã¦è¦ç‚¹ã‚’ç®‡æ¡æ›¸ãâ†’çŸ­ã„ã¾ã¨ã‚ã®é †ã§æ•´ç†ã—ã¦ãã ã•ã„ã€‚"
            "\n\n# ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆ\n{snippets}"
        ),
        height=140,
        disabled=not gen_enabled
    )

# ============== æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ==============
st.markdown("### ã‚¯ã‚¨ãƒª")
c1, c2 = st.columns([3,2])
with c1:
    query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šã§ AND / OR æŒ‡å®šï¼‰", value="")
with c2:
    bool_mode = st.radio("ãƒ¢ãƒ¼ãƒ‰", ["AND", "OR"], index=0, horizontal=True)

c3, c4, c5, c6 = st.columns(4)
with c3:
    use_regex = st.checkbox("æ­£è¦è¡¨ç¾", value=False)
with c4:
    case_sensitive = st.checkbox("å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥", value=False)
with c5:
    normalize_query = st.checkbox("æ—¥æœ¬èªã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–ï¼ˆæ¨å¥¨ï¼‰", value=True)
with c6:
    norm_body = st.checkbox("æœ¬æ–‡ã‚‚æ­£è¦åŒ–ã—ã¦æ¤œç´¢", value=True, help="å–ã‚Šè¾¼ã¿æ™‚ã«æ­£è¦åŒ–ã—ã¦ã„ãªã„ã‚³ãƒ¼ãƒ‘ã‚¹å‘ã‘")

go = st.button("æ¤œç´¢ã‚’å®Ÿè¡Œ", type="primary")

# ============== æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ==============
def to_flags(case_sensitive: bool) -> int:
    return 0 if case_sensitive else re.IGNORECASE

def compile_terms(q: str, use_regex: bool, case_sensitive: bool) -> List[re.Pattern]:
    if normalize_query:
        q = normalize_ja_text(q)
    terms = [t for t in q.split() if t]
    if not terms:
        return []
    flags = to_flags(case_sensitive)
    pats = []
    for t in terms:
        if use_regex:
            try:
                pats.append(re.compile(t, flags))
            except re.error:
                # ä¸æ­£ãªæ­£è¦è¡¨ç¾ã¯ãƒªãƒ†ãƒ©ãƒ«æ‰±ã„
                pats.append(re.compile(re.escape(t), flags))
        else:
            pats.append(re.compile(re.escape(t), flags))
    return pats

def find_first_span(text: str, pats: List[re.Pattern]) -> Tuple[int,int,List[str]]:
    """
    æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ’ãƒƒãƒˆä½ç½®ï¼ˆmin start, max endï¼‰ã¨ã€ãƒ’ãƒƒãƒˆã—ãŸèªã®ä¸€è¦§ã‚’è¿”ã™
    """
    hits = []
    s_min = None
    e_max = None
    for p in pats:
        m = p.search(text)
        if m:
            hits.append(p.pattern)
            s, e = m.start(), m.end()
            s_min = s if s_min is None else min(s_min, s)
            e_max = e if e_max is None else max(e_max, e)
    if s_min is None:
        return -1, -1, []
    return s_min, e_max, hits

def make_snippet(text: str, pats: List[re.Pattern], total_len: int = 240) -> str:
    s, e, _ = find_first_span(text, pats)
    if s < 0:
        s, e = 0, min(len(text), total_len)
    margin = max(0, total_len // 2)
    left = max(0, s - margin)
    right = min(len(text), e + margin)
    snippet = text[left:right]

    # ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆHTMLï¼‰
    for p in pats:
        try:
            snippet = p.sub(lambda m: f"<mark>{m.group(0)}</mark>", snippet)
        except re.error:
            pass
    # ç«¯ã«çœç•¥è¨˜å·
    if left > 0:
        snippet = "â€¦"+snippet
    if right < len(text):
        snippet = snippet+"â€¦"
    return snippet

def copy_button(text: str, label: str, key: str):
    payload = json.dumps(text, ensure_ascii=False)
    html = f"""
    <button id="{key}" style="
        padding:6px 10px;border-radius:8px;border:1px solid #dadce0;
        background:#fff;cursor:pointer;font-size:0.9rem;">ğŸ“‹ {label}</button>
    <script>
      const btn = document.getElementById("{key}");
      if (btn) {{
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({payload});
            const old = btn.innerText;
            btn.innerText = "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(()=>{{ btn.innerText = old; }}, 1200);
          }} catch(e) {{
            console.error(e);
            alert("ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: " + e);
          }}
        }});
      }}
    </script>
    """
    st.components.v1.html(html, height=38)

if go:
    if not sel_shards:
        st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    pats = compile_terms(query, use_regex=use_regex, case_sensitive=case_sensitive)
    if not pats:
        st.warning("æ¤œç´¢èªãŒç©ºã§ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    rows: List[Dict[str,Any]] = []
    total_scanned = 0

    for sid in sel_shards:
        meta_path = base_dir / sid / "meta.jsonl"
        for obj in iter_jsonl(meta_path):
            total_scanned += 1
            # å¹´ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿
            yr = obj.get("year", None)
            if isinstance(yr, int):
                if year_min and yr < year_min: continue
                if year_max and year_max < 9999 and yr > year_max: continue
            if file_filter:
                if file_filter.lower() not in str(obj.get("file","")).lower():
                    continue

            text = str(obj.get("text",""))
            if norm_body:
                text_for_match = normalize_ja_text(text)
            else:
                text_for_match = text

            # ãƒãƒƒãƒåˆ¤å®š
            if bool_mode == "AND":
                ok = all(p.search(text_for_match) for p in pats)
            else:
                ok = any(p.search(text_for_match) for p in pats)

            if not ok:
                continue

            # ã‚¹ã‚³ã‚¢ = ãƒãƒƒãƒèªã®åˆè¨ˆå‡ºç¾æ•°ï¼ˆç°¡æ˜“ï¼‰
            score = 0
            for p in pats:
                score += len(list(p.finditer(text_for_match)))

            rows.append({
                "file": obj.get("file"),
                "year": obj.get("year"),
                "page": obj.get("page"),
                "shard_id": obj.get("shard_id", sid),
                "chunk_id": obj.get("chunk_id"),
                "chunk_index": obj.get("chunk_index"),
                "score": int(score),
                "text": make_snippet(text, pats, total_len=int(snippet_len)),
            })

            if len(rows) >= int(max_rows):
                break
        if len(rows) >= int(max_rows):
            break

    if not rows:
        st.warning("ãƒ’ãƒƒãƒˆãªã—ã€‚æ¤œç´¢èªã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ã‚¹ã‚³ã‚¢é™é †ã§ä¸¦ã¹æ›¿ãˆ
    df = pd.DataFrame(rows).sort_values(["score","year","file","page"], ascending=[False, True, True, True])

    st.success(f"ãƒ’ãƒƒãƒˆ {len(df):,d} ä»¶ / èµ°æŸ» {total_scanned:,d} ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆä¸Šä½ã®ã¿è¡¨ç¤ºï¼‰")
    # HTMLãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’åŠ¹ã‹ã›ã‚‹ãŸã‚ text åˆ—ã¯ markdown è¡¨ç¤ºã«
    show_order = [c for c in show_cols if c in df.columns]
    if "text" in show_order:
        non_text_cols = [c for c in show_order if c != "text"]
    else:
        non_text_cols = show_order

    st.dataframe(df[non_text_cols], use_container_width=True, height=420)
    if "text" in show_order:
        st.markdown("#### ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆ")
        for i, row in df.head(200).iterrows():  # ã‚¹ãƒ‹ãƒšãƒƒãƒˆéƒ¨åˆ†ã¯åˆ¥ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
            colA, colB = st.columns([4,1])
            with colA:
                st.markdown(f"**{row.get('file')}**  year={row.get('year')}  p.{row.get('page')}  "
                            f"score={row.get('score')}", help=row.get("chunk_id"))
                st.markdown(row.get("text",""), unsafe_allow_html=True)
            with colB:
                copy_button(text=str(row.get("file")), label="year/file ã‚’ã‚³ãƒ”ãƒ¼", key=f"cpy_{i}")

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSVï¼‰
    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ“¥ CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name="keyword_hits.csv", mime="text/csv")

    # ============== ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ ==============
    if gen_enabled:
        try:
            st.divider()
            st.subheader("ğŸ§  ç”Ÿæˆè¦ç´„ï¼ˆOpenAIï¼‰")

            # ä¸Šä½ N ä»¶ã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’çµåˆï¼ˆHTMLé™¤å»ï¼‰
            take_n = int(topn_snippets)
            selected = df.head(take_n).copy()
            # ã‚¹ãƒ‹ãƒšãƒƒãƒˆæœ¬æ–‡ï¼ˆtextåˆ—ã¯HTMLãƒã‚¤ãƒ©ã‚¤ãƒˆãªã®ã§é™¤å»ï¼‰
            joined_snippets = []
            for _, r in selected.iterrows():
                src = f"{r.get('file')} p.{r.get('page')} (score={r.get('score')})"
                snip = strip_html(str(r.get("text","")))
                joined_snippets.append(f"---\n# Source: {src}\n{snip}")
            snippets_text = "\n\n".join(joined_snippets)

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            user_prompt = user_prompt_tpl.format(query=query, snippets=snippets_text)

            # ã–ã£ãã‚Šãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆç›®å®‰ï¼‰
            approx_tokens = _count_tokens(user_prompt, model_hint=model)
            st.caption(f"ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: ~{approx_tokens:,} tokï¼‰")

            client = OpenAI()
            with st.spinner("OpenAI ã§è¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                resp = client.chat.completions.create(
                    model=model,
                    temperature=float(temperature),
                    max_tokens=int(max_tokens),
                    messages=[
                        {"role": "system", "content": sys_prompt.strip()},
                        {"role": "user", "content": user_prompt},
                    ],
                )
            out_text = resp.choices[0].message.content
            st.markdown(out_text)

        except Exception as e:
            st.error(f"ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

else:
    st.info("å·¦ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã¨æ¡ä»¶ã‚’é¸ã³ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€æ¤œç´¢ã‚’å®Ÿè¡Œã€ã—ã¦ãã ã•ã„ã€‚")
